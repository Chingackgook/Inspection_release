{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/Surprise",
    "API_Calls": [
        {
            "Name": "KNNBasic",
            "Description": "call_KNNBasic",
            "Code": "\"\"\"\nThis module descibes how to train on a full dataset (when no testset is\nbuilt/specified) and how to use the predict() method.\n\"\"\"\n\n\nfrom surprise import Dataset, KNNBasic\n\n# Load the movielens-100k dataset\ndata = Dataset.load_builtin(\"ml-100k\")\n\n# Retrieve the trainset.\ntrainset = data.build_full_trainset()\n\n# Build an algorithm, and train it.\nalgo = KNNBasic()\nalgo.fit(trainset)\n\n# we can now query for specific predicions\nuid = str(196)  # raw user id (as in the ratings file). They are **strings**!\niid = str(302)  # raw item id (as in the ratings file). They are **strings**!\n\n# get a prediction for specific users and items.\npred = algo.predict(uid, iid, r_ui=4, verbose=True)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Surprise/examples/predict_ratings.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "KNNBasic",
            "Description": "KNNBasic impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Surprise/surprise/prediction_algorithms/knns.py",
            "Implementation": "\nimport heapq\n\nimport numpy as np\n\nfrom .algo_base import AlgoBase\n\nfrom .predictions import PredictionImpossible\n\n\n\n\nclass AlgoBase:\n    \"\"\"Abstract class where is defined the basic behavior of a prediction\n    algorithm.\n\n    Keyword Args:\n        baseline_options(dict, optional): If the algorithm needs to compute a\n            baseline estimate, the ``baseline_options`` parameter is used to\n            configure how they are computed. See\n            :ref:`baseline_estimates_configuration` for usage.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n\n        self.bsl_options = kwargs.get(\"bsl_options\", {})\n        self.sim_options = kwargs.get(\"sim_options\", {})\n        if \"user_based\" not in self.sim_options:\n            self.sim_options[\"user_based\"] = True\n\n    def fit(self, trainset):\n        \"\"\"Train an algorithm on a given training set.\n\n        This method is called by every derived class as the first basic step\n        for training an algorithm. It basically just initializes some internal\n        structures and set the self.trainset attribute.\n\n        Args:\n            trainset(:obj:`Trainset <surprise.Trainset>`) : A training\n                set, as returned by the :meth:`folds\n                <surprise.dataset.Dataset.folds>` method.\n\n        Returns:\n            self\n        \"\"\"\n\n        self.trainset = trainset\n\n        # (re) Initialise baselines\n        self.bu = self.bi = None\n\n        return self\n\n    def predict(self, uid, iid, r_ui=None, clip=True, verbose=False):\n        \"\"\"Compute the rating prediction for given user and item.\n\n        The ``predict`` method converts raw ids to inner ids and then calls the\n        ``estimate`` method which is defined in every derived class. If the\n        prediction is impossible (e.g. because the user and/or the item is\n        unknown), the prediction is set according to\n        :meth:`default_prediction()\n        <surprise.prediction_algorithms.algo_base.AlgoBase.default_prediction>`.\n\n        Args:\n            uid: (Raw) id of the user. See :ref:`this note<raw_inner_note>`.\n            iid: (Raw) id of the item. See :ref:`this note<raw_inner_note>`.\n            r_ui(float): The true rating :math:`r_{ui}`. Optional, default is\n                ``None``.\n            clip(bool): Whether to clip the estimation into the rating scale.\n                For example, if :math:`\\\\hat{r}_{ui}` is :math:`5.5` while the\n                rating scale is :math:`[1, 5]`, then :math:`\\\\hat{r}_{ui}` is\n                set to :math:`5`. Same goes if :math:`\\\\hat{r}_{ui} < 1`.\n                Default is ``True``.\n            verbose(bool): Whether to print details of the prediction.  Default\n                is False.\n\n        Returns:\n            A :obj:`Prediction\\\n            <surprise.prediction_algorithms.predictions.Prediction>` object\n            containing:\n\n            - The (raw) user id ``uid``.\n            - The (raw) item id ``iid``.\n            - The true rating ``r_ui`` (:math:`r_{ui}`).\n            - The estimated rating (:math:`\\\\hat{r}_{ui}`).\n            - Some additional details about the prediction that might be useful\n              for later analysis.\n        \"\"\"\n\n        # Convert raw ids to inner ids\n        try:\n            iuid = self.trainset.to_inner_uid(uid)\n        except ValueError:\n            iuid = \"UKN__\" + str(uid)\n        try:\n            iiid = self.trainset.to_inner_iid(iid)\n        except ValueError:\n            iiid = \"UKN__\" + str(iid)\n\n        details = {}\n        try:\n            est = self.estimate(iuid, iiid)\n\n            # If the details dict was also returned\n            if isinstance(est, tuple):\n                est, details = est\n\n            details[\"was_impossible\"] = False\n\n        except PredictionImpossible as e:\n            est = self.default_prediction()\n            details[\"was_impossible\"] = True\n            details[\"reason\"] = str(e)\n\n        # clip estimate into [lower_bound, higher_bound]\n        if clip:\n            lower_bound, higher_bound = self.trainset.rating_scale\n            est = min(higher_bound, est)\n            est = max(lower_bound, est)\n\n        pred = Prediction(uid, iid, r_ui, est, details)\n\n        if verbose:\n            print(pred)\n\n        return pred\n\n    def default_prediction(self):\n        \"\"\"Used when the ``PredictionImpossible`` exception is raised during a\n        call to :meth:`predict()\n        <surprise.prediction_algorithms.algo_base.AlgoBase.predict>`. By\n        default, return the global mean of all ratings (can be overridden in\n        child classes).\n\n        Returns:\n            (float): The mean of all ratings in the trainset.\n        \"\"\"\n\n        return self.trainset.global_mean\n\n    def test(self, testset, verbose=False):\n        \"\"\"Test the algorithm on given testset, i.e. estimate all the ratings\n        in the given testset.\n\n        Args:\n            testset: A test set, as returned by a :ref:`cross-validation\n                itertor<use_cross_validation_iterators>` or by the\n                :meth:`build_testset() <surprise.Trainset.build_testset>`\n                method.\n            verbose(bool): Whether to print details for each predictions.\n                Default is False.\n\n        Returns:\n            A list of :class:`Prediction\\\n            <surprise.prediction_algorithms.predictions.Prediction>` objects\n            that contains all the estimated ratings.\n        \"\"\"\n\n        # The ratings are translated back to their original scale.\n        predictions = [\n            self.predict(uid, iid, r_ui_trans, verbose=verbose)\n            for (uid, iid, r_ui_trans) in testset\n        ]\n        return predictions\n\n    def compute_baselines(self):\n        \"\"\"Compute users and items baselines.\n\n        The way baselines are computed depends on the ``bsl_options`` parameter\n        passed at the creation of the algorithm (see\n        :ref:`baseline_estimates_configuration`).\n\n        This method is only relevant for algorithms using :func:`Pearson\n        baseline similarity<surprise.similarities.pearson_baseline>` or the\n        :class:`BaselineOnly\n        <surprise.prediction_algorithms.baseline_only.BaselineOnly>` algorithm.\n\n        Returns:\n            A tuple ``(bu, bi)``, which are users and items baselines.\"\"\"\n\n        # Firt of, if this method has already been called before on the same\n        # trainset, then just return. Indeed, compute_baselines may be called\n        # more than one time, for example when a similarity metric (e.g.\n        # pearson_baseline) uses baseline estimates.\n        if self.bu is not None:\n            return self.bu, self.bi\n\n        method = dict(als=baseline_als, sgd=baseline_sgd)\n\n        method_name = self.bsl_options.get(\"method\", \"als\")\n\n        try:\n            if getattr(self, \"verbose\", False):\n                print(\"Estimating biases using\", method_name + \"...\")\n            self.bu, self.bi = method[method_name](self)\n            return self.bu, self.bi\n        except KeyError:\n            raise ValueError(\n                \"Invalid method \"\n                + method_name\n                + \" for baseline computation.\"\n                + \" Available methods are als and sgd.\"\n            )\n\n    def compute_similarities(self):\n        \"\"\"Build the similarity matrix.\n\n        The way the similarity matrix is computed depends on the\n        ``sim_options`` parameter passed at the creation of the algorithm (see\n        :ref:`similarity_measures_configuration`).\n\n        This method is only relevant for algorithms using a similarity measure,\n        such as the :ref:`k-NN algorithms <pred_package_knn_inpired>`.\n\n        Returns:\n            The similarity matrix.\"\"\"\n\n        construction_func = {\n            \"cosine\": sims.cosine,\n            \"msd\": sims.msd,\n            \"pearson\": sims.pearson,\n            \"pearson_baseline\": sims.pearson_baseline,\n        }\n\n        if self.sim_options[\"user_based\"]:\n            n_x, yr = self.trainset.n_users, self.trainset.ir\n        else:\n            n_x, yr = self.trainset.n_items, self.trainset.ur\n\n        min_support = self.sim_options.get(\"min_support\", 1)\n\n        args = [n_x, yr, min_support]\n\n        name = self.sim_options.get(\"name\", \"msd\").lower()\n        if name == \"pearson_baseline\":\n            shrinkage = self.sim_options.get(\"shrinkage\", 100)\n            bu, bi = self.compute_baselines()\n            if self.sim_options[\"user_based\"]:\n                bx, by = bu, bi\n            else:\n                bx, by = bi, bu\n\n            args += [self.trainset.global_mean, bx, by, shrinkage]\n\n        try:\n            if getattr(self, \"verbose\", False):\n                print(f\"Computing the {name} similarity matrix...\")\n            sim = construction_func[name](*args)\n            if getattr(self, \"verbose\", False):\n                print(\"Done computing similarity matrix.\")\n            return sim\n        except KeyError:\n            raise NameError(\n                \"Wrong sim name \"\n                + name\n                + \". Allowed values \"\n                + \"are \"\n                + \", \".join(construction_func.keys())\n                + \".\"\n            )\n\n    def get_neighbors(self, iid, k):\n        \"\"\"Return the ``k`` nearest neighbors of ``iid``, which is the inner id\n        of a user or an item, depending on the ``user_based`` field of\n        ``sim_options`` (see :ref:`similarity_measures_configuration`).\n\n        As the similarities are computed on the basis of a similarity measure,\n        this method is only relevant for algorithms using a similarity measure,\n        such as the :ref:`k-NN algorithms <pred_package_knn_inpired>`.\n\n        For a usage example, see the :ref:`FAQ <get_k_nearest_neighbors>`.\n\n        Args:\n            iid(int): The (inner) id of the user (or item) for which we want\n                the nearest neighbors. See :ref:`this note<raw_inner_note>`.\n\n            k(int): The number of neighbors to retrieve.\n\n        Returns:\n            The list of the ``k`` (inner) ids of the closest users (or items)\n            to ``iid``.\n        \"\"\"\n\n        if self.sim_options[\"user_based\"]:\n            all_instances = self.trainset.all_users\n        else:\n            all_instances = self.trainset.all_items\n        others = [(x, self.sim[iid, x]) for x in all_instances() if x != iid]\n        others = heapq.nlargest(k, others, key=lambda tple: tple[1])\n        k_nearest_neighbors = [j for (j, _) in others]\n\n        return k_nearest_neighbors\n\n\n\n\nclass SymmetricAlgo(AlgoBase):\n    \"\"\"This is an abstract class aimed to ease the use of symmetric algorithms.\n\n    A symmetric algorithm is an algorithm that can be based on users or on\n    items indifferently, e.g. all the algorithms in this module.\n\n    When the algo is user-based x denotes a user and y an item. Else, it's\n    reversed.\n    \"\"\"\n\n    def __init__(self, sim_options={}, verbose=True, **kwargs):\n\n        AlgoBase.__init__(self, sim_options=sim_options, **kwargs)\n        self.verbose = verbose\n\n    def switch(self, u_stuff, i_stuff):\n        \"\"\"Return x_stuff and y_stuff depending on the user_based field.\"\"\"\n\n        if self.sim_options[\"user_based\"]:\n            return u_stuff, i_stuff\n        else:\n            return i_stuff, u_stuff\n\n\nclass KNNBasic(SymmetricAlgo):\n    \"\"\"A basic collaborative filtering algorithm.\n\n    The prediction :math:`\\\\hat{r}_{ui}` is set as:\n\n    .. math::\n        \\\\hat{r}_{ui} = \\\\frac{\n        \\\\sum\\\\limits_{v \\\\in N^k_i(u)} \\\\text{sim}(u, v) \\\\cdot r_{vi}}\n        {\\\\sum\\\\limits_{v \\\\in N^k_i(u)} \\\\text{sim}(u, v)}\n\n    or\n\n    .. math::\n        \\\\hat{r}_{ui} = \\\\frac{\n        \\\\sum\\\\limits_{j \\\\in N^k_u(i)} \\\\text{sim}(i, j) \\\\cdot r_{uj}}\n        {\\\\sum\\\\limits_{j \\\\in N^k_u(i)} \\\\text{sim}(i, j)}\n\n    depending on the ``user_based`` field of the ``sim_options`` parameter.\n\n    Args:\n        k(int): The (max) number of neighbors to take into account for\n            aggregation (see :ref:`this note <actual_k_note>`). Default is\n            ``40``.\n        min_k(int): The minimum number of neighbors to take into account for\n            aggregation. If there are not enough neighbors, the prediction is\n            set to the global mean of all ratings. Default is ``1``.\n        sim_options(dict): A dictionary of options for the similarity\n            measure. See :ref:`similarity_measures_configuration` for accepted\n            options.\n        verbose(bool): Whether to print trace messages of bias estimation,\n            similarity, etc.  Default is True.\n    \"\"\"\n\n    def __init__(self, k=40, min_k=1, sim_options={}, verbose=True, **kwargs):\n\n        SymmetricAlgo.__init__(self, sim_options=sim_options, verbose=verbose, **kwargs)\n        self.k = k\n        self.min_k = min_k\n\n    def fit(self, trainset):\n\n        SymmetricAlgo.fit(self, trainset)\n        self.sim = self.compute_similarities()\n\n        return self\n\n    def estimate(self, u, i):\n\n        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n            raise PredictionImpossible(\"User and/or item is unknown.\")\n\n        x, y = self.switch(u, i)\n\n        neighbors = [(self.sim[x, x2], r) for (x2, r) in self.yr[y]]\n        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[0])\n\n        # compute weighted average\n        sum_sim = sum_ratings = actual_k = 0\n        for (sim, r) in k_neighbors:\n            if sim > 0:\n                sum_sim += sim\n                sum_ratings += sim * r\n                actual_k += 1\n\n        if actual_k < self.min_k:\n            raise PredictionImpossible(\"Not enough neighbors.\")\n\n        est = sum_ratings / sum_sim\n\n        details = {\"actual_k\": actual_k}\n        return est, details",
            "Examples": [
                "\n"
            ]
        }
    ]
}