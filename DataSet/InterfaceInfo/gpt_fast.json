{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/gpt-fast",
    "API_Calls": [
        {
            "Name": "call_generate",
            "Description": "call generate",
            "Code": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport itertools\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union\n\nimport torch\nimport torch._dynamo.config\nimport torch._inductor.config\nfrom torch.nn.attention.flex_attention import BlockMask, create_block_mask\n\ndef device_sync(device):\n    if \"cuda\" in device:\n        torch.cuda.synchronize(device)\n    elif (\"cpu\" in device) or (\"mps\" in device):\n        pass\n    else:\n        print(f\"device={device} is not yet suppported\")\n\n\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.triton.unique_kernel_names = True\n# Experimental features to reduce compilation times, will be on by default in future\ntorch._inductor.config.fx_graph_cache = True \ntorch._functorch.config.enable_autograd_cache = True\n\ndefault_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ncreate_block_mask = torch.compile(create_block_mask)\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom model import Transformer\nfrom tokenizer import get_tokenizer\n\n\n\ndef prefill(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> torch.Tensor:\n    # input_pos: [B, S]\n    mask = create_block_mask(causal_mask, 1, 1, input_pos.shape[0], model.max_seq_length, device=x.device)\n    logits = model(mask, x, input_pos)\n    return sample(logits, **sampling_kwargs)[0]\n\ndef decode_one_token(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, block_mask: BlockMask, **sampling_kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n    # input_pos: [B, 1]\n    assert input_pos.shape[-1] == 1\n    block_index = input_pos // block_mask.BLOCK_SIZE[0]\n    mask = block_mask[:, :, block_index]\n    mask.mask_mod = block_mask.mask_mod\n    mask.seq_lengths = (1, model.max_seq_length)\n    logits = model(mask, x, input_pos)\n    return sample(logits, **sampling_kwargs)\n\n\ndef model_forward(model, x, input_pos):\n    return model(x, input_pos)\n\n\n\ndef encode_tokens(tokenizer, string, bos=True, device=default_device):\n    tokens = tokenizer.encode(string)\n    if bos:\n        tokens = [tokenizer.bos_id()] + tokens\n    return torch.tensor(tokens, dtype=torch.int, device=device)\n\ndef _load_model(checkpoint_path, device, precision, use_tp):\n    use_cuda = 'cuda' in device\n    with torch.device('meta'):\n        model = Transformer.from_name(checkpoint_path.parent.name)\n\n    if \"int8\" in str(checkpoint_path):\n        print(\"Using int8 weight-only quantization!\")\n        from quantize import WeightOnlyInt8QuantHandler\n        simple_quantizer = WeightOnlyInt8QuantHandler(model)\n        model = simple_quantizer.convert_for_runtime()\n\n    if \"int4\" in str(checkpoint_path):\n        print(\"Using int4 weight-only quantization!\")\n        path_comps = checkpoint_path.name.split(\".\")\n        groupsize = int(path_comps[-2][1:])\n        from quantize import WeightOnlyInt4QuantHandler\n        simple_quantizer = WeightOnlyInt4QuantHandler(model, groupsize)\n        model = simple_quantizer.convert_for_runtime()\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n    if \"model\" in checkpoint and \"stories\" in str(checkpoint_path):\n        checkpoint = checkpoint[\"model\"]\n    model.load_state_dict(checkpoint, assign=True)\n\n    if use_tp:\n        from tp import apply_tp\n        print(\"Applying tensor parallel to model ...\")\n        apply_tp(model)\n\n    model = model.to(device=device, dtype=precision)\n    return model.eval()\n\ndef _get_model_size(model):\n    model_size = 0\n    params = 0\n    for name, child in model.named_children():\n        if not isinstance(child, torch.nn.Embedding):\n            model_size += sum(\n                [\n                    p.numel() * p.dtype.itemsize\n                    for p in itertools.chain(child.parameters(), child.buffers())\n                ]\n            )\n            params += sum(\n                [\n                    p.numel()\n                    for p in itertools.chain(child.parameters(), child.buffers())\n                ]\n            )\n    return model_size, params\n\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\n\ndef main(\n    prompt: Union[int, str] = \"Hello, my name is\",\n    interactive: bool = False,\n    num_samples: int = 5,\n    max_new_tokens: int = 100,\n    batch_size: int = 1,\n    top_k: int = 200,\n    temperature: float = 0.8,\n    checkpoint_path: Path = Path(\"checkpoints/meta-Transformer/Transformer-2-7b-chat-hf/model.pth\"),\n    compile: bool = True,\n    compile_prefill: bool = False,\n    profile: Optional[Path] = None,\n    draft_checkpoint_path: Optional[Path] = None,\n    speculate_k: int = 5,\n    device=default_device,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained Transformer model and tokenizer.\n    \"\"\"\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n    assert tokenizer_path.is_file(), str(tokenizer_path)\n\n    global print\n    from tp import maybe_init_dist\n    rank = maybe_init_dist()\n    use_tp = rank is not None\n    if use_tp:\n        if rank != 0:\n            # only print on rank 0\n            print = lambda *args, **kwargs: None\n\n    print(f\"Using device={device}\")\n    precision = torch.bfloat16\n    is_speculative = draft_checkpoint_path is not None\n    is_chat = \"chat\" in str(checkpoint_path)\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n    model = _load_model(checkpoint_path, device, precision, use_tp)\n\n    if is_speculative:\n        draft_model = _load_model(draft_checkpoint_path, device, precision, use_tp)\n    else:\n        draft_model = None\n\n    device_sync(device=device) # MKG\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds\")\n\n    tokenizer = get_tokenizer(tokenizer_path, checkpoint_path)\n\n    if isinstance(prompt, str):\n        encoded = encode_tokens(tokenizer, prompt, bos=True, device=device)\n    else:\n        # generate a fully synthetic prompt\n        encoded = torch.randint(0, 1024, (prompt,), device=device, dtype=torch.int64)\n    prompt_length = encoded.size(-1)\n\n    torch.manual_seed(1234)\n    model_size, params = _get_model_size(model)\n    if compile:\n        if is_speculative and use_tp: # and (\"cuda\" in device):\n            torch._inductor.config.triton.cudagraph_trees = False # Bug with cudagraph trees in this case\n\n        if is_speculative:\n            global model_forward, logits_to_prob\n            model_forward = torch.compile(model_forward, mode=\"reduce-overhead\", fullgraph=True)\n\n        global decode_one_token, prefill\n        decode_one_token = torch.compile(decode_one_token, mode=\"reduce-overhead\", fullgraph=True)\n\n        # Uncomment to squeeze more perf out of prefill\n        if compile_prefill:\n            prefill = torch.compile(prefill, fullgraph=True, dynamic=True)\n\n\n    aggregate_metrics = {\n        'tokens_per_sec': [],\n        'accept_counts': [],\n    }\n    start = -1 if compile else 0\n\n    for i in range(start, num_samples):\n        device_sync(device=device) # MKG\n        if i >= 0 and interactive:\n            prompt = input(\"What is your prompt? \")\n            if is_chat:\n                prompt = f\"{B_INST} {prompt.strip()} {E_INST}\"\n            encoded = encode_tokens(tokenizer, prompt, bos=True, device=device)\n\n        if interactive and i >= 0:\n            buffer = []\n            period_id = tokenizer.encode('.')[0]\n            done_generating = False\n            def callback(x):\n                nonlocal done_generating\n                if done_generating:\n                    return\n                buffer.append(tokenizer.decode([period_id] + x.tolist())[1:])\n                if x.item() == tokenizer.eos_id():\n                    done_generating = True\n                if len(buffer) == 4 or done_generating:\n                    print(''.join(buffer), end='', flush=True)\n                    buffer.clear()\n                # print(, end='', flush=True)\n        else:\n            callback = lambda x : x\n        t0 = time.perf_counter()\n        import contextlib\n        if (i != num_samples - 1 or not profile) or (use_tp and rank != 0):\n            prof = contextlib.nullcontext()\n        else:\n            torch.profiler._utils._init_for_cuda_graphs()\n            prof = torch.profiler.profile()\n        with prof:\n            y, metrics = generate(\n                model,\n                encoded,\n                max_new_tokens,\n                batch_size=batch_size,\n                draft_model=draft_model,\n                speculate_k=speculate_k,\n                interactive=interactive,\n                callback=callback,\n                temperature=temperature,\n                top_k=top_k,\n            )\n            aggregate_metrics['accept_counts'].append(metrics['accept_counts'])\n        if i == -1:\n            print(f\"Compilation time: {time.perf_counter() - t0:.2f} seconds\")\n            continue\n        if hasattr(prof, \"export_chrome_trace\"):\n            if use_tp:\n                prof.export_chrome_trace(f\"{profile}_rank_{rank}.json\")\n            else:\n                prof.export_chrome_trace(f\"{profile}.json\")\n        device_sync(device=device) # MKG\n        t = time.perf_counter() - t0\n\n        if not interactive:\n            # Just displaying the first generation\n            if batch_size > 1:\n                print(\"Only displaying the first generation of the batch\")\n            print(tokenizer.decode(y[0].tolist()))\n        else:\n            print()\n        tokens_generated = y.size(-1) - prompt_length\n        generated_tokens_sec = tokens_generated / t\n        aggregate_metrics['tokens_per_sec'].append(generated_tokens_sec)\n        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {generated_tokens_sec:.02f} tokens/sec\")\n        print(f\"Bandwidth achieved: {model_size * generated_tokens_sec / 1e9:.02f} GB/s\")\n        total_tokens_sec = y.numel() / t\n        print(f\"FLOPS achieved: {params * total_tokens_sec * 2 / 1e12:.02f} TF/s\")\n        print()\n    print(\"==========\")\n    if is_speculative:\n        counts_aggregated = [sum(i) for i in zip(*aggregate_metrics['accept_counts'])]\n        acceptance_probs = [i/sum(counts_aggregated) for i in counts_aggregated]\n        print(f\"Acceptance probs: {acceptance_probs}\")\n        print(f\"Mean Accepted: {sum([idx * i for idx, i in enumerate(counts_aggregated)])/sum(counts_aggregated)}\")\n\n    print(f\"Batch Size: {batch_size}\")\n    print(f\"Prompt Length: {prompt_length}\")\n    print(f\"Generated tokens: {max_new_tokens}\")\n    print(f\"Average tokens/sec: {torch.mean(torch.tensor(aggregate_metrics['tokens_per_sec'])).item():.2f}\")\n    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    def int_or_str(x):\n        try:\n            return int(x)\n        except:\n            return x\n\n    parser.add_argument('--prompt', type=int_or_str, default=\"Hello, my name is\", help=\"Input prompt. If it's an integer, will instead generate a synthetic prompt.\")\n    parser.add_argument('--interactive', action='store_true', help='Whether to launch in interactive mode')\n    parser.add_argument('--num_samples', type=int, default=5, help='Number of samples.')\n    parser.add_argument('--max_new_tokens', type=int, default=200, help='Maximum number of new tokens.')\n    parser.add_argument('--batch_size', type=int, default=1, help='Batch size to benchmark with')\n    parser.add_argument('--top_k', type=int, default=200, help='Top-k for sampling.')\n    parser.add_argument('--temperature', type=float, default=0.8, help='Temperature for sampling.')\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"checkpoints/meta-Transformer/Transformer-2-7b-chat-hf/model.pth\"), help='Model checkpoint path.')\n    parser.add_argument('--compile', action='store_true', help='Whether to compile the model.')\n    parser.add_argument('--compile_prefill', action='store_true', help='Whether to compile the prefill (improves prefill perf, but higher compile times)')\n    parser.add_argument('--profile', type=Path, default=None, help='Profile path.')\n    parser.add_argument('--speculate_k', type=int, default=5, help='Speculative execution depth.')\n    parser.add_argument('--draft_checkpoint_path', type=Path, default=None, help='Draft checkpoint path.')\n    parser.add_argument('--device', type=str, default=default_device, help='Device to use')\n\n    args = parser.parse_args()\n    main(\n        args.prompt, args.interactive, args.num_samples, args.max_new_tokens, args.batch_size, args.top_k,\n        args.temperature, args.checkpoint_path, args.compile, args.compile_prefill, args.profile, args.draft_checkpoint_path,\n        args.speculate_k, args.device\n    )\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/gpt-fast/generate.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "generate",
            "Description": "generate",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/gpt-fast/generate.py",
            "Implementation": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport itertools\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union\n\nimport torch\nimport torch._dynamo.config\nimport torch._inductor.config\nfrom torch.nn.attention.flex_attention import BlockMask, create_block_mask\n\n\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.triton.unique_kernel_names = True\n# Experimental features to reduce compilation times, will be on by default in future\ntorch._inductor.config.fx_graph_cache = True \ntorch._functorch.config.enable_autograd_cache = True\n\ndefault_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ncreate_block_mask = torch.compile(create_block_mask)\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom model import Transformer\nfrom tokenizer import get_tokenizer\n\n\n\n@torch.no_grad()\ndef generate(\n    model: Transformer,\n    prompt: torch.Tensor,\n    max_new_tokens: int,\n    batch_size: int,\n    *,\n    interactive: bool,\n    draft_model: Transformer,\n    speculate_k: Optional[int] = 8,\n    callback = lambda x: x,\n    **sampling_kwargs\n) -> torch.Tensor:\n    \"\"\"\n    Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n    \"\"\"\n\n    is_speculative = draft_model is not None\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = prompt.size(-1)\n    T_new = T + max_new_tokens\n    if interactive:\n        max_seq_length = 350\n    else:\n        max_seq_length = min(T_new, model.config.block_size)\n\n    device, dtype = prompt.device, prompt.dtype\n    max_seq_length = max_seq_length + speculate_k + 1 if is_speculative else max_seq_length\n    with torch.device(device):\n        model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n        if is_speculative and draft_model is not model:\n            draft_model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty(batch_size, T_new, dtype=dtype, device=device)\n    # We are just making the same prompt for every batch\n    prompt = prompt.view(1, -1).repeat(batch_size, 1)\n    empty[:, :T] = prompt\n    seq = empty\n    input_pos = torch.arange(0, T, device=device)\n\n    next_token = prefill(model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs).clone()\n    if is_speculative:\n        prefill(draft_model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs)\n    seq[:, T] = next_token.squeeze()\n\n    input_pos = torch.tensor([T], device=device, dtype=torch.int)\n    accept_counts = [0] * (speculate_k + 1)\n\n    if is_speculative:\n        input_pos = input_pos.item()  # for speculative decoding easier to keep on host\n        while input_pos < T_new - 1:\n            cur_token = next_token.view(())\n\n            next_tokens = speculative_decode(\n                model, draft_model, cur_token, input_pos, speculate_k, **sampling_kwargs\n            )\n\n            accept_counts[len(next_tokens) - 1] += 1\n            num_added = min(T_new - input_pos - 1, len(next_tokens))\n            seq[input_pos + 1 : input_pos + num_added + 1] = next_tokens[: num_added]\n            for i in next_tokens[: num_added,]:\n                callback(i)\n            input_pos = input_pos + num_added\n            next_token = next_tokens[-1]\n    else:\n        generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)\n        seq[:, T + 1:] = torch.cat(generated_tokens, dim=-1)\n\n    generate_stats = {\n        'accept_counts': accept_counts\n    }\n    return seq, generate_stats",
            "Examples": [
                "\n"
            ]
        }
    ]
}