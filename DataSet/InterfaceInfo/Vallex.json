{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/VALL-E-X",
    "API_Calls": [
        {
            "Name": "call_apis",
            "Description": "a ui to generate wav",
            "Code": "# coding: utf-8\nimport argparse\nimport logging\nimport os\nimport pathlib\nimport time\nimport tempfile\nimport platform\nimport webbrowser\nimport sys\nprint(f\"default encoding is {sys.getdefaultencoding()},file system encoding is {sys.getfilesystemencoding()}\")\nprint(f\"You are using Python version {platform.python_version()}\")\nif(sys.version_info[0]<3 or sys.version_info[1]<7):\n    print(\"The Python version is too low and may cause problems\")\n\nif platform.system().lower() == 'windows':\n    temp = pathlib.PosixPath\n    pathlib.PosixPath = pathlib.WindowsPath\nelse:\n    temp = pathlib.WindowsPath\n    pathlib.WindowsPath = pathlib.PosixPath\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n\nimport langid\nlangid.set_languages(['en', 'zh', 'ja'])\n\nimport nltk\nnltk.data.path = nltk.data.path + [os.path.join(os.getcwd(), \"nltk_data\")]\n\nimport torch\nimport torchaudio\nimport random\n\nimport numpy as np\n\nfrom data.tokenizer import (\n    AudioTokenizer,\n    tokenize_audio,\n)\nfrom data.collation import get_text_token_collater\nfrom models.vallex import VALLE\nfrom utils.g2p import PhonemeBpeTokenizer\nfrom descriptions import *\nfrom macros import N_DIM, NUM_HEAD, NUM_LAYERS, PREFIX_MODE, NUM_QUANTIZERS, lang2token, token2lang, lang2code, langdropdown2token , code2lang\nfrom examples import *\n\nimport gradio as gr\nimport whisper\nfrom vocos import Vocos\nimport multiprocessing\n\nthread_count = multiprocessing.cpu_count()\n\nprint(\"Use\",thread_count,\"cpu cores for computing\")\n\ntorch.set_num_threads(thread_count)\ntorch.set_num_interop_threads(thread_count)\ntorch._C._jit_set_profiling_executor(False)\ntorch._C._jit_set_profiling_mode(False)\ntorch._C._set_graph_executor_optimize(False)\n\ntext_tokenizer = PhonemeBpeTokenizer(tokenizer_path=\"./utils/g2p/bpe_69.json\")\ntext_collater = get_text_token_collater()\n\ndevice = torch.device(\"cpu\")\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\", 0)\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n# VALL-E-X model\nif not os.path.exists(\"./checkpoints/\"): os.mkdir(\"./checkpoints/\")\nif not os.path.exists(os.path.join(\"./checkpoints/\", \"vallex-checkpoint.pt\")):\n    import wget\n    try:\n        logging.info(\"Downloading model from https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt ...\")\n        # download from https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt to ./checkpoints/vallex-checkpoint.pt\n        wget.download(\"https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt\",\n                      out=\"./checkpoints/vallex-checkpoint.pt\", bar=wget.bar_adaptive)\n    except Exception as e:\n        logging.info(e)\n        raise Exception(\n            \"\\n Model weights download failed, please go to 'https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt'\"\n            \"\\n manually download model weights and put it to {} .\".format(os.getcwd() + \"\\checkpoints\"))\n\nmodel = VALLE(\n        N_DIM,\n        NUM_HEAD,\n        NUM_LAYERS,\n        norm_first=True,\n        add_prenet=False,\n        prefix_mode=PREFIX_MODE,\n        share_embedding=True,\n        nar_scale_factor=1.0,\n        prepend_bos=True,\n        num_quantizers=NUM_QUANTIZERS,\n    )\ncheckpoint = torch.load(\"./checkpoints/vallex-checkpoint.pt\", map_location='cpu')\nmissing_keys, unexpected_keys = model.load_state_dict(\n    checkpoint[\"model\"], strict=True\n)\nassert not missing_keys\nmodel.eval()\n\n# Encodec model\naudio_tokenizer = AudioTokenizer(device)\n\n# Vocos decoder\nvocos = Vocos.from_pretrained('charactr/vocos-encodec-24khz').to(device)\n\n# ASR\nif not os.path.exists(\"./whisper/\"): os.mkdir(\"./whisper/\")\ntry:\n    whisper_model = whisper.load_model(\"medium\",download_root=os.path.join(os.getcwd(), \"whisper\")).cpu()\nexcept Exception as e:\n    logging.info(e)\n    raise Exception(\n        \"\\n Whisper download failed or damaged, please go to \"\n        \"'https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt'\"\n        \"\\n manually download model and put it to {} .\".format(os.getcwd() + \"\\whisper\"))\n\n# Voice Presets\npreset_list = os.walk(\"./presets/\").__next__()[2]\npreset_list = [preset[:-4] for preset in preset_list if preset.endswith(\".npz\")]\n\ndef clear_prompts():\n    try:\n        path = tempfile.gettempdir()\n        for eachfile in os.listdir(path):\n            filename = os.path.join(path, eachfile)\n            if os.path.isfile(filename) and filename.endswith(\".npz\"):\n                lastmodifytime = os.stat(filename).st_mtime\n                endfiletime = time.time() - 60\n                if endfiletime > lastmodifytime:\n                    os.remove(filename)\n    except:\n        return\n\ndef transcribe_one(model, audio_path):\n    # load audio and pad/trim it to fit 30 seconds\n    audio = whisper.load_audio(audio_path)\n    audio = whisper.pad_or_trim(audio)\n\n    # make log-Mel spectrogram and move to the same device as the model\n    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n    # detect the spoken language\n    _, probs = model.detect_language(mel)\n    print(f\"Detected language: {max(probs, key=probs.get)}\")\n    lang = max(probs, key=probs.get)\n    # decode the audio\n    options = whisper.DecodingOptions(temperature=1.0, best_of=5, fp16=False if device == torch.device(\"cpu\") else True, sample_len=150)\n    result = whisper.decode(model, mel, options)\n\n    # print the recognized text\n    print(result.text)\n\n    text_pr = result.text\n    if text_pr.strip(\" \")[-1] not in \"?!.,。，？！。、\":\n        text_pr += \".\"\n    return lang, text_pr\n\ndef make_npz_prompt(name, uploaded_audio, recorded_audio, transcript_content):\n    global model, text_collater, text_tokenizer, audio_tokenizer\n    clear_prompts()\n    audio_prompt = uploaded_audio if uploaded_audio is not None else recorded_audio\n    sr, wav_pr = audio_prompt\n    if not isinstance(wav_pr, torch.FloatTensor):\n        wav_pr = torch.FloatTensor(wav_pr)\n    if wav_pr.abs().max() > 1:\n        wav_pr /= wav_pr.abs().max()\n    if wav_pr.size(-1) == 2:\n        wav_pr = wav_pr[:, 0]\n    if wav_pr.ndim == 1:\n        wav_pr = wav_pr.unsqueeze(0)\n    assert wav_pr.ndim and wav_pr.size(0) == 1\n\n    if transcript_content == \"\":\n        text_pr, lang_pr = make_prompt(name, wav_pr, sr, save=False)\n    else:\n        lang_pr = langid.classify(str(transcript_content))[0]\n        lang_token = lang2token[lang_pr]\n        text_pr = f\"{lang_token}{str(transcript_content)}{lang_token}\"\n    # tokenize audio\n    encoded_frames = tokenize_audio(audio_tokenizer, (wav_pr, sr))\n    audio_tokens = encoded_frames[0][0].transpose(2, 1).cpu().numpy()\n\n    # tokenize text\n    phonemes, _ = text_tokenizer.tokenize(text=f\"{text_pr}\".strip())\n    text_tokens, enroll_x_lens = text_collater(\n        [\n            phonemes\n        ]\n    )\n\n    message = f\"Detected language: {lang_pr}\\n Detected text {text_pr}\\n\"\n\n    # save as npz file\n    np.savez(os.path.join(tempfile.gettempdir(), f\"{name}.npz\"),\n             audio_tokens=audio_tokens, text_tokens=text_tokens, lang_code=lang2code[lang_pr])\n    return message, os.path.join(tempfile.gettempdir(), f\"{name}.npz\")\n\n\ndef make_prompt(name, wav, sr, save=True):\n    global whisper_model\n    whisper_model.to(device)\n    if not isinstance(wav, torch.FloatTensor):\n        wav = torch.tensor(wav)\n    if wav.abs().max() > 1:\n        wav /= wav.abs().max()\n    if wav.size(-1) == 2:\n        wav = wav.mean(-1, keepdim=False)\n    if wav.ndim == 1:\n        wav = wav.unsqueeze(0)\n    assert wav.ndim and wav.size(0) == 1\n    torchaudio.save(f\"./prompts/{name}.wav\", wav, sr)\n    lang, text = transcribe_one(whisper_model, f\"./prompts/{name}.wav\")\n    lang_token = lang2token[lang]\n    text = lang_token + text + lang_token\n    with open(f\"./prompts/{name}.txt\", 'w', encoding='utf-8') as f:\n        f.write(text)\n    if not save:\n        os.remove(f\"./prompts/{name}.wav\")\n        os.remove(f\"./prompts/{name}.txt\")\n\n    whisper_model.cpu()\n    torch.cuda.empty_cache()\n    return text, lang\n\n@torch.no_grad()\ndef infer_from_audio(text, language, accent, audio_prompt, record_audio_prompt, transcript_content):\n    global model, text_collater, text_tokenizer, audio_tokenizer\n    audio_prompt = audio_prompt if audio_prompt is not None else record_audio_prompt\n    sr, wav_pr = audio_prompt\n    if not isinstance(wav_pr, torch.FloatTensor):\n        wav_pr = torch.FloatTensor(wav_pr)\n    if wav_pr.abs().max() > 1:\n        wav_pr /= wav_pr.abs().max()\n    if wav_pr.size(-1) == 2:\n        wav_pr = wav_pr[:, 0]\n    if wav_pr.ndim == 1:\n        wav_pr = wav_pr.unsqueeze(0)\n    assert wav_pr.ndim and wav_pr.size(0) == 1\n\n    if transcript_content == \"\":\n        text_pr, lang_pr = make_prompt('dummy', wav_pr, sr, save=False)\n    else:\n        lang_pr = langid.classify(str(transcript_content))[0]\n        lang_token = lang2token[lang_pr]\n        text_pr = f\"{lang_token}{str(transcript_content)}{lang_token}\"\n\n    if language == 'auto-detect':\n        lang_token = lang2token[langid.classify(text)[0]]\n    else:\n        lang_token = langdropdown2token[language]\n    lang = token2lang[lang_token]\n    text = lang_token + text + lang_token\n\n    # onload model\n    model.to(device)\n\n    # tokenize audio\n    encoded_frames = tokenize_audio(audio_tokenizer, (wav_pr, sr))\n    audio_prompts = encoded_frames[0][0].transpose(2, 1).to(device)\n\n    # tokenize text\n    logging.info(f\"synthesize text: {text}\")\n    phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n    text_tokens, text_tokens_lens = text_collater(\n        [\n            phone_tokens\n        ]\n    )\n\n    enroll_x_lens = None\n    if text_pr:\n        text_prompts, _ = text_tokenizer.tokenize(text=f\"{text_pr}\".strip())\n        text_prompts, enroll_x_lens = text_collater(\n            [\n                text_prompts\n            ]\n        )\n    text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n    text_tokens_lens += enroll_x_lens\n    lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n    encoded_frames = model.inference(\n        text_tokens.to(device),\n        text_tokens_lens.to(device),\n        audio_prompts,\n        enroll_x_lens=enroll_x_lens,\n        top_k=-100,\n        temperature=1,\n        prompt_language=lang_pr,\n        text_language=langs if accent == \"no-accent\" else lang,\n        best_of=5,\n    )\n    # Decode with Vocos\n    frames = encoded_frames.permute(2,0,1)\n    features = vocos.codes_to_features(frames)\n    samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n    # offload model\n    model.to('cpu')\n    torch.cuda.empty_cache()\n\n    message = f\"text prompt: {text_pr}\\nsythesized text: {text}\"\n    return message, (24000, samples.squeeze(0).cpu().numpy())\n\n@torch.no_grad()\ndef infer_from_prompt(text, language, accent, preset_prompt, prompt_file):\n    clear_prompts()\n    model.to(device)\n    # text to synthesize\n    if language == 'auto-detect':\n        lang_token = lang2token[langid.classify(text)[0]]\n    else:\n        lang_token = langdropdown2token[language]\n    lang = token2lang[lang_token]\n    text = lang_token + text + lang_token\n\n    # load prompt\n    if prompt_file is not None:\n        prompt_data = np.load(prompt_file.name)\n    else:\n        prompt_data = np.load(os.path.join(\"./presets/\", f\"{preset_prompt}.npz\"))\n    audio_prompts = prompt_data['audio_tokens']\n    text_prompts = prompt_data['text_tokens']\n    lang_pr = prompt_data['lang_code']\n    lang_pr = code2lang[int(lang_pr)]\n\n    # numpy to tensor\n    audio_prompts = torch.tensor(audio_prompts).type(torch.int32).to(device)\n    text_prompts = torch.tensor(text_prompts).type(torch.int32)\n\n    enroll_x_lens = text_prompts.shape[-1]\n    logging.info(f\"synthesize text: {text}\")\n    phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n    text_tokens, text_tokens_lens = text_collater(\n        [\n            phone_tokens\n        ]\n    )\n    text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n    text_tokens_lens += enroll_x_lens\n    # accent control\n    lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n    encoded_frames = model.inference(\n        text_tokens.to(device),\n        text_tokens_lens.to(device),\n        audio_prompts,\n        enroll_x_lens=enroll_x_lens,\n        top_k=-100,\n        temperature=1,\n        prompt_language=lang_pr,\n        text_language=langs if accent == \"no-accent\" else lang,\n        best_of=5,\n    )\n    # Decode with Vocos\n    frames = encoded_frames.permute(2,0,1)\n    features = vocos.codes_to_features(frames)\n    samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n    model.to('cpu')\n    torch.cuda.empty_cache()\n\n    message = f\"sythesized text: {text}\"\n    return message, (24000, samples.squeeze(0).cpu().numpy())\n\n\nfrom utils.sentence_cutter import split_text_into_sentences\n@torch.no_grad()\ndef infer_long_text(text, preset_prompt, prompt=None, language='auto', accent='no-accent'):\n    \"\"\"\n    For long audio generation, two modes are available.\n    fixed-prompt: This mode will keep using the same prompt the user has provided, and generate audio sentence by sentence.\n    sliding-window: This mode will use the last sentence as the prompt for the next sentence, but has some concern on speaker maintenance.\n    \"\"\"\n    mode = 'fixed-prompt'\n    global model, audio_tokenizer, text_tokenizer, text_collater\n    model.to(device)\n    if (prompt is None or prompt == \"\") and preset_prompt == \"\":\n        mode = 'sliding-window'  # If no prompt is given, use sliding-window mode\n    sentences = split_text_into_sentences(text)\n    # detect language\n    if language == \"auto-detect\":\n        language = langid.classify(text)[0]\n    else:\n        language = token2lang[langdropdown2token[language]]\n\n    # if initial prompt is given, encode it\n    if prompt is not None and prompt != \"\":\n        # load prompt\n        prompt_data = np.load(prompt.name)\n        audio_prompts = prompt_data['audio_tokens']\n        text_prompts = prompt_data['text_tokens']\n        lang_pr = prompt_data['lang_code']\n        lang_pr = code2lang[int(lang_pr)]\n\n        # numpy to tensor\n        audio_prompts = torch.tensor(audio_prompts).type(torch.int32).to(device)\n        text_prompts = torch.tensor(text_prompts).type(torch.int32)\n    elif preset_prompt is not None and preset_prompt != \"\":\n        prompt_data = np.load(os.path.join(\"./presets/\", f\"{preset_prompt}.npz\"))\n        audio_prompts = prompt_data['audio_tokens']\n        text_prompts = prompt_data['text_tokens']\n        lang_pr = prompt_data['lang_code']\n        lang_pr = code2lang[int(lang_pr)]\n\n        # numpy to tensor\n        audio_prompts = torch.tensor(audio_prompts).type(torch.int32).to(device)\n        text_prompts = torch.tensor(text_prompts).type(torch.int32)\n    else:\n        audio_prompts = torch.zeros([1, 0, NUM_QUANTIZERS]).type(torch.int32).to(device)\n        text_prompts = torch.zeros([1, 0]).type(torch.int32)\n        lang_pr = language if language != 'mix' else 'en'\n    if mode == 'fixed-prompt':\n        complete_tokens = torch.zeros([1, NUM_QUANTIZERS, 0]).type(torch.LongTensor).to(device)\n        for text in sentences:\n            text = text.replace(\"\\n\", \"\").strip(\" \")\n            if text == \"\":\n                continue\n            lang_token = lang2token[language]\n            lang = token2lang[lang_token]\n            text = lang_token + text + lang_token\n\n            enroll_x_lens = text_prompts.shape[-1]\n            logging.info(f\"synthesize text: {text}\")\n            phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n            text_tokens, text_tokens_lens = text_collater(\n                [\n                    phone_tokens\n                ]\n            )\n            text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n            text_tokens_lens += enroll_x_lens\n            # accent control\n            lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n            encoded_frames = model.inference(\n                text_tokens.to(device),\n                text_tokens_lens.to(device),\n                audio_prompts,\n                enroll_x_lens=enroll_x_lens,\n                top_k=-100,\n                temperature=1,\n                prompt_language=lang_pr,\n                text_language=langs if accent == \"no-accent\" else lang,\n                best_of=5,\n            )\n            complete_tokens = torch.cat([complete_tokens, encoded_frames.transpose(2, 1)], dim=-1)\n        # Decode with Vocos\n        frames = complete_tokens.permute(1, 0, 2)\n        features = vocos.codes_to_features(frames)\n        samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n        model.to('cpu')\n        message = f\"Cut into {len(sentences)} sentences\"\n        return message, (24000, samples.squeeze(0).cpu().numpy())\n    elif mode == \"sliding-window\":\n        complete_tokens = torch.zeros([1, NUM_QUANTIZERS, 0]).type(torch.LongTensor).to(device)\n        original_audio_prompts = audio_prompts\n        original_text_prompts = text_prompts\n        for text in sentences:\n            text = text.replace(\"\\n\", \"\").strip(\" \")\n            if text == \"\":\n                continue\n            lang_token = lang2token[language]\n            lang = token2lang[lang_token]\n            text = lang_token + text + lang_token\n\n            enroll_x_lens = text_prompts.shape[-1]\n            logging.info(f\"synthesize text: {text}\")\n            phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n            text_tokens, text_tokens_lens = text_collater(\n                [\n                    phone_tokens\n                ]\n            )\n            text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n            text_tokens_lens += enroll_x_lens\n            # accent control\n            lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n            encoded_frames = model.inference(\n                text_tokens.to(device),\n                text_tokens_lens.to(device),\n                audio_prompts,\n                enroll_x_lens=enroll_x_lens,\n                top_k=-100,\n                temperature=1,\n                prompt_language=lang_pr,\n                text_language=langs if accent == \"no-accent\" else lang,\n                best_of=5,\n            )\n            complete_tokens = torch.cat([complete_tokens, encoded_frames.transpose(2, 1)], dim=-1)\n            if torch.rand(1) < 1.0:\n                audio_prompts = encoded_frames[:, :, -NUM_QUANTIZERS:]\n                text_prompts = text_tokens[:, enroll_x_lens:]\n            else:\n                audio_prompts = original_audio_prompts\n                text_prompts = original_text_prompts\n        # Decode with Vocos\n        frames = complete_tokens.permute(1, 0, 2)\n        features = vocos.codes_to_features(frames)\n        samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n        model.to('cpu')\n        message = f\"Cut into {len(sentences)} sentences\"\n        return message, (24000, samples.squeeze(0).cpu().numpy())\n    else:\n        raise ValueError(f\"No such mode {mode}\")\n\n\ndef main():\n    app = gr.Blocks(title=\"VALL-E X\")\n    with app:\n        gr.Markdown(top_md)\n        with gr.Tab(\"Infer from audio\"):\n            gr.Markdown(infer_from_audio_md)\n            with gr.Row():\n                with gr.Column():\n\n                    textbox = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=\"Welcome back, Master. What can I do for you today?\", elem_id=f\"tts-input\")\n                    language_dropdown = gr.Dropdown(choices=['auto-detect', 'English', '中文', '日本語'], value='auto-detect', label='language')\n                    accent_dropdown = gr.Dropdown(choices=['no-accent', 'English', '中文', '日本語'], value='no-accent', label='accent')\n                    textbox_transcript = gr.TextArea(label=\"Transcript\",\n                                          placeholder=\"Write transcript here. (leave empty to use whisper)\",\n                                          value=\"\", elem_id=f\"prompt-name\")\n                    upload_audio_prompt = gr.Audio(label='uploaded audio prompt', source='upload', interactive=True)\n                    record_audio_prompt = gr.Audio(label='recorded audio prompt', source='microphone', interactive=True)\n                with gr.Column():\n                    text_output = gr.Textbox(label=\"Message\")\n                    audio_output = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn = gr.Button(\"Generate!\")\n                    btn.click(infer_from_audio,\n                              inputs=[textbox, language_dropdown, accent_dropdown, upload_audio_prompt, record_audio_prompt, textbox_transcript],\n                              outputs=[text_output, audio_output])\n                    textbox_mp = gr.TextArea(label=\"Prompt name\",\n                                          placeholder=\"Name your prompt here\",\n                                          value=\"prompt_1\", elem_id=f\"prompt-name\")\n                    btn_mp = gr.Button(\"Make prompt!\")\n                    prompt_output = gr.File(interactive=False)\n                    btn_mp.click(make_npz_prompt,\n                                inputs=[textbox_mp, upload_audio_prompt, record_audio_prompt, textbox_transcript],\n                                outputs=[text_output, prompt_output])\n            gr.Examples(examples=infer_from_audio_examples,\n                        inputs=[textbox, language_dropdown, accent_dropdown, upload_audio_prompt, record_audio_prompt, textbox_transcript],\n                        outputs=[text_output, audio_output],\n                        fn=infer_from_audio,\n                        cache_examples=False,)\n        with gr.Tab(\"Make prompt\"):\n            gr.Markdown(make_prompt_md)\n            with gr.Row():\n                with gr.Column():\n                    textbox2 = gr.TextArea(label=\"Prompt name\",\n                                          placeholder=\"Name your prompt here\",\n                                          value=\"prompt_1\", elem_id=f\"prompt-name\")\n                    # 添加选择语言和输入台本的地方\n                    textbox_transcript2 = gr.TextArea(label=\"Transcript\",\n                                          placeholder=\"Write transcript here. (leave empty to use whisper)\",\n                                          value=\"\", elem_id=f\"prompt-name\")\n                    upload_audio_prompt_2 = gr.Audio(label='uploaded audio prompt', source='upload', interactive=True)\n                    record_audio_prompt_2 = gr.Audio(label='recorded audio prompt', source='microphone', interactive=True)\n                with gr.Column():\n                    text_output_2 = gr.Textbox(label=\"Message\")\n                    prompt_output_2 = gr.File(interactive=False)\n                    btn_2 = gr.Button(\"Make!\")\n                    btn_2.click(make_npz_prompt,\n                              inputs=[textbox2, upload_audio_prompt_2, record_audio_prompt_2, textbox_transcript2],\n                              outputs=[text_output_2, prompt_output_2])\n            gr.Examples(examples=make_npz_prompt_examples,\n                        inputs=[textbox2, upload_audio_prompt_2, record_audio_prompt_2, textbox_transcript2],\n                        outputs=[text_output_2, prompt_output_2],\n                        fn=make_npz_prompt,\n                        cache_examples=False,)\n        with gr.Tab(\"Infer from prompt\"):\n            gr.Markdown(infer_from_prompt_md)\n            with gr.Row():\n                with gr.Column():\n                    textbox_3 = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=\"Welcome back, Master. What can I do for you today?\", elem_id=f\"tts-input\")\n                    language_dropdown_3 = gr.Dropdown(choices=['auto-detect', 'English', '中文', '日本語', 'Mix'], value='auto-detect',\n                                                    label='language')\n                    accent_dropdown_3 = gr.Dropdown(choices=['no-accent', 'English', '中文', '日本語'], value='no-accent',\n                                                  label='accent')\n                    preset_dropdown_3 = gr.Dropdown(choices=preset_list, value=None, label='Voice preset')\n                    prompt_file = gr.File(file_count='single', file_types=['.npz'], interactive=True)\n                with gr.Column():\n                    text_output_3 = gr.Textbox(label=\"Message\")\n                    audio_output_3 = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn_3 = gr.Button(\"Generate!\")\n                    btn_3.click(infer_from_prompt,\n                              inputs=[textbox_3, language_dropdown_3, accent_dropdown_3, preset_dropdown_3, prompt_file],\n                              outputs=[text_output_3, audio_output_3])\n            gr.Examples(examples=infer_from_prompt_examples,\n                        inputs=[textbox_3, language_dropdown_3, accent_dropdown_3, preset_dropdown_3, prompt_file],\n                        outputs=[text_output_3, audio_output_3],\n                        fn=infer_from_prompt,\n                        cache_examples=False,)\n        with gr.Tab(\"Infer long text\"):\n            gr.Markdown(\"This is a long text generation demo. You can use this to generate long audio. \")\n            with gr.Row():\n                with gr.Column():\n                    textbox_4 = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=long_text_example, elem_id=f\"tts-input\")\n                    language_dropdown_4 = gr.Dropdown(choices=['auto-detect', 'English', '中文', '日本語'], value='auto-detect',\n                                                    label='language')\n                    accent_dropdown_4 = gr.Dropdown(choices=['no-accent', 'English', '中文', '日本語'], value='no-accent',\n                                                    label='accent')\n                    preset_dropdown_4 = gr.Dropdown(choices=preset_list, value=None, label='Voice preset')\n                    prompt_file_4 = gr.File(file_count='single', file_types=['.npz'], interactive=True)\n                with gr.Column():\n                    text_output_4 = gr.TextArea(label=\"Message\")\n                    audio_output_4 = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn_4 = gr.Button(\"Generate!\")\n                    btn_4.click(infer_long_text,\n                              inputs=[textbox_4, preset_dropdown_4, prompt_file_4, language_dropdown_4, accent_dropdown_4],\n                              outputs=[text_output_4, audio_output_4])\n\n    webbrowser.open(\"http://127.0.0.1:7860\")\n    app.launch()\n\nif __name__ == \"__main__\":\n    formatter = (\n        \"%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(message)s\"\n    )\n    logging.basicConfig(format=formatter, level=logging.INFO)\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/VALL-E-X/launch_ui.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "VALLE",
            "Description": "VALLE model",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/VALL-E-X/models/vallex.py",
            "Implementation": "# Copyright    2023                             (authors: Feiteng Li)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\nfrom typing import Dict, Iterator, List, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from icefall.utils import make_pad_mask\n# from torchmetrics.classification import MulticlassAccuracy\n\nfrom data.input_strategies import PromptedFeatures\nfrom modules.embedding import SinePositionalEmbedding, TokenEmbedding\nfrom modules.transformer import (\n    AdaptiveLayerNorm,\n    LayerNorm,\n    TransformerDecoderLayer,\n    TransformerEncoder,\n    TransformerEncoderLayer,\n)\n\nfrom .macros import NUM_AUDIO_TOKENS, NUM_TEXT_TOKENS\nfrom .visualizer import visualize\n\nclass VALLE(VALLF):\n    \"\"\"It implements https://arxiv.org/abs/2301.02111\n    \"Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers\"\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        nhead: int,\n        num_layers: int,\n        norm_first: bool = True,\n        add_prenet: bool = False,\n        prefix_mode: int = 0,\n        share_embedding: bool = True,\n        nar_scale_factor: float = 1.0,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n          d_model:\n            The number of expected features in the input (required).\n          nhead:\n            The number of heads in the multiheadattention models (required).\n          num_layers:\n            The number of sub-decoder-layers in the decoder (required).\n        \"\"\"\n        super(VALLE, self).__init__(\n            d_model,\n            nhead,\n            num_layers,\n            norm_first=norm_first,\n            add_prenet=add_prenet,\n            decoder_cls=TransformerEncoder,\n            decoder_layer_cls=TransformerEncoderLayer,\n            prefix_mode=prefix_mode,\n            share_embedding=share_embedding,\n            nar_scale_factor=nar_scale_factor,\n            **kwargs,\n        )\n        self.language_ID = {\n            'en': 0,\n            'zh': 1,\n            'ja': 2,\n        }\n        self.ar_language_embedding = TokenEmbedding(d_model, len(self.language_ID))\n        self.nar_language_embedding = TokenEmbedding(d_model, len(self.language_ID))\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        x_lens: torch.Tensor,\n        y: Union[torch.Tensor, PromptedFeatures],\n        y_lens: Union[torch.Tensor, PromptedFeatures],\n        reduction: str = \"sum\",\n        train_stage: int = 0,\n        **kwargs,\n    ):\n        raise NotImplementedError\n    def inference(\n        self,\n        x: torch.Tensor,\n        x_lens: torch.Tensor,\n        y: torch.Tensor,\n        enroll_x_lens: torch.Tensor,\n        top_k: int = -100,\n        temperature: float = 1.0,\n        prompt_language: str = None,\n        text_language: str = None,\n        best_of: int = 1,\n        length_penalty: float = 1.0,\n        return_worst: bool = False,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n          x:\n            A 2-D tensor of shape (1, S).\n          x_lens:\n            A 1-D tensor of shape (1,). It contains the number of tokens in `x`\n            before padding.\n          y:\n            A 3-D tensor of shape (1, T, 8).\n          top_k: (`optional`) int\n            The number of highest probability tokens to keep for top-k-filtering. Default to -100.\n          temperature: (`optional`) float\n            The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n        Returns:\n          Return the predicted audio code matrix.\n        \"\"\"\n        assert x.ndim == 2, x.shape\n        assert x_lens.ndim == 1, x_lens.shape\n        assert y.ndim == 3, y.shape\n        assert y.shape[0] == 1, y.shape\n\n        assert torch.all(x_lens > 0)\n\n        # NOTE: x has been padded in TextTokenCollater\n        text = x\n        x = self.ar_text_embedding(text)\n        # Add language embedding\n        prompt_language_id = torch.LongTensor(np.array([self.language_ID[prompt_language]])).to(x.device)\n        if isinstance(text_language, str):\n            text_language_id = torch.LongTensor(np.array([self.language_ID[text_language]])).to(x.device)\n        elif isinstance(text_language, List):\n            text_language_id = torch.LongTensor(np.array([self.language_ID[tl] for tl in text_language])).to(x.device)\n        x[:, :enroll_x_lens, :] += self.ar_language_embedding(prompt_language_id)\n        x[:, enroll_x_lens:, :] += self.ar_language_embedding(text_language_id)\n        x = self.ar_text_prenet(x)\n        x = self.ar_text_position(x)\n\n        text_len = x_lens.max()\n        prompts = y\n        prefix_len = y.shape[1]\n\n        # AR Decoder\n        # TODO: Managing decoder steps avoid repetitive computation\n        y = prompts[..., 0]\n        if self.ar_audio_prepend_bos:\n            y = F.pad(y, (1, 0), value=NUM_AUDIO_TOKENS + 1)\n\n        x_len = x_lens.max()\n        x_attn_mask = torch.zeros((x_len, x_len), dtype=torch.bool)\n\n        kv_cache = None\n        use_kv_caching = True\n\n        sum_logprobs = torch.zeros(best_of, device=y.device)  # implement batch decoding here\n        x = x.repeat(best_of, 1, 1)\n        y = y.repeat(best_of, 1)\n        while True:\n            y_emb = self.ar_audio_embedding(y)\n            y_emb = self.ar_audio_prenet(y_emb)\n            y_pos = self.ar_audio_position(y_emb)\n            xy_pos = torch.concat([x, y_pos], dim=1)\n\n            y_len = y.shape[1]\n            x_attn_mask_pad = F.pad(\n                x_attn_mask,\n                (0, y_len),\n                value=True,\n            )\n            y_attn_mask = F.pad(\n                torch.triu(\n                    torch.ones(y_len, y_len, dtype=torch.bool), diagonal=1\n                ),\n                (x_len, 0),\n                value=False,\n            )\n            xy_attn_mask = torch.concat(\n                [x_attn_mask_pad, y_attn_mask], dim=0\n            ).to(y.device)\n\n\n            if use_kv_caching and kv_cache is not None:\n                xy_pos = xy_pos[:, [-1]]\n            else:\n                pass\n\n            xy_dec, kv_cache = self.ar_decoder.infer(\n                xy_pos,\n                mask=xy_attn_mask,\n                past_kv=kv_cache,\n                use_cache=use_kv_caching,\n            )\n            # xy_dec, _ = self.ar_decoder(\n            #     (xy_pos, None),\n            #     mask=xy_attn_mask,\n            # )\n\n            logits = self.ar_predict_layer(xy_dec[:, -1])\n            samples, current_logprobs = topk_sampling(\n                logits, top_k=top_k, top_p=1, temperature=temperature\n            )\n            sum_logprobs += current_logprobs * (y[:, -1] != NUM_AUDIO_TOKENS)\n            samples[y[:, -1] == NUM_AUDIO_TOKENS] = NUM_AUDIO_TOKENS\n            completed = (samples[:, -1] == NUM_AUDIO_TOKENS).all()\n            if (\n                completed\n                or (y.shape[1] - prompts.shape[1]) > x_lens.max() * 16\n            ):\n                if prompts.shape[1] == y.shape[1]:\n                    raise SyntaxError(\n                        \"well trained model shouldn't reach here.\"\n                    )\n                lengths = torch.sum(y != NUM_AUDIO_TOKENS, dim=1)\n                avg_logprobs = sum_logprobs / lengths ** length_penalty\n                # choose the best beam according to sum_logprobs\n                best_beam = y[torch.argmax(avg_logprobs), :]\n                worst_beam = y[torch.argmin(avg_logprobs), :]\n                # strip all eos tokens\n                best_beam = best_beam[best_beam != NUM_AUDIO_TOKENS]\n                worst_beam = worst_beam[worst_beam != NUM_AUDIO_TOKENS]\n                if return_worst:\n                    y = worst_beam.unsqueeze(0)\n                else:\n                    y = best_beam.unsqueeze(0)\n                print(f\"VALL-E EOS [{prompts.shape[1]} -> {y.shape[1]}]\")\n                break\n\n            y = torch.concat([y, samples], dim=1)\n\n        codes = [y[:, prefix_len + int(self.ar_audio_prepend_bos) :]]\n        if self.num_quantizers == 1:\n            return torch.stack(codes, dim=-1)\n\n        # Non-AR Decoders\n        y_emb = self.nar_audio_embeddings[0](\n            y[:, int(self.ar_audio_prepend_bos) :]\n        )\n\n        if self.prefix_mode in [2, 4]:  # Exclude enrolled_phonemes\n            enrolled_len = enroll_x_lens.max().item()\n            # SOS + Synthesis Text + EOS\n            text = torch.concat(\n                [\n                    text[:, :1],\n                    text[:, enrolled_len - 1 :],\n                ],\n                dim=1,\n            )\n            text_len = text_len - (enrolled_len - 2)\n            assert text.shape[0] == 1\n\n        x = self.nar_text_embedding(text)\n        # Add language embedding\n        prompt_language_id = torch.LongTensor(np.array([self.language_ID[prompt_language]])).to(x.device)\n        if isinstance(text_language, str):\n            text_language_id = torch.LongTensor(np.array([self.language_ID[text_language]])).to(x.device)\n        elif isinstance(text_language, List):\n            text_language_id = torch.LongTensor(np.array([self.language_ID[tl] for tl in text_language])).to(x.device)\n        x[:, :enroll_x_lens, :] += self.nar_language_embedding(prompt_language_id)\n        x[:, enroll_x_lens:, :] += self.nar_language_embedding(text_language_id)\n        x = self.nar_text_prenet(x)\n        x = self.nar_text_position(x)\n\n        if self.prefix_mode == 0:\n            for i, (predict_layer, embedding_layer) in enumerate(\n                zip(\n                    self.nar_predict_layers,\n                    self.nar_audio_embeddings[1:],\n                )\n            ):\n                y_pos = self.nar_audio_prenet(y_emb)\n                y_pos = self.nar_audio_position(y_pos)\n                xy_pos = torch.concat([x, y_pos], dim=1)\n\n                xy_dec, _ = self.nar_decoder(\n                    (xy_pos, self.nar_stage_embeddings[i].weight)\n                )\n                logits = predict_layer(xy_dec[:, text_len + prefix_len :])\n\n                samples = torch.argmax(logits, dim=-1)\n                codes.append(samples)\n\n                if i < self.num_quantizers - 2:\n                    y_emb[:, :prefix_len] += embedding_layer(\n                        prompts[..., i + 1]\n                    )\n                    y_emb[:, prefix_len:] += embedding_layer(samples)\n        else:\n            for j in range(1, self.num_quantizers):\n                y_emb[:, :prefix_len] += self.nar_audio_embeddings[j](\n                    prompts[..., j]\n                )\n\n            for i, (predict_layer, embedding_layer) in enumerate(\n                zip(\n                    self.nar_predict_layers,\n                    self.nar_audio_embeddings[1:],\n                )\n            ):\n                y_pos = self.nar_audio_prenet(y_emb)\n                y_pos = self.nar_audio_position(y_pos)\n                xy_pos = torch.concat([x, y_pos], dim=1)\n\n                xy_dec, _ = self.nar_decoder(\n                    (xy_pos, self.nar_stage_embeddings[i].weight)\n                )\n                logits = predict_layer(xy_dec[:, text_len + prefix_len :])\n\n                samples = torch.argmax(logits, dim=-1)\n                codes.append(samples)\n\n                if i < self.num_quantizers - 2:\n                    y_emb[:, prefix_len:] += embedding_layer(samples)\n\n        assert len(codes) == self.num_quantizers\n        return torch.stack(codes, dim=-1)\n\n    def continual(\n        self,\n        x: torch.Tensor,\n        x_lens: torch.Tensor,\n        y: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n          x:\n            A 2-D tensor of shape (1, S).\n          x_lens:\n            A 1-D tensor of shape (1,). It contains the number of tokens in `x`\n            before padding.\n          y:\n            A 3-D tensor of shape (1, T, 8).\n        Returns:\n          Return the predicted audio code matrix.\n        \"\"\"\n        assert x.ndim == 2, x.shape\n        assert x_lens.ndim == 1, x_lens.shape\n        assert y.ndim == 3, y.shape\n        assert y.shape[0] == 1, y.shape\n\n        assert torch.all(x_lens > 0)\n        assert self.num_quantizers == 8\n\n        # NOTE: x has been padded in TextTokenCollater\n        text = x\n        x = self.ar_text_embedding(text)\n        x = self.ar_text_prenet(x)\n        x = self.ar_text_position(x)\n\n        text_len = x_lens.max()\n\n        prefix_len = min(int(y.shape[1] * 0.5), 3 * 75)\n\n        # AR Decoder\n        prompts = y[:, :prefix_len]\n\n        codes = [y[:, prefix_len:, 0]]\n        # Non-AR Decoders\n        x = self.nar_text_embedding(text)\n        x = self.nar_text_prenet(x)\n        x = self.nar_text_position(x)\n\n        y_emb = self.nar_audio_embeddings[0](y[..., 0])\n\n        if self.prefix_mode == 0:\n            for i, (predict_layer, embedding_layer) in enumerate(\n                zip(\n                    self.nar_predict_layers,\n                    self.nar_audio_embeddings[1:],\n                )\n            ):\n                y_pos = self.nar_audio_position(y_emb)\n                y_pos = self.nar_audio_prenet(y_pos)\n                xy_pos = torch.concat([x, y_pos], dim=1)\n\n                xy_dec, _ = self.nar_decoder(\n                    (xy_pos, self.nar_stage_embeddings[i].weight)\n                )\n                logits = predict_layer(xy_dec[:, text_len + prefix_len :])\n\n                samples = torch.argmax(logits, dim=-1)\n                codes.append(samples)\n\n                if i < 6:\n                    y_emb[:, :prefix_len] += embedding_layer(\n                        prompts[..., i + 1]\n                    )\n                    y_emb[:, prefix_len:] += embedding_layer(samples)\n        else:\n            for j in range(1, 8):\n                y_emb[:, :prefix_len] += self.nar_audio_embeddings[j](\n                    prompts[..., j]\n                )\n\n            for i, (predict_layer, embedding_layer) in enumerate(\n                zip(\n                    self.nar_predict_layers,\n                    self.nar_audio_embeddings[1:],\n                )\n            ):\n                y_pos = self.nar_audio_prenet(y_emb)\n                y_pos = self.nar_audio_position(y_pos)\n                xy_pos = torch.concat([x, y_pos], dim=1)\n\n                xy_dec, _ = self.nar_decoder(\n                    (xy_pos, self.nar_stage_embeddings[i].weight)\n                )\n                logits = predict_layer(xy_dec[:, text_len + prefix_len :])\n\n                samples = torch.argmax(logits, dim=-1)\n                codes.append(samples)\n\n                if i < 6:\n                    y_emb[:, prefix_len:] += embedding_layer(samples)\n\n        assert len(codes) == 8\n        return torch.stack(codes, dim=-1)\n\n\n# https://github.com/microsoft/unilm/blob/master/xtune/src/transformers/modeling_utils.py\ndef top_k_top_p_filtering(\n    logits, top_k=0, top_p=1.0, filter_value=-float(\"Inf\"), min_tokens_to_keep=1\n):\n    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n    Args:\n        logits: logits distribution shape (batch size, vocabulary size)\n        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n        Make sure we keep at least min_tokens_to_keep per batch example in the output\n    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"\n    if top_k > 0:\n        top_k = min(\n            max(top_k, min_tokens_to_keep), logits.size(-1)\n        )  # Safety check\n        # Remove all tokens with a probability less than the last token of the top-k\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p < 1.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(\n            F.softmax(sorted_logits, dim=-1), dim=-1\n        )\n\n        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n        sorted_indices_to_remove = cumulative_probs > top_p\n        if min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n        # Shift the indices to the right to keep also the first token above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n            ..., :-1\n        ].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        # scatter sorted tensors to original indexing\n        indices_to_remove = sorted_indices_to_remove.scatter(\n            1, sorted_indices, sorted_indices_to_remove\n        )\n        logits[indices_to_remove] = filter_value\n    return logits",
            "Examples": [
                "\n"
            ]
        }
    ]
}