{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/generative-models",
    "API_Calls": [
        {
            "Name": "run_img2vid_preprocess_video_call",
            "Description": "call api to preprocess video and run image to video generation",
            "Code": "import os\nimport sys\nfrom glob import glob\nfrom typing import List, Optional\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.realpath(os.path.join(os.path.dirname(__file__), \"../../\")))\nimport numpy as np\nimport torch\nfrom fire import Fire\nfrom scripts.demo.sv4d_helpers import (\n    load_model,\n    preprocess_video,\n    read_video,\n    run_img2vid,\n    save_video,\n)\nfrom sgm.modules.encoders.modules import VideoPredictionEmbedderWithEncoder\n\nsv4d2_configs = {\n    \"sv4d2\": {\n        \"T\": 12,  # number of frames per sample\n        \"V\": 4,  # number of views per sample\n        \"model_config\": \"scripts/sampling/configs/sv4d2.yaml\",\n        \"version_dict\": {\n            \"T\": 12 * 4,\n            \"options\": {\n                \"discretization\": 1,\n                \"cfg\": 2.0,\n                \"min_cfg\": 2.0,\n                \"num_views\": 4,\n                \"sigma_min\": 0.002,\n                \"sigma_max\": 700.0,\n                \"rho\": 7.0,\n                \"guider\": 2,\n                \"force_uc_zero_embeddings\": [\n                    \"cond_frames\",\n                    \"cond_frames_without_noise\",\n                    \"cond_view\",\n                    \"cond_motion\",\n                ],\n                \"additional_guider_kwargs\": {\n                    \"additional_cond_keys\": [\"cond_view\", \"cond_motion\"]\n                },\n            },\n        },\n    },\n    \"sv4d2_8views\": {\n        \"T\": 5,  # number of frames per sample\n        \"V\": 8,  # number of views per sample\n        \"model_config\": \"scripts/sampling/configs/sv4d2_8views.yaml\",\n        \"version_dict\": {\n            \"T\": 5 * 8,\n            \"options\": {\n                \"discretization\": 1,\n                \"cfg\": 2.5,\n                \"min_cfg\": 1.5,\n                \"num_views\": 8,\n                \"sigma_min\": 0.002,\n                \"sigma_max\": 700.0,\n                \"rho\": 7.0,\n                \"guider\": 5,\n                \"force_uc_zero_embeddings\": [\n                    \"cond_frames\",\n                    \"cond_frames_without_noise\",\n                    \"cond_view\",\n                    \"cond_motion\",\n                ],\n                \"additional_guider_kwargs\": {\n                    \"additional_cond_keys\": [\"cond_view\", \"cond_motion\"]\n                },\n            },\n        },\n    },\n}\n\n\ndef sample(\n    input_path: str = \"assets/sv4d_videos/camel.gif\",  # Can either be image file or folder with image files\n    model_path: Optional[str] = \"checkpoints/sv4d2.safetensors\",\n    output_folder: Optional[str] = \"outputs\",\n    num_steps: Optional[int] = 50,\n    img_size: int = 576,  # image resolution\n    n_frames: int = 21,  # number of input and output video frames\n    seed: int = 23,\n    encoding_t: int = 8,  # Number of frames encoded at a time! This eats most VRAM. Reduce if necessary.\n    decoding_t: int = 4,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n    device: str = \"cuda\",\n    elevations_deg: Optional[List[float]] = 0.0,\n    azimuths_deg: Optional[List[float]] = None,\n    image_frame_ratio: Optional[float] = 0.9,\n    verbose: Optional[bool] = False,\n    remove_bg: bool = False,\n):\n    \"\"\"\n    Simple script to generate multiple novel-view videos conditioned on a video `input_path` or multiple frames, one for each\n    image file in folder `input_path`. If you run out of VRAM, try decreasing `decoding_t` and `encoding_t`.\n    \"\"\"\n    # Set model config\n    assert os.path.basename(model_path) in [\n        \"sv4d2.safetensors\",\n        \"sv4d2_8views.safetensors\",\n    ]\n    sv4d2_model = os.path.splitext(os.path.basename(model_path))[0]\n    config = sv4d2_configs[sv4d2_model]\n    print(sv4d2_model, config)\n    T = config[\"T\"]\n    V = config[\"V\"]\n    model_config = config[\"model_config\"]\n    version_dict = config[\"version_dict\"]\n    F = 8  # vae factor to downsize image->latent\n    C = 4\n    H, W = img_size, img_size\n    n_views = V + 1  # number of output video views (1 input view + 8 novel views)\n    subsampled_views = np.arange(n_views)\n    version_dict[\"H\"] = H\n    version_dict[\"W\"] = W\n    version_dict[\"C\"] = C\n    version_dict[\"f\"] = F\n    version_dict[\"options\"][\"num_steps\"] = num_steps\n\n    torch.manual_seed(seed)\n    output_folder = os.path.join(output_folder, sv4d2_model)\n    os.makedirs(output_folder, exist_ok=True)\n\n    # Read input video frames i.e. images at view 0\n    print(f\"Reading {input_path}\")\n    base_count = len(glob(os.path.join(output_folder, \"*.mp4\"))) // n_views\n    processed_input_path = preprocess_video(\n        input_path,\n        remove_bg=remove_bg,\n        n_frames=n_frames,\n        W=W,\n        H=H,\n        output_folder=output_folder,\n        image_frame_ratio=image_frame_ratio,\n        base_count=base_count,\n    )\n    images_v0 = read_video(processed_input_path, n_frames=n_frames, device=device)\n    images_t0 = torch.zeros(n_views, 3, H, W).float().to(device)\n\n    # Get camera viewpoints\n    if isinstance(elevations_deg, float) or isinstance(elevations_deg, int):\n        elevations_deg = [elevations_deg] * n_views\n    assert (\n        len(elevations_deg) == n_views\n    ), f\"Please provide 1 value, or a list of {n_views} values for elevations_deg! Given {len(elevations_deg)}\"\n    if azimuths_deg is None:\n        # azimuths_deg = np.linspace(0, 360, n_views + 1)[1:] % 360\n        azimuths_deg = (\n            np.array([0, 60, 120, 180, 240])\n            if sv4d2_model == \"sv4d2\"\n            else np.array([0, 30, 75, 120, 165, 210, 255, 300, 330])\n        )\n    assert (\n        len(azimuths_deg) == n_views\n    ), f\"Please provide a list of {n_views} values for azimuths_deg! Given {len(azimuths_deg)}\"\n    polars_rad = np.array([np.deg2rad(90 - e) for e in elevations_deg])\n    azimuths_rad = np.array(\n        [np.deg2rad((a - azimuths_deg[-1]) % 360) for a in azimuths_deg]\n    )\n\n    # Initialize image matrix\n    img_matrix = [[None] * n_views for _ in range(n_frames)]\n    for i, v in enumerate(subsampled_views):\n        img_matrix[0][i] = images_t0[v].unsqueeze(0)\n    for t in range(n_frames):\n        img_matrix[t][0] = images_v0[t]\n\n    # Load SV4D++ model\n    model, _ = load_model(\n        model_config,\n        device,\n        version_dict[\"T\"],\n        num_steps,\n        verbose,\n        model_path,\n    )\n    model.en_and_decode_n_samples_a_time = decoding_t\n    for emb in model.conditioner.embedders:\n        if isinstance(emb, VideoPredictionEmbedderWithEncoder):\n            emb.en_and_decode_n_samples_a_time = encoding_t\n\n    # Sampling novel-view videos\n    v0 = 0\n    view_indices = np.arange(V) + 1\n    t0_list = (\n        range(0, n_frames, T)\n        if sv4d2_model == \"sv4d2\"\n        else range(0, n_frames - T + 1, T - 1)\n    )\n    for t0 in tqdm(t0_list):\n        if t0 + T > n_frames:\n            t0 = n_frames - T\n        frame_indices = t0 + np.arange(T)\n        print(f\"Sampling frames {frame_indices}\")\n        image = img_matrix[t0][v0]\n        cond_motion = torch.cat([img_matrix[t][v0] for t in frame_indices], 0)\n        cond_view = torch.cat([img_matrix[t0][v] for v in view_indices], 0)\n        polars = polars_rad[subsampled_views[1:]][None].repeat(T, 0).flatten()\n        azims = azimuths_rad[subsampled_views[1:]][None].repeat(T, 0).flatten()\n        polars = (polars - polars_rad[v0] + torch.pi / 2) % (torch.pi * 2)\n        azims = (azims - azimuths_rad[v0]) % (torch.pi * 2)\n        cond_mv = False if t0 == 0 else True\n        samples = run_img2vid(\n            version_dict,\n            model,\n            image,\n            seed,\n            polars,\n            azims,\n            cond_motion,\n            cond_view,\n            decoding_t,\n            cond_mv=cond_mv,\n        )\n        samples = samples.view(T, V, 3, H, W)\n\n        for i, t in enumerate(frame_indices):\n            for j, v in enumerate(view_indices):\n                img_matrix[t][v] = samples[i, j][None] * 2 - 1\n\n    # Save output videos\n    for v in view_indices:\n        vid_file = os.path.join(output_folder, f\"{base_count:06d}_v{v:03d}.mp4\")\n        print(f\"Saving {vid_file}\")\n        save_video(\n            vid_file,\n            [img_matrix[t][v] for t in range(n_frames) if img_matrix[t][v] is not None],\n        )\n\n\nif __name__ == \"__main__\":\n    Fire(sample)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/generative-models/scripts/sampling/simple_video_sample_4d2.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "preprocess_video_run_img2vid",
            "Description": "some api of image to vid",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/generative-models/scripts/sampling/simple_video_sample_4d2.py",
            "Implementation": "import math\nimport os\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport cv2\nimport imageio\nimport numpy as np\nimport torch\nimport torchvision.transforms as TT\nfrom einops import rearrange, repeat\nfrom omegaconf import ListConfig, OmegaConf\nfrom PIL import Image, ImageSequence\nfrom rembg import remove\nfrom scripts.util.detection.nsfw_and_watermark_dectection import DeepFloydDataFiltering\nfrom sgm.modules.autoencoding.temporal_ae import VideoDecoder\nfrom sgm.modules.diffusionmodules.guiders import (\n    LinearPredictionGuider,\n    SpatiotemporalPredictionGuider,\n    TrapezoidPredictionGuider,\n    TrianglePredictionGuider,\n    VanillaCFG,\n)\nfrom sgm.modules.diffusionmodules.sampling import (\n    DPMPP2MSampler,\n    DPMPP2SAncestralSampler,\n    EulerAncestralSampler,\n    EulerEDMSampler,\n    HeunEDMSampler,\n    LinearMultistepSampler,\n)\nfrom sgm.util import default, instantiate_from_config\nfrom torch import autocast\nfrom torchvision.transforms import ToTensor\n\n\ndef load_module_gpu(model):\n    model.cuda()\n\n\ndef unload_module_gpu(model):\n    model.cpu()\n    torch.cuda.empty_cache()\n\n\ndef initial_model_load(model):\n    model.model.half()\n    return model\n\n\ndef preprocess_video(\n    input_path,\n    remove_bg=False,\n    n_frames=21,\n    W=576,\n    H=576,\n    output_folder=None,\n    image_frame_ratio=0.917,\n    base_count=0,\n):\n    print(f\"preprocess {input_path}\")\n    if output_folder is None:\n        output_folder = os.path.dirname(input_path)\n    path = Path(input_path)\n    is_video_file = False\n    all_img_paths = []\n    if path.is_file():\n        if any([input_path.endswith(x) for x in [\".gif\", \".mp4\"]]):\n            is_video_file = True\n        else:\n            raise ValueError(\"Path is not a valid video file.\")\n    elif path.is_dir():\n        all_img_paths = sorted(\n            [\n                f\n                for f in path.iterdir()\n                if f.is_file() and f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n            ]\n        )[:n_frames]\n    elif \"*\" in input_path:\n        all_img_paths = sorted(glob(input_path))[:n_frames]\n    else:\n        raise ValueError\n\n    if is_video_file and input_path.endswith(\".gif\"):\n        images = read_gif(input_path, n_frames)[:n_frames]\n    elif is_video_file and input_path.endswith(\".mp4\"):\n        images = read_mp4(input_path, n_frames)[:n_frames]\n    else:\n        print(f\"Loading {len(all_img_paths)} video frames...\")\n        images = [Image.open(img_path) for img_path in all_img_paths]\n\n    if len(images) != n_frames:\n        raise ValueError(\n            f\"Input video contains {len(images)} frames, fewer than {n_frames} frames.\"\n        )\n\n    # Remove background\n    for i, image in enumerate(images):\n        if remove_bg:\n            if image.mode == \"RGBA\":\n                pass\n            else:\n                # image.thumbnail([W, H], Image.Resampling.LANCZOS)\n                image = remove(image.convert(\"RGBA\"), alpha_matting=True)\n            images[i] = image\n\n    # Crop video frames, assume the object is already in the center of the image\n    white_thresh = 250\n    images_v0 = []\n    box_coord = [np.inf, np.inf, 0, 0]\n    for image in images:\n        image_arr = np.array(image)\n        in_w, in_h = image_arr.shape[:2]\n        original_center = (in_w // 2, in_h // 2)\n        if image.mode == \"RGBA\":\n            ret, mask = cv2.threshold(\n                np.array(image.split()[-1]), 0, 255, cv2.THRESH_BINARY\n            )\n        else:\n            # assume the input image has white background\n            ret, mask = cv2.threshold(\n                (np.array(image).mean(-1) <= white_thresh).astype(np.uint8) * 255,\n                0,\n                255,\n                cv2.THRESH_BINARY,\n            )\n\n        x, y, w, h = cv2.boundingRect(mask)\n        box_coord[0] = min(box_coord[0], x)\n        box_coord[1] = min(box_coord[1], y)\n        box_coord[2] = max(box_coord[2], x + w)\n        box_coord[3] = max(box_coord[3], y + h)\n    box_square = max(\n        original_center[0] - box_coord[0], original_center[1] - box_coord[1]\n    )\n    box_square = max(box_square, box_coord[2] - original_center[0])\n    box_square = max(box_square, box_coord[3] - original_center[1])\n    x, y = max(0, original_center[0] - box_square), max(\n        0, original_center[1] - box_square\n    )\n    w, h = min(image_arr.shape[0], 2 * box_square), min(\n        image_arr.shape[1], 2 * box_square\n    )\n    box_size = box_square * 2\n\n    for image in images:\n        if image.mode == \"RGB\":\n            image = image.convert(\"RGBA\")\n        image_arr = np.array(image)\n        side_len = (\n            int(box_size / image_frame_ratio) if image_frame_ratio is not None else in_w\n        )\n        padded_image = np.zeros((side_len, side_len, 4), dtype=np.uint8)\n        center = side_len // 2\n        box_size_w = min(w, box_size)\n        box_size_h = min(h, box_size)\n        padded_image[\n            center - box_size_w // 2 : center - box_size_w // 2 + box_size_w,\n            center - box_size_h // 2 : center - box_size_h // 2 + box_size_h,\n        ] = image_arr[x : x + w, y : y + h]\n\n        rgba = Image.fromarray(padded_image).resize((W, H), Image.LANCZOS)\n        # rgba = image.resize((W, H), Image.LANCZOS)\n        rgba_arr = np.array(rgba) / 255.0\n        rgb = rgba_arr[..., :3] * rgba_arr[..., -1:] + (1 - rgba_arr[..., -1:])\n        image = (rgb * 255).astype(np.uint8)\n\n        images_v0.append(image)\n\n    processed_file = os.path.join(output_folder, f\"{base_count:06d}_process_input.mp4\")\n    imageio.mimwrite(processed_file, images_v0, fps=10)\n    return processed_file\n\n\ndef do_sample(\n    model,\n    sampler,\n    value_dict,\n    num_samples,\n    H,\n    W,\n    C,\n    F,\n    force_uc_zero_embeddings: Optional[List] = None,\n    force_cond_zero_embeddings: Optional[List] = None,\n    batch2model_input: List = None,\n    return_latents=False,\n    filter=None,\n    T=None,\n    additional_batch_uc_fields=None,\n    decoding_t=None,\n):\n    force_uc_zero_embeddings = default(force_uc_zero_embeddings, [])\n    batch2model_input = default(batch2model_input, [])\n    additional_batch_uc_fields = default(additional_batch_uc_fields, [])\n\n    precision_scope = autocast\n    with torch.no_grad():\n        with precision_scope(\"cuda\"):\n            with model.ema_scope():\n                if T is not None:\n                    num_samples = [num_samples, T]\n                else:\n                    num_samples = [num_samples]\n\n                load_module_gpu(model.conditioner)\n                batch, batch_uc = get_batch(\n                    get_unique_embedder_keys_from_conditioner(model.conditioner),\n                    value_dict,\n                    num_samples,\n                    T=T,\n                    additional_batch_uc_fields=additional_batch_uc_fields,\n                )\n                c, uc = model.conditioner.get_unconditional_conditioning(\n                    batch,\n                    batch_uc=batch_uc,\n                    force_uc_zero_embeddings=force_uc_zero_embeddings,\n                    force_cond_zero_embeddings=force_cond_zero_embeddings,\n                )\n                unload_module_gpu(model.conditioner)\n\n                for k in c:\n                    if not k == \"crossattn\":\n                        c[k], uc[k] = map(\n                            lambda y: y[k][: math.prod(num_samples)].to(\"cuda\"), (c, uc)\n                        )\n\n                if value_dict[\"image_only_indicator\"] == 0:\n                    c[\"cond_view\"] *= 0\n                    uc[\"cond_view\"] *= 0\n\n                additional_model_inputs = {}\n                for k in batch2model_input:\n                    if k == \"image_only_indicator\":\n                        assert T is not None\n\n                        if isinstance(\n                            sampler.guider,\n                            (\n                                VanillaCFG,\n                                LinearPredictionGuider,\n                                TrianglePredictionGuider,\n                                TrapezoidPredictionGuider,\n                                SpatiotemporalPredictionGuider,\n                            ),\n                        ):\n                            additional_model_inputs[k] = (\n                                torch.zeros(num_samples[0] * 2, num_samples[1]).to(\n                                    \"cuda\"\n                                )\n                                + value_dict[\"image_only_indicator\"]\n                            )\n                        else:\n                            additional_model_inputs[k] = torch.zeros(num_samples).to(\n                                \"cuda\"\n                            )\n                    else:\n                        additional_model_inputs[k] = batch[k]\n\n                shape = (math.prod(num_samples), C, H // F, W // F)\n                randn = torch.randn(shape).to(\"cuda\")\n\n                def denoiser(input, sigma, c):\n                    return model.denoiser(\n                        model.model, input, sigma, c, **additional_model_inputs\n                    )\n\n                load_module_gpu(model.model)\n                load_module_gpu(model.denoiser)\n                samples_z = sampler(denoiser, randn, cond=c, uc=uc)\n                unload_module_gpu(model.denoiser)\n                unload_module_gpu(model.model)\n\n                load_module_gpu(model.first_stage_model)\n                if isinstance(model.first_stage_model.decoder, VideoDecoder):\n                    samples_x = model.decode_first_stage(\n                        samples_z, timesteps=default(decoding_t, T)\n                    )\n                else:\n                    samples_x = model.decode_first_stage(samples_z)\n                samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n                unload_module_gpu(model.first_stage_model)\n\n                if filter is not None:\n                    samples = filter(samples)\n\n                if return_latents:\n                    return samples, samples_z\n\n                return samples\n\n\ndef run_img2vid(\n    version_dict,\n    model,\n    image,\n    seed=23,\n    polar_rad=[10] * 21,\n    azim_rad=np.linspace(0, 360, 21 + 1)[1:],\n    cond_motion=None,\n    cond_view=None,\n    decoding_t=None,\n    cond_mv=True,\n):\n    options = version_dict[\"options\"]\n    H = version_dict[\"H\"]\n    W = version_dict[\"W\"]\n    T = version_dict[\"T\"]\n    C = version_dict[\"C\"]\n    F = version_dict[\"f\"]\n    init_dict = {\n        \"orig_width\": 576,\n        \"orig_height\": 576,\n        \"target_width\": W,\n        \"target_height\": H,\n    }\n    ukeys = set(get_unique_embedder_keys_from_conditioner(model.conditioner))\n\n    value_dict = init_embedder_options_no_st(\n        ukeys,\n        init_dict,\n        negative_prompt=options.get(\"negative_promt\", \"\"),\n        prompt=\"A 3D model.\",\n    )\n    if \"fps\" not in ukeys:\n        value_dict[\"fps\"] = 6\n\n    value_dict[\"is_image\"] = 0\n    value_dict[\"is_webvid\"] = 0\n    if cond_mv:\n        value_dict[\"image_only_indicator\"] = 1.0\n    else:\n        value_dict[\"image_only_indicator\"] = 0.0\n\n    cond_aug = 0.00\n    if cond_motion is not None:\n        value_dict[\"cond_frames_without_noise\"] = cond_motion\n        value_dict[\"cond_frames\"] = (\n            cond_motion[:, None].repeat(1, cond_view.shape[0], 1, 1, 1).flatten(0, 1)\n        )\n    else:\n        value_dict[\"cond_frames_without_noise\"] = image\n        value_dict[\"cond_frames\"] = image + cond_aug * torch.randn_like(image)\n    value_dict[\"cond_aug\"] = cond_aug\n    value_dict[\"polar_rad\"] = polar_rad\n    value_dict[\"azimuth_rad\"] = azim_rad\n    value_dict[\"rotated\"] = False\n    value_dict[\"cond_motion\"] = cond_motion\n    value_dict[\"cond_view\"] = cond_view\n\n    # seed_everything(seed)\n\n    options[\"num_frames\"] = T\n    sampler, num_rows, num_cols = init_sampling_no_st(options=options)\n    num_samples = num_rows * num_cols\n\n    samples = do_sample(\n        model,\n        sampler,\n        value_dict,\n        num_samples,\n        H,\n        W,\n        C,\n        F,\n        T=T,\n        batch2model_input=[\"num_video_frames\", \"image_only_indicator\"],\n        force_uc_zero_embeddings=options.get(\"force_uc_zero_embeddings\", None),\n        force_cond_zero_embeddings=options.get(\"force_cond_zero_embeddings\", None),\n        return_latents=False,\n        decoding_t=decoding_t,\n    )\n\n    return samples\n\n\ndef load_model(\n    config: str,\n    device: str,\n    num_frames: int,\n    num_steps: int,\n    verbose: bool = False,\n    ckpt_path: str = None,\n):\n    config = OmegaConf.load(config)\n    if device == \"cuda\":\n        config.model.params.conditioner_config.params.emb_models[\n            0\n        ].params.open_clip_embedding_config.params.init_device = device\n\n    config.model.params.sampler_config.params.verbose = verbose\n    config.model.params.sampler_config.params.num_steps = num_steps\n    config.model.params.sampler_config.params.guider_config.params.num_frames = (\n        num_frames\n    )\n    if ckpt_path is not None:\n        config.model.params.ckpt_path = ckpt_path\n    if device == \"cuda\":\n        with torch.device(device):\n            model = instantiate_from_config(config.model).to(device).eval()\n    else:\n        model = instantiate_from_config(config.model).to(device).eval()\n\n    filter = DeepFloydDataFiltering(verbose=False, device=device)\n    return model, filter\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}