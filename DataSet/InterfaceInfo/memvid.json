{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/memvid",
    "API_Calls": [
        {
            "Name": "call_MemvidChat",
            "Description": "call MemvidChat to analyze the file type",
            "Code": "#!/usr/bin/env python3\n\"\"\"\nfile_chat.py - Enhanced script for testing MemvidChat with external files\n\nThis script allows you to:\n1. Create a memory video from your own files with configurable parameters\n2. Chat with the created memory using different LLM providers\n3. Store results in output/ directory to avoid contaminating the main repo\n4. Handle FAISS training issues gracefully\n5. Configure chunking and compression parameters\n\nUsage:\n    python file_chat.py --input-dir /path/to/documents --provider openai\n    python file_chat.py --files file1.txt file2.pdf --provider openai --chunk-size 2048\n    python file_chat.py --load-existing output/my_memory --provider openai\n    python file_chat.py --input-dir ~/docs --index-type Flat --codec h265\n\nExamples:\n    # Create memory from a directory and chat with Google\n    python file_chat.py --input-dir ~/Documents/research --provider openai\n\n    # Create memory with custom chunking for large documents\n    python file_chat.py --files report.pdf --chunk-size 2048 --overlap 32 --provider openai\n\n    # Use Flat index for small datasets (avoids FAISS training issues)\n    python file_chat.py --files single_doc.pdf --index-type Flat --provider openai\n\n    # Load existing memory and continue chatting\n    python file_chat.py --load-existing output/research_memory --provider openai\n\n    # Create memory with H.265 compression\n    python file_chat.py --input-dir ~/docs --codec h265 --provider anthropic\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom datetime import datetime\nimport json\n\n# Add the parent directory to the path so we can import memvid\nsys.path.insert(0, str(Path(__file__).parent.parent))  # Go up TWO levels from examples/\n\nfrom memvid import MemvidEncoder, MemvidChat\nfrom memvid.config import get_default_config, get_codec_parameters\n\ndef setup_output_dir():\n    \"\"\"Create output directory if it doesn't exist\"\"\"\n    output_dir = Path(\"output\")\n    output_dir.mkdir(exist_ok=True)\n    return output_dir\n\ndef generate_memory_name(input_source):\n    \"\"\"Generate a meaningful name for the memory files\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    if isinstance(input_source, list):\n        # Multiple files\n        base_name = f\"files_{len(input_source)}items\"\n    else:\n        # Directory\n        dir_name = Path(input_source).name\n        base_name = f\"dir_{dir_name}\"\n\n    return f\"{base_name}_{timestamp}\"\n\ndef collect_files_from_directory(directory_path, extensions=None):\n    \"\"\"Collect supported files from a directory\"\"\"\n    if extensions is None:\n        extensions = {'.txt', '.md', '.pdf', '.doc', '.docx', '.rtf', '.epub', '.html', '.htm'}\n\n    directory = Path(directory_path)\n    if not directory.exists():\n        raise ValueError(f\"Directory does not exist: {directory_path}\")\n\n    files = []\n    for ext in extensions:\n        files.extend(directory.rglob(f\"*{ext}\"))\n\n    return [str(f) for f in files if f.is_file()]\n\ndef create_memory_with_fallback(encoder, video_path, index_path):\n    \"\"\"Create memory with graceful FAISS fallback for training issues\"\"\"\n    try:\n        build_stats = encoder.build_video(str(video_path), str(index_path))\n        return build_stats\n    except Exception as e:\n        error_str = str(e)\n        if \"is_trained\" in error_str or \"IndexIVFFlat\" in error_str or \"training\" in error_str.lower():\n            print(f\"⚠️  FAISS IVF training failed: {e}\")\n            print(f\"🔄 Auto-switching to Flat index for compatibility...\")\n\n            # Override config to use Flat index\n            original_index_type = encoder.config[\"index\"][\"type\"]\n            encoder.config[\"index\"][\"type\"] = \"Flat\"\n\n            try:\n                # Recreate the index manager with Flat index\n                encoder._setup_index()\n                build_stats = encoder.build_video(str(video_path), str(index_path))\n                print(f\"✅ Successfully created memory using Flat index\")\n                return build_stats\n            except Exception as fallback_error:\n                print(f\"❌ Fallback also failed: {fallback_error}\")\n                raise\n        else:\n            raise\n\ndef create_memory_from_files(files, output_dir, memory_name, **config_overrides):\n    \"\"\"Create a memory video from a list of files with configurable parameters\"\"\"\n    print(f\"Creating memory from {len(files)} files...\")\n\n    # Start timing\n    start_time = time.time()\n\n    # Apply config overrides to default config\n    config = get_default_config()\n    for key, value in config_overrides.items():\n        if key in ['chunk_size', 'overlap']:\n            config[\"chunking\"][key] = value\n        elif key == 'index_type':\n            config[\"index\"][\"type\"] = value\n        elif key == 'codec':\n            config[key] = value\n\n    # Initialize encoder with config first (this ensures config consistency)\n    encoder = MemvidEncoder(config)\n\n    # Get the actual codec and video extension from the encoder's config\n    actual_codec = encoder.config.get(\"codec\")  # Use encoder's resolved codec\n    video_ext = get_codec_parameters(actual_codec).get(\"video_file_type\", \"mp4\")\n\n    # Import tqdm for progress bars\n    try:\n        from tqdm import tqdm\n        use_progress = True\n    except ImportError:\n        print(\"Note: Install tqdm for progress bars (pip install tqdm)\")\n        use_progress = False\n\n    processed_count = 0\n    skipped_count = 0\n\n    # Process files with progress tracking\n    file_iterator = tqdm(files, desc=\"Processing files\") if use_progress else files\n\n    for file_path in file_iterator:\n        file_path = Path(file_path)\n        if not use_progress:\n            print(f\"Processing: {file_path.name}\")\n\n        try:\n            chunk_size = config[\"chunking\"][\"chunk_size\"]\n            overlap = config[\"chunking\"][\"overlap\"]\n\n            if file_path.suffix.lower() == '.pdf':\n                encoder.add_pdf(str(file_path), chunk_size, overlap)\n            elif file_path.suffix.lower() == '.epub':\n                encoder.add_epub(str(file_path), chunk_size, overlap)\n            elif file_path.suffix.lower() in ['.html', '.htm']:\n                # Process HTML with BeautifulSoup\n                try:\n                    from bs4 import BeautifulSoup\n                except ImportError:\n                    print(f\"Warning: BeautifulSoup not available for HTML processing. Skipping {file_path.name}\")\n                    skipped_count += 1\n                    continue\n\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    soup = BeautifulSoup(f.read(), 'html.parser')\n                    for script in soup([\"script\", \"style\"]):\n                        script.decompose()\n                    text = soup.get_text()\n                    lines = (line.strip() for line in text.splitlines())\n                    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n                    clean_text = ' '.join(chunk for chunk in chunks if chunk)\n                    if clean_text.strip():\n                        encoder.add_text(clean_text, chunk_size, overlap)\n            else:\n                # Read as text file\n                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    content = f.read()\n                    if content.strip():\n                        encoder.add_text(content, chunk_size, overlap)\n\n            processed_count += 1\n\n        except Exception as e:\n            print(f\"Warning: Could not process {file_path.name}: {e}\")\n            skipped_count += 1\n            continue\n\n    processing_time = time.time() - start_time\n    print(f\"\\n📊 Processing Summary:\")\n    print(f\"  ✅ Successfully processed: {processed_count} files\")\n    print(f\"  ⚠️  Skipped: {skipped_count} files\")\n    print(f\"  ⏱️  Processing time: {processing_time:.2f} seconds\")\n\n    if processed_count == 0:\n        raise ValueError(\"No files were successfully processed\")\n\n    # Build the video (video_ext already determined from encoder config)\n    video_path = output_dir / f\"{memory_name}.{video_ext}\"\n    index_path = output_dir / f\"{memory_name}_index.json\"\n\n    print(f\"\\n🎬 Building memory video: {video_path}\")\n    print(f\"📊 Total chunks to encode: {len(encoder.chunks)}\")\n\n    encoding_start = time.time()\n\n    # Use fallback-enabled build function\n    build_stats = create_memory_with_fallback(encoder, video_path, index_path)\n\n    encoding_time = time.time() - encoding_start\n    total_time = time.time() - start_time\n\n    # Enhanced statistics\n    print(f\"\\n🎉 Memory created successfully!\")\n    print(f\"  📁 Video: {video_path}\")\n    print(f\"  📋 Index: {index_path}\")\n    print(f\"  📊 Chunks: {build_stats.get('total_chunks', 'unknown')}\")\n    print(f\"  🎞️  Frames: {build_stats.get('total_frames', 'unknown')}\")\n    print(f\"  📏 Video size: {video_path.stat().st_size / (1024 * 1024):.1f} MB\")\n    print(f\"  ⏱️  Encoding time: {encoding_time:.2f} seconds\")\n    print(f\"  ⏱️  Total time: {total_time:.2f} seconds\")\n\n    if build_stats.get('video_size_mb', 0) > 0:\n        # Calculate rough compression stats\n        total_chars = sum(len(chunk) for chunk in encoder.chunks)\n        original_size_mb = total_chars / (1024 * 1024)  # Rough estimate\n        compression_ratio = original_size_mb / build_stats['video_size_mb'] if build_stats['video_size_mb'] > 0 else 0\n        print(f\"  📦 Estimated compression ratio: {compression_ratio:.1f}x\")\n\n    # Save metadata about this memory\n    metadata = {\n        'created': datetime.now().isoformat(),\n        'source_files': files,\n        'video_path': str(video_path),\n        'index_path': str(index_path),\n        'config_used': config,\n        'processing_stats': {\n            'files_processed': processed_count,\n            'files_skipped': skipped_count,\n            'processing_time_seconds': processing_time,\n            'encoding_time_seconds': encoding_time,\n            'total_time_seconds': total_time\n        },\n        'build_stats': build_stats\n    }\n\n    metadata_path = output_dir / f\"{memory_name}_metadata.json\"\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n\n    print(f\"  📄 Metadata: {metadata_path}\")\n\n    return str(video_path), str(index_path)\n\ndef load_existing_memory(memory_path):\n    \"\"\"Load and validate existing memory from the output directory\"\"\"\n    memory_path = Path(memory_path)\n\n    # Handle different input formats\n    if memory_path.is_dir():\n        # Directory provided, look for memory files\n        # Try all possible video extensions\n        video_files = []\n        for ext in ['mp4', 'avi', 'mkv']:\n            video_files.extend(memory_path.glob(f\"*.{ext}\"))\n\n        if not video_files:\n            raise ValueError(f\"No video files found in {memory_path}\")\n\n        video_path = video_files[0]\n        # Look for corresponding index file\n        possible_index_paths = [\n            video_path.with_name(video_path.stem + '_index.json'),\n            video_path.with_suffix('.json'),\n            video_path.with_suffix('_index.json')\n        ]\n\n        index_path = None\n        for possible_path in possible_index_paths:\n            if possible_path.exists():\n                index_path = possible_path\n                break\n\n        if not index_path:\n            raise ValueError(f\"No index file found for {video_path}\")\n\n    elif memory_path.suffix in ['.mp4', '.avi', '.mkv']:\n        # Video file provided\n        video_path = memory_path\n        index_path = memory_path.with_name(memory_path.stem + '_index.json')\n\n    else:\n        # Assume it's a base name, try to find files\n        base_path = memory_path\n        video_path = None\n\n        # Try different video extensions\n        for ext in ['mp4', 'avi', 'mkv']:\n            candidate = base_path.with_suffix(f'.{ext}')\n            if candidate.exists():\n                video_path = candidate\n                break\n\n        if not video_path:\n            raise ValueError(f\"No video file found with base name: {memory_path}\")\n\n        index_path = base_path.with_suffix('_index.json')\n\n    # Validate files exist and are readable\n    if not video_path.exists():\n        raise ValueError(f\"Video file not found: {video_path}\")\n    if not index_path.exists():\n        raise ValueError(f\"Index file not found: {index_path}\")\n\n    # Validate file integrity\n    try:\n        with open(index_path, 'r') as f:\n            index_data = json.load(f)\n        chunk_count = len(index_data.get('metadata', []))\n        print(f\"✅ Index contains {chunk_count} chunks\")\n    except Exception as e:\n        raise ValueError(f\"Index file corrupted: {e}\")\n\n    # Check video file size\n    video_size_mb = video_path.stat().st_size / (1024 * 1024)\n    print(f\"✅ Video file: {video_size_mb:.1f} MB\")\n\n    print(f\"Loading existing memory:\")\n    print(f\"  📁 Video: {video_path}\")\n    print(f\"  📋 Index: {index_path}\")\n\n    return str(video_path), str(index_path)\n\ndef start_chat_session(video_path, index_path, provider='openai', model=None):\n    \"\"\"Start an interactive chat session\"\"\"\n    print(f\"\\nInitializing chat with {provider}...\")\n\n    try:\n        chat = MemvidChat(\n            video_file=video_path,\n            index_file=index_path,\n            llm_provider=provider,\n            llm_model=model\n        )\n\n        print(\"✓ Chat initialized successfully!\")\n        print(\"\\nStarting interactive session...\")\n        print(\"Commands:\")\n        print(\"  - Type your questions normally\")\n        print(\"  - Type 'quit' or 'exit' to end\")\n        print(\"  - Type 'clear' to clear conversation history\")\n        print(\"  - Type 'stats' to see session statistics\")\n        print(\"=\" * 50)\n\n        # Start interactive chat\n        while True:\n            try:\n                user_input = input(\"\\nYou: \").strip()\n\n                if user_input.lower() in ['quit', 'exit', 'q']:\n                    # Export conversation before exiting\n                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                    export_path = Path(\"output\") / f\"conversation_{timestamp}.json\"\n                    chat.export_conversation(str(export_path))\n                    print(f\"💾 Conversation saved to: {export_path}\")\n                    print(\"Goodbye!\")\n                    break\n\n                elif user_input.lower() == 'clear':\n                    chat.clear_history()\n                    print(\"🗑️ Conversation history cleared\")\n                    continue\n\n                elif user_input.lower() == 'stats':\n                    stats = chat.get_stats()\n                    print(f\"📊 Session stats: {stats}\")\n                    continue\n\n                if not user_input:\n                    continue\n\n                # Get response (always stream for better UX)\n                chat.chat(user_input, stream=True)\n\n            except KeyboardInterrupt:\n                print(\"\\nGoodbye!\")\n                break\n            except Exception as e:\n                print(f\"Error: {e}\")\n\n    except Exception as e:\n        print(f\"Error initializing chat: {e}\")\n        return False\n\n    return True\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Chat with your documents using MemVid with enhanced configuration options\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=__doc__\n    )\n\n    # Input options (mutually exclusive)\n    input_group = parser.add_mutually_exclusive_group(required=True)\n    input_group.add_argument(\n        '--input-dir',\n        help='Directory containing documents to process'\n    )\n    input_group.add_argument(\n        '--files',\n        nargs='+',\n        help='Specific files to process'\n    )\n    input_group.add_argument(\n        '--load-existing',\n        help='Load existing memory (provide path to video file or directory)'\n    )\n\n    # LLM options\n    parser.add_argument(\n        '--provider',\n        choices=['openai', 'google', 'anthropic'],\n        default='openai',\n        help='LLM provider to use (default: openai)'\n    )\n    parser.add_argument(\n        '--model',\n        help='Specific model to use (uses provider defaults if not specified)'\n    )\n\n    # Memory options\n    parser.add_argument(\n        '--memory-name',\n        help='Custom name for the memory files (auto-generated if not provided)'\n    )\n\n    # Processing configuration options\n    parser.add_argument(\n        '--chunk-size',\n        type=int,\n        help='Override default chunk size (e.g., 2048, 4096)'\n    )\n    parser.add_argument(\n        '--overlap',\n        type=int,\n        help='Override default chunk overlap (e.g., 16, 32, 64)'\n    )\n    parser.add_argument(\n        '--index-type',\n        choices=['Flat', 'IVF'],\n        help='FAISS index type (Flat for small datasets, IVF for large datasets)'\n    )\n    parser.add_argument(\n        '--codec',\n        choices=['h264', 'h265', 'mp4v'],\n        help='Video codec to use for compression'\n    )\n\n    # File processing options\n    parser.add_argument(\n        '--extensions',\n        nargs='+',\n        default=['.txt', '.md', '.pdf', '.doc', '.docx', '.epub', '.html', '.htm'],\n        help='File extensions to include when processing directories'\n    )\n\n    args = parser.parse_args()\n\n    # Setup output directory\n    output_dir = setup_output_dir()\n\n    try:\n        # Get or create memory\n        if args.load_existing:\n            video_path, index_path = load_existing_memory(args.load_existing)\n        else:\n            # Collect files\n            if args.input_dir:\n                files = collect_files_from_directory(args.input_dir, set(args.extensions))\n                if not files:\n                    print(f\"No supported files found in {args.input_dir}\")\n                    return 1\n                print(f\"Found {len(files)} files to process\")\n                input_source = args.input_dir\n            else:\n                files = args.files\n                for f in files:\n                    if not Path(f).exists():\n                        print(f\"File not found: {f}\")\n                        return 1\n                input_source = files\n\n            # Generate memory name\n            memory_name = args.memory_name or generate_memory_name(input_source)\n\n            # Build config overrides from command line arguments\n            config_overrides = {}\n            if args.chunk_size:\n                config_overrides['chunk_size'] = args.chunk_size\n            if args.overlap:\n                config_overrides['overlap'] = args.overlap\n            if args.index_type:\n                config_overrides['index_type'] = args.index_type\n            if args.codec:\n                config_overrides['codec'] = args.codec\n\n            # Show what defaults are being used if no overrides provided\n            if not config_overrides:\n                default_config = get_default_config()\n                print(f\"📋 Using default configuration:\")\n                print(f\"   Chunk size: {default_config['chunking']['chunk_size']}\")\n                print(f\"   Overlap: {default_config['chunking']['overlap']}\")\n                print(f\"   Index type: {default_config['index']['type']}\")\n                print(f\"   Codec: {default_config.get('codec', 'h265')}\")\n\n            # Create memory with configuration\n            video_path, index_path = create_memory_from_files(\n                files, output_dir, memory_name, **config_overrides\n            )\n\n        # Start chat session\n        success = start_chat_session(video_path, index_path, args.provider, args.model)\n        return 0 if success else 1\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return 1\n\nif __name__ == \"__main__\":\n    sys.exit(main())",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/memvid/examples/file_chat.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "MemvidChat",
            "Description": "MemvidChat impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/memvid/memvid/chat.py",
            "Implementation": "\"\"\"\nMemvidChat - Enhanced conversational interface with multi-provider LLM support\n\"\"\"\n\nimport json\nimport os\nimport logging\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom .llm_client import LLMClient\nfrom .retriever import MemvidRetriever\nfrom .config import get_default_config\n\nlogger = logging.getLogger(__name__)\n\n\nclass MemvidChat:\n    \"\"\"Enhanced MemvidChat with multi-provider LLM support\"\"\"\n\n    def __init__(\n            self,\n            video_file: str,\n            index_file: str,\n            llm_provider: str = 'google',\n            llm_model: str = None,\n            llm_api_key: str = None,\n            config: Optional[Dict] = None,\n            retriever_kwargs: Dict = None\n    ):\n        \"\"\"\n        Initialize MemvidChat with flexible LLM provider support\n\n        Args:\n            video_file: Path to the video memory file\n            index_file: Path to the index JSON file\n            llm_provider: LLM provider ('openai', 'google', 'anthropic')\n            llm_model: Model name (uses provider defaults if None)\n            llm_api_key: API key (uses environment variables if None)\n            config: Optional configuration dictionary\n            retriever_kwargs: Additional arguments for MemvidRetriever\n        \"\"\"\n        self.video_file = video_file\n        self.index_file = index_file\n        self.config = config or get_default_config()\n\n        # Initialize retriever\n        retriever_kwargs = retriever_kwargs or {}\n        self.retriever = MemvidRetriever(video_file, index_file, self.config)\n\n        # Initialize LLM client\n        try:\n            self.llm_client = LLMClient(\n                provider=llm_provider,\n                model=llm_model,\n                api_key=llm_api_key\n            )\n            self.llm_provider = llm_provider\n            logger.info(f\"✓ Initialized {llm_provider} LLM client\")\n        except Exception as e:\n            logger.error(f\"✗ Failed to initialize LLM client: {e}\")\n            self.llm_client = None\n            self.llm_provider = None\n\n        # Chat configuration\n        self.context_chunks = self.config.get(\"chat\", {}).get(\"context_chunks\", 5)\n        self.max_history = self.config.get(\"chat\", {}).get(\"max_history\", 10)\n\n        # Chat history\n        self.conversation_history = []\n        self.session_id = None\n        self.system_prompt = None\n\n    def start_session(self, system_prompt: str = None, session_id: str = None):\n        \"\"\"Start a new chat session with optional system prompt\"\"\"\n        self.conversation_history = []\n        self.session_id = session_id or f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\n        if system_prompt:\n            self.system_prompt = system_prompt\n        else:\n            self.system_prompt = self._get_default_system_prompt()\n\n        logger.info(f\"Chat session started: {self.session_id}\")\n        if self.llm_provider:\n            print(f\"Using {self.llm_provider} for responses.\")\n        else:\n            print(\"LLM not available - will return context only.\")\n        print(\"-\" * 50)\n\n    def _get_default_system_prompt(self) -> str:\n        \"\"\"Get the default system prompt\"\"\"\n        return \"\"\"You are a helpful AI assistant with access to a knowledge base stored in video format. \n\nWhen answering questions:\n1. Use the provided context from the knowledge base when relevant\n2. Be clear about what information comes from the knowledge base vs. your general knowledge\n3. If the context doesn't contain enough information, say so clearly\n4. Provide helpful, accurate, and concise responses\n\nThe context will be provided with each query based on semantic similarity to the user's question.\"\"\"\n\n    def chat(self, message: str, stream: bool = False, max_context_tokens: int = 2000) -> str:\n        \"\"\"\n        Send a message and get a response using retrieved context\n\n        Args:\n            message: User message\n            stream: Whether to stream the response\n            max_context_tokens: Maximum tokens to use for context\n        \"\"\"\n        if not self.session_id:\n            self.start_session()\n\n        if not self.llm_client:\n            return self._generate_context_only_response(message)\n\n        # Retrieve relevant context\n        context = self._get_context(message, max_context_tokens)\n\n        # Build messages for LLM\n        messages = self._build_messages(message, context)\n\n        # Add to conversation history\n        self.conversation_history.append({\"role\": \"user\", \"content\": message})\n\n        # Get response from LLM\n        if stream:\n            return self._handle_streaming_response(messages)\n        else:\n            response = self.llm_client.chat(messages)\n            if response:\n                self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n                return response\n            else:\n                return \"Sorry, I encountered an error generating a response.\"\n\n    def _get_context(self, query: str, max_tokens: int = 2000) -> str:\n        \"\"\"Retrieve relevant context from the video memory\"\"\"\n        try:\n            # Use the existing retriever's search method\n            context_chunks = self.retriever.search(query, top_k=self.context_chunks)\n\n            # Join chunks into context string\n            context = \"\\n\\n\".join([f\"[Context {i+1}]: {chunk}\"\n                                   for i, chunk in enumerate(context_chunks)])\n\n            # Rough token limiting (4 chars ≈ 1 token)\n            if len(context) > max_tokens * 4:\n                context = context[:max_tokens * 4] + \"...\"\n\n            return context\n        except Exception as e:\n            logger.error(f\"Error retrieving context: {e}\")\n            return \"\"\n\n    def _build_messages(self, message: str, context: str) -> List[Dict[str, str]]:\n        \"\"\"Build the message list for the LLM\"\"\"\n        messages = []\n\n        # Add system prompt\n        if self.system_prompt:\n            messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n\n        # Add conversation history (last few exchanges to stay within limits)\n        history_to_include = self.conversation_history[-6:]  # Last 3 exchanges\n        messages.extend(history_to_include)\n\n        # Prepare the current message with context\n        if context.strip():\n            enhanced_message = f\"\"\"Context from knowledge base:\n{context}\n\nUser question: {message}\"\"\"\n        else:\n            enhanced_message = message\n\n        messages.append({\"role\": \"user\", \"content\": enhanced_message})\n\n        return messages\n\n    def _handle_streaming_response(self, messages: List[Dict[str, str]]) -> str:\n        \"\"\"Handle streaming response from LLM\"\"\"\n        print(\"Assistant: \", end=\"\", flush=True)\n        full_response = \"\"\n\n        try:\n            for chunk in self.llm_client.chat_stream(messages):\n                print(chunk, end=\"\", flush=True)\n                full_response += chunk\n\n            print()  # New line after streaming\n\n            # Add to conversation history\n            if full_response:\n                self.conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n\n            return full_response\n\n        except Exception as e:\n            error_msg = f\"\\nError during streaming: {e}\"\n            print(error_msg)\n            return error_msg\n\n    def _generate_context_only_response(self, query: str) -> str:\n        \"\"\"Generate response without LLM (context only fallback)\"\"\"\n        try:\n            context_chunks = self.retriever.search(query, top_k=self.context_chunks)\n\n            if not context_chunks:\n                return \"I couldn't find any relevant information in the knowledge base.\"\n\n            # Check if the chunks are actually relevant\n            avg_chunk_length = sum(len(chunk) for chunk in context_chunks) / len(context_chunks)\n            if avg_chunk_length < 50:  # Likely fragment matches\n                return \"I couldn't find any relevant information about that topic in the knowledge base.\"\n\n            response = \"Based on the knowledge base, here's what I found:\\n\\n\"\n            for i, chunk in enumerate(context_chunks[:3]):  # Limit to top 3\n                response += f\"{i+1}. {chunk[:200]}...\\n\\n\" if len(chunk) > 200 else f\"{i+1}. {chunk}\\n\\n\"\n\n            return response.strip()\n\n        except Exception as e:\n            return f\"Error searching knowledge base: {e}\"\n\n    def interactive_chat(self):\n        \"\"\"Start an interactive chat session\"\"\"\n        if not self.llm_client:\n            print(\"Warning: LLM client not initialized. Will return context-only responses.\")\n\n        self.start_session()\n\n        print(\"Commands:\")\n        print(\"  - Type your questions normally\")\n        print(\"  - Type 'quit' or 'exit' to end\")\n        print(\"  - Type 'clear' to clear conversation history\")\n        print(\"  - Type 'stats' to see session statistics\")\n        print(\"=\" * 50)\n\n        while True:\n            try:\n                user_input = input(\"\\nYou: \").strip()\n\n                if user_input.lower() in ['quit', 'exit', 'q']:\n                    # Export conversation before exiting\n                    if self.conversation_history:\n                        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                        export_path = f\"output/conversation_{timestamp}.json\"\n                        self.export_conversation(export_path)\n                    print(\"Goodbye!\")\n                    break\n\n                elif user_input.lower() == 'clear':\n                    self.clear_history()\n                    continue\n\n                elif user_input.lower() == 'stats':\n                    stats = self.get_stats()\n                    print(f\"Session stats: {stats}\")\n                    continue\n\n                if not user_input:\n                    continue\n\n                # Get response (always stream for better UX if LLM available)\n                if self.llm_client:\n                    self.chat(user_input, stream=True)\n                else:\n                    response = self.chat(user_input, stream=False)\n                    print(f\"Assistant: {response}\")\n\n            except KeyboardInterrupt:\n                print(\"\\nGoodbye!\")\n                break\n            except Exception as e:\n                print(f\"Error: {e}\")\n\n    def search_context(self, query: str, top_k: int = 5) -> List[str]:\n        \"\"\"\n        Search for context without generating a response\n\n        Args:\n            query: Search query\n            top_k: Number of results\n\n        Returns:\n            List of search results\n        \"\"\"\n        try:\n            return self.retriever.search(query, top_k)\n        except Exception as e:\n            logger.error(f\"Error in search_context: {e}\")\n            return []\n\n    def clear_history(self):\n        \"\"\"Clear the conversation history\"\"\"\n        self.conversation_history = []\n        print(\"Conversation history cleared.\")\n\n    def export_conversation(self, path: str):\n        \"\"\"Export conversation history to a JSON file\"\"\"\n        # Ensure output directory exists\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n\n        conversation_data = {\n            'session_id': self.session_id,\n            'system_prompt': self.system_prompt,\n            'llm_provider': self.llm_provider,\n            'conversation': self.conversation_history,\n            'video_file': self.video_file,\n            'index_file': self.index_file,\n            'timestamp': datetime.now().isoformat(),\n            'stats': self.get_stats()\n        }\n\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(conversation_data, f, indent=2, ensure_ascii=False)\n\n        print(f\"Conversation exported to {path}\")\n\n    def load_session(self, session_file: str):\n        \"\"\"\n        Load session from file\n\n        Args:\n            session_file: Path to session file\n        \"\"\"\n        with open(session_file, 'r', encoding='utf-8') as f:\n            session_data = json.load(f)\n\n        self.session_id = session_data.get(\"session_id\")\n        self.conversation_history = session_data.get(\"conversation\", [])\n        self.system_prompt = session_data.get(\"system_prompt\", self._get_default_system_prompt())\n\n        logger.info(f\"Loaded session: {self.session_id}\")\n\n    def reset_session(self):\n        \"\"\"Reset conversation history\"\"\"\n        self.conversation_history = []\n        self.session_id = None\n        logger.info(\"Reset conversation session\")\n\n    def get_stats(self) -> Dict:\n        \"\"\"Get stats about the current session\"\"\"\n        return {\n            'session_id': self.session_id,\n            'messages_exchanged': len(self.conversation_history),\n            'llm_provider': self.llm_provider,\n            'llm_available': self.llm_client is not None,\n            'video_file': self.video_file,\n            'index_file': self.index_file,\n            'context_chunks_per_query': self.context_chunks,\n            'max_history': self.max_history\n        }\n\n\n# Backwards compatibility aliases\ndef chat_with_memory(video_file: str, index_file: str, api_key: str = None,\n                     provider: str = 'google', model: str = None):\n    \"\"\"\n    Quick chat function for backwards compatibility\n\n    Args:\n        video_file: Path to video memory file\n        index_file: Path to index file\n        api_key: LLM API key\n        provider: LLM provider\n        model: LLM model\n    \"\"\"\n    chat = MemvidChat(\n        video_file=video_file,\n        index_file=index_file,\n        llm_provider=provider,\n        llm_model=model,\n        llm_api_key=api_key\n    )\n\n    chat.interactive_chat()\n\n\ndef quick_chat(video_file: str, index_file: str, message: str,\n               provider: str = 'google', api_key: str = None) -> str:\n    \"\"\"\n    Quick single message chat\n\n    Args:\n        video_file: Path to video memory file\n        index_file: Path to index file\n        message: Message to send\n        provider: LLM provider\n        api_key: LLM API key\n\n    Returns:\n        Response from the assistant\n    \"\"\"\n    chat = MemvidChat(\n        video_file=video_file,\n        index_file=index_file,\n        llm_provider=provider,\n        llm_api_key=api_key\n    )\n\n    return chat.chat(message)",
            "Examples": [
                "\n"
            ]
        }
    ]
}