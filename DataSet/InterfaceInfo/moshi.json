{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/moshi/moshi",
    "API_Calls": [
        {
            "Name": "call_LMGen",
            "Description": "call LMGen",
            "Code": "import argparse\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport random\nimport sys\nimport time\n\nimport numpy as np\nimport sentencepiece\nimport torch\nimport sphn\n\n\nfrom .client_utils import log, AnyPrinter, Printer, RawPrinter\nfrom .conditioners import ConditionAttributes, ConditionTensors\nfrom .models import loaders, MimiModel, LMModel, LMGen\n\n\ndef seed_all(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)  # for multi-GPU setups\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = False\n\n\ndef get_condition_tensors(\n    model_type: str, lm: LMModel, batch_size: int, cfg_coef: float\n) -> ConditionTensors:\n    condition_tensors = {}\n    if lm.condition_provider is not None and lm.condition_provider.conditioners:\n        conditions: list[ConditionAttributes] | None = None\n        if model_type == \"hibiki\":\n            conditions = [\n                ConditionAttributes(text={\"description\": \"very_good\"}, tensor={})\n                for _ in range(batch_size)\n            ]\n            if cfg_coef != 1.0:\n                # Extending the conditions with the negatives for the CFG.\n                conditions += [\n                    ConditionAttributes(text={\"description\": \"very_bad\"}, tensor={})\n                    for _ in range(batch_size)\n                ]\n        else:\n            raise RuntimeError(\n                f\"Model expects conditioning but model type {model_type} is not supported.\"\n            )\n        assert conditions is not None\n        prepared = lm.condition_provider.prepare(conditions)\n        condition_tensors = lm.condition_provider(prepared)\n    return condition_tensors\n\n\n@dataclass\nclass InferenceState:\n    mimi: MimiModel\n    text_tokenizer: sentencepiece.SentencePieceProcessor\n    lm_gen: LMGen\n\n    def __init__(\n        self,\n        checkpoint_info: loaders.CheckpointInfo,\n        mimi: MimiModel,\n        text_tokenizer: sentencepiece.SentencePieceProcessor,\n        lm: LMModel,\n        batch_size: int,\n        cfg_coef: float,\n        device: str | torch.device,\n        **kwargs,\n    ):\n        self.checkpoint_info = checkpoint_info\n        model_type = checkpoint_info.model_type\n        self.model_type = model_type\n        self.mimi = mimi\n        self.text_tokenizer = text_tokenizer\n        condition_tensors = get_condition_tensors(model_type, lm, batch_size, cfg_coef)\n        self.lm_gen = LMGen(\n            lm, cfg_coef=cfg_coef, condition_tensors=condition_tensors, **kwargs\n        )\n        self.device = device\n        self.frame_size = int(self.mimi.sample_rate / self.mimi.frame_rate)\n        self.batch_size = batch_size\n        self.mimi.streaming_forever(batch_size)\n        self.lm_gen.streaming_forever(batch_size)\n        self.printer: AnyPrinter\n        if sys.stdout.isatty():\n            self.printer = Printer()\n        else:\n            self.printer = RawPrinter()\n\n    def run(self, in_pcms: torch.Tensor) -> list[tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Returns a list of tupel `(text_tokens, audio_tokens)`\"\"\"\n        out_pcms_per_item: list[list[torch.Tensor]] = [\n            [] for _ in range(self.batch_size)\n        ]\n        out_text_tokens_per_item: list[list[torch.Tensor]] = [\n            [] for _ in range(self.batch_size)\n        ]\n        # For the Hibiki translation model, we feed a special token for the end of the input stream,\n        # which corresponds to `2048` on all the codebooks of the audio stream, and wait\n        # for the EOS on the output text stream to be emitted, as indication that the model is done.\n        eos_reached: list[bool] = [False] * self.batch_size\n        need_eos_input: bool = True\n        self.printer.log(\n            \"info\",\n            \"starting inference, \"\n            f\"sampling: {self.lm_gen.use_sampling}, \"\n            f\"audio temp: {self.lm_gen.temp}, \"\n            f\"text temp: {self.lm_gen.temp_text}\",\n        )\n        device = self.lm_gen.lm_model.device\n        start_time = time.time()\n        ntokens = 0\n        first_frame = True\n        if self.model_type == \"stt\":\n            stt_config = self.checkpoint_info.stt_config\n            pad_right = stt_config.get(\"audio_delay_seconds\", 0.0)\n            pad_left = stt_config.get(\"audio_silence_prefix_seconds\", 0.0)\n            pad_left = int(pad_left * 24000)\n            pad_right = int((pad_right + 1.0) * 24000)\n            in_pcms = torch.nn.functional.pad(in_pcms, (pad_left, pad_right), mode=\"constant\")\n        # We keep only fully frames.\n        chunks = deque(\n            [\n                chunk\n                for chunk in in_pcms.split(self.frame_size, dim=2)\n                if chunk.shape[-1] == self.frame_size\n            ]\n        )\n\n        self.printer.print_header()\n        while not all(eos_reached):\n            if chunks:\n                chunk = chunks.popleft()\n                codes = self.mimi.encode(chunk)\n            else:\n                if self.model_type == \"hibiki\":\n                    if need_eos_input:\n                        # First frame after the end of the file, we feed a code full of 2048\n                        # to indicate the end of stream.\n                        need_eos_input = False\n                        eos_value = self.mimi.cardinality\n                        codes = torch.full(\n                            (self.batch_size, self.mimi.num_codebooks, 1),\n                            eos_value,\n                            device=device,\n                            dtype=torch.long,\n                        )\n                    else:\n                        silence = torch.zeros(\n                            (self.batch_size, self.mimi.channels, self.frame_size),\n                            device=device,\n                        )\n                        codes = self.mimi.encode(silence)\n                else:\n                    # For other models, we stop as soon as we are reaching the end of the audio.\n                    break\n            if first_frame:\n                # Ensure that the first slice of codes is properly seen by the transformer\n                # as otherwise the first slice is replaced by the initial tokens.\n                tokens = self.lm_gen.step(codes)\n                if max(self.lm_gen.lm_model.delays) > 0:\n                    assert tokens is None\n                first_frame = False\n            tokens = self.lm_gen.step(codes)\n            if tokens is None:\n                continue\n            assert tokens.shape[1] == self.lm_gen.lm_model.dep_q + 1\n            if self.lm_gen.lm_model.dep_q > 0:\n                out_pcm = self.mimi.decode(tokens[:, 1:]).cpu()\n                for b, (one_text, one_pcm) in enumerate(\n                    zip(tokens[:, 0].cpu(), out_pcm)\n                ):\n                    if eos_reached[b]:\n                        continue\n                    elif one_text.item() == self.text_tokenizer.eos_id():\n                        if need_eos_input:\n                            # We sampled the EOS before the end of the file! Not possible.\n                            self.printer.log(\"warning\", \"EOS sampled too early.\")\n                        else:\n                            eos_reached[b] = True\n\n                    out_text_tokens_per_item[b].append(one_text)\n                    out_pcms_per_item[b].append(one_pcm)\n                    if b == 0:\n                        if one_text.item() not in [0, 3]:\n                            text = self.text_tokenizer.id_to_piece(one_text.item())  # pyright: ignore\n                            text = text.replace(\"▁\", \" \")\n                            self.printer.print_token(text)\n            else:\n                one_text = tokens[0, 0].cpu()\n                if one_text.item() not in [0, 3]:\n                    text = self.text_tokenizer.id_to_piece(one_text.item())  # pyright: ignore\n                    text = text.replace(\"▁\", \" \")\n                    self.printer.print_token(text)\n            ntokens += 1\n        dt = time.time() - start_time\n        self.printer.log(\n            \"info\",\n            f\"processed {ntokens} steps in {dt:.0f}s, {1000 * dt / ntokens:.2f}ms/step\",\n        )\n        if self.lm_gen.lm_model.dep_q > 0:\n            out = [\n                (torch.cat(one_texts, dim=0), torch.cat(one_pcms, dim=1))\n                for one_texts, one_pcms in zip(\n                    out_text_tokens_per_item, out_pcms_per_item\n                )\n            ]\n            return out\n        else:\n            return []\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--tokenizer\", type=str, help=\"Path to a local tokenizer file.\")\n    parser.add_argument(\n        \"--moshi-weight\", type=str, help=\"Path to a local checkpoint file for Moshi.\"\n    )\n    parser.add_argument(\n        \"--mimi-weight\", type=str, help=\"Path to a local checkpoint file for Mimi.\"\n    )\n    parser.add_argument(\n        \"--hf-repo\",\n        type=str,\n        default=loaders.DEFAULT_REPO,\n        help=\"HF repo to look into, defaults Moshiko. \"\n        \"Use this to select a different pre-trained model.\",\n    )\n    parser.add_argument(\n        \"--batch-size\", type=int, default=8, help=\"Batch size to be used for inference.\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\",\n        help=\"Device on which to run, defaults to 'cuda'.\",\n    )\n    parser.add_argument(\n        \"--half\",\n        action=\"store_const\",\n        const=torch.float16,\n        default=torch.bfloat16,\n        dest=\"dtype\",\n        help=\"Run inference with float16, not bfloat16, better for old GPUs.\",\n    )\n    parser.add_argument(\n        \"--config\",\n        \"--lm-config\",\n        dest=\"config\",\n        type=str,\n        help=\"The config as a json file.\",\n    )\n    parser.add_argument(\"--cfg-coef\", type=float, default=1.0, help=\"CFG coefficient.\")\n    parser.add_argument(\"infile\", type=str, help=\"Input audio file.\")\n    parser.add_argument(\n        \"outfile\",\n        type=str,\n        help=\"Output audio file in wav format.\",\n        nargs=\"?\",\n        default=\"\",\n    )\n\n    args = parser.parse_args()\n    seed_all(4242)\n\n    log(\"info\", \"retrieving checkpoint\")\n    checkpoint_info = loaders.CheckpointInfo.from_hf_repo(\n        args.hf_repo, args.moshi_weight, args.mimi_weight, args.tokenizer, args.config\n    )\n    log(\"info\", \"loading mimi\")\n    mimi = checkpoint_info.get_mimi(device=args.device)\n    log(\"info\", \"mimi loaded\")\n    text_tokenizer = checkpoint_info.get_text_tokenizer()\n    log(\"info\", \"loading moshi\")\n    lm = checkpoint_info.get_moshi(device=args.device, dtype=args.dtype)\n    log(\"info\", \"moshi loaded\")\n    if lm.dep_q == 0:\n        args.batch_size = 1\n\n    log(\"info\", f\"loading input file {args.infile}\")\n    in_pcms, _ = sphn.read(args.infile, sample_rate=mimi.sample_rate)\n    in_pcms = torch.from_numpy(in_pcms).to(device=args.device)\n    in_pcms = in_pcms[None, 0:1].expand(args.batch_size, -1, -1)\n\n    state = InferenceState(\n        checkpoint_info,\n        mimi,\n        text_tokenizer,\n        lm,\n        args.batch_size,\n        args.cfg_coef,\n        args.device,\n        **checkpoint_info.lm_gen_config,\n    )\n    out_items = state.run(in_pcms)\n\n    if args.outfile:\n        outfile = Path(args.outfile)\n        for index, (_, out_pcm) in enumerate(out_items):\n            if len(out_items) > 1:\n                outfile_ = outfile.with_name(f\"{outfile.stem}-{index}{outfile.suffix}\")\n            else:\n                outfile_ = outfile\n            duration = out_pcm.shape[1] / mimi.sample_rate\n            log(\"info\", f\"writing {outfile_} with duration {duration:.1f} sec.\")\n            sphn.write_wav(\n                str(outfile_), out_pcm[0].numpy(), sample_rate=mimi.sample_rate\n            )\n\n\nif __name__ == \"__main__\":\n    with torch.no_grad():\n        main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/moshi/moshi/moshi/run_inference.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "LMGen",
            "Description": "impl for LMGen",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/moshi/moshi/moshi/models/lm.py",
            "Implementation": "# Copyright (c) Kyutai, all rights reserved.\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom contextlib import ExitStack\nfrom dataclasses import dataclass, field\nfrom functools import partial\nimport logging\nimport typing as tp\nimport torch\nfrom torch import nn\nfrom ..conditioners import ConditionProvider, ConditionFuser, ConditionTensors\nfrom ..utils.sampling import sample_token\nfrom ..utils.compile import CUDAGraphed\nfrom ..utils.quantize import replace_linear_with_qlinear\nfrom ..modules.streaming import StreamingContainer, StreamingModule, State\nfrom ..modules.transformer import StreamingTransformer, create_norm_fn\nfrom .lm_utils import (_delay_sequence,\n                       _undelay_sequence,\n                       _init_layer,\n                       ScaledEmbedding)\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef scatter_with_mask_(tensor: torch.Tensor, dim: int,\n                       index: torch.Tensor, value: torch.Tensor, mask: torch.Tensor) -> None:\n    \"\"\"Scatter but skipping the updates that are masked.\"\"\"\n    old_value = tensor.gather(dim, index)\n    value = torch.where(mask, value, old_value)\n    tensor.scatter_(dim, index, value)\n\nclass LMGen(StreamingModule[_LMGenState]):\n    def __init__(\n        self,\n        lm_model: LMModel,\n        use_sampling: bool = True,\n        temp: float = 0.8,\n        temp_text: float = 0.7,\n        top_k: int = 250,\n        top_k_text: int = 25,\n        cfg_coef: float = 1.,\n        check: bool = False,\n        condition_tensors: ConditionTensors | None = None,\n        on_text_hook: tp.Optional[tp.Callable[[torch.Tensor], None]] = None,\n        on_text_logits_hook: tp.Optional[tp.Callable[[torch.Tensor], None]] = None,\n        on_audio_hook: tp.Optional[tp.Callable[[torch.Tensor], None]] = None,\n        support_out_of_sync: bool = False,\n        cfg_is_masked_until: list[int] | None = None,\n        cfg_is_no_text: bool = False,\n    ):\n        assert not lm_model.training, \"generation shouldn't be used in training mode.\"\n        super().__init__()\n\n        self.lm_model = lm_model\n        self.lm_model.set_streaming_detached(True)\n        self.use_sampling = use_sampling\n        self.temp = temp\n        self.temp_text = temp_text\n        self.top_k = top_k\n        self.top_k_text = top_k_text\n        self.cfg_coef = cfg_coef\n        self.check = check\n        self.max_delay = max(\n            lm_model.delays\n        )  # with delays, we need to generate a few more time steps.\n        self.delays_cuda = torch.tensor(\n            lm_model.delays, device=lm_model.device, dtype=torch.long\n        )\n        self.condition_tensors = condition_tensors\n        self.on_text_hook = on_text_hook\n        self.on_text_logits_hook = on_text_logits_hook\n        self.on_audio_hook = on_audio_hook\n        self.support_out_of_sync = support_out_of_sync\n        self.cfg_is_masked_until = cfg_is_masked_until\n        self.cfg_is_no_text = cfg_is_no_text\n        if self.cfg_coef != 1.:\n            if not self.cfg_is_no_text and not self.cfg_is_masked_until:\n                assert self.lm_model.fuser is not None, \"Model has no fuser, cannot do CFG.\"\n                assert self.condition_tensors, \"Missing condition tensors for CFG.\"\n\n    def _init_streaming_state(self, batch_size: int) -> _LMGenState:\n        lm_model = self.lm_model\n        initial = lm_model._get_initial_token()\n        cache = torch.full(\n            (batch_size, self.lm_model.num_codebooks, self.max_delay + 2),\n            lm_model.ungenerated_token_id,\n            device=lm_model.device,\n            dtype=torch.long,\n        )\n        offsets = torch.zeros(batch_size, device=lm_model.device, dtype=torch.long)\n\n        if self.lm_model.fuser is None:\n            assert not self.condition_tensors\n            condition_sum = None\n            condition_cross = None\n        else:\n            assert self.condition_tensors is not None\n            condition_sum = self.lm_model.fuser.get_sum(self.condition_tensors)\n            condition_cross = self.lm_model.fuser.get_cross(self.condition_tensors)\n            if condition_sum is not None:\n                condition_sum = condition_sum.to(self.lm_model.dtype)\n            if condition_cross is not None:\n                condition_cross = condition_cross.to(self.lm_model.dtype)\n\n        disable = lm_model.device.type != 'cuda'\n        graphed_main = CUDAGraphed(lm_model.forward_text, disable=disable)\n        if lm_model.depformer is not None:\n            graphed_depth = CUDAGraphed(self.depformer_step, disable=disable)\n        else:\n            graphed_depth = None\n\n        if self.cfg_is_masked_until is None:\n            cfg_is_masked_until = None\n        else:\n            cfg_is_masked_until = torch.tensor(self.cfg_is_masked_until, dtype=torch.long, device=lm_model.device)\n\n        state = _LMGenState(\n            batch_size, lm_model.device, cache, initial, graphed_main, graphed_depth,\n            offsets, condition_sum=condition_sum, condition_cross=condition_cross,\n            cfg_is_masked_until=cfg_is_masked_until)\n\n        if self.cfg_coef != 1.:\n            batch_size *= 2\n            if state.condition_sum is not None:\n                assert state.condition_sum.shape[0] == batch_size, \"cfg requires 2x more conditions.\"\n            if state.condition_cross is not None:\n                assert state.condition_cross.shape[0] == batch_size, \"cfg requires 2x more conditions.\"\n        state.exit_stack.enter_context(self.lm_model.streaming(batch_size))\n\n        def _reset_callback(reset_mask: torch.Tensor) -> None:\n            if self.cfg_coef != 1.:\n                reset_mask = reset_mask.repeat(2)\n            self.lm_model.reset_streaming(reset_mask)\n\n        def _set_exec_mask_callback(exec_mask: torch.Tensor) -> None:\n            if self.cfg_coef != 1.:\n                exec_mask = exec_mask.repeat(2)\n            self.lm_model.set_exec_mask(exec_mask)\n\n        state.reset_callback = _reset_callback\n        state.set_exec_mask_callback = _set_exec_mask_callback\n        return state\n\n    @torch.no_grad()\n    def _step(self, input_tokens: torch.Tensor,\n              depformer_replace_tokens: torch.Tensor | None = None\n              ) -> tuple[torch.Tensor, torch.Tensor] | None:\n        state = self._streaming_state\n        if state is None:\n            raise RuntimeError(\n                \"You should wrap those calls with a `with lm_gen.streaming(): ...`.\"\n            )\n        lm_model = self.lm_model\n\n        assert input_tokens.dim() == 3, \"Shape should be [B, K, T].\"\n        B, Ki, S = input_tokens.shape\n        assert B == state.batch_size, f\"Got a batch size {B}, expected {state.batch_size}\"\n        assert S == 1, \"Only support being given steps one by one.\"\n        needed_tokens = lm_model.num_codebooks - lm_model.dep_q - 1\n        assert (\n            Ki >= needed_tokens\n        ), f\"We expect {needed_tokens} tokens from the user stream, got {Ki}.\"\n\n        if Ki > needed_tokens:\n            input_tokens = input_tokens[:, :needed_tokens, :]\n\n        CT = state.cache.shape[2]\n\n        delays = self.delays_cuda[lm_model.dep_q + 1:]\n        write_positions = (state.offsets[:, None, None] + delays[:, None]) % CT\n        scatter_with_mask_(state.cache[:, lm_model.dep_q + 1:], -1, write_positions, input_tokens,\n                           state.exec_mask[:, None, None])\n\n        is_init = state.offsets[:, None, None] <= self.delays_cuda[:, None]\n        is_init |= ~state.exec_mask[:, None, None]  # we also give init tokens if not executing to avoid crashing.\n        positions = (state.offsets % CT)[:, None, None].expand_as(is_init)\n        input_ = state.cache.gather(dim=2, index=positions)\n        input_ = torch.where(is_init, state.initial, input_)\n\n        if self.check:\n            # Check that we are not feeding in any value that is not generated yet.\n            assert not (input_ == lm_model.ungenerated_token_id).any(), (\n                state.offsets,\n                input_,\n            )\n            assert (input_[:, lm_model.audio_offset :] <= lm_model.card).all(), input_\n            assert (input_[:, :1] <= lm_model.text_card).all()\n\n        zero = torch.full((1,), self.lm_model.zero_token_id, dtype=torch.long, device=input_.device)\n        if self.cfg_coef != 1.:\n            if state.cfg_is_masked_until is not None:\n                limit = self.delays_cuda[:, None] + state.cfg_is_masked_until.view(-1, 1, 1)\n                is_zeroed = state.offsets[:, None, None] <= limit\n\n                masked = torch.where(is_zeroed & ~is_init, zero, input_)\n                input_ = torch.cat([input_, masked], dim=0)\n            else:\n                input_ = input_.repeat(2, 1, 1)\n            if self.cfg_is_no_text:\n                input_[B:, :1] = torch.where(~is_init[:, :1], zero, input_[B:, :1])\n\n        transformer_out, text_logits = state.graphed_main(input_, state.condition_sum, state.condition_cross)\n        if self.cfg_coef != 1.:\n            logits, logits_null = text_logits.chunk(2)\n            if self.cfg_is_no_text:\n                text_logits = logits\n            else:\n                text_logits = logits_null + (logits - logits_null) * self.cfg_coef\n        # Shape of text_logits should be [B, K_text=1, T=1, Card_text]\n        if self.on_text_logits_hook:\n            self.on_text_logits_hook(text_logits)\n        text_token = sample_token(\n            text_logits.float(),\n            self.use_sampling,\n            self.temp_text,\n            self.top_k_text,\n        )\n        assert text_token.dim() == 3, text_token.shape\n        assert text_token.shape[2] == 1\n        assert text_token.shape[1] == 1, \"Only one text stream supported.\"\n        text_token = text_token[:, 0, 0]  # shape is [B]\n        if self.on_text_hook is not None:\n            self.on_text_hook(text_token)\n        if state.graphed_depth is None:\n            audio_tokens = None\n        elif depformer_replace_tokens is None:\n            audio_tokens = state.graphed_depth(text_token, transformer_out)\n            if self.on_audio_hook is not None:\n                self.on_audio_hook(audio_tokens)\n        else:\n            assert depformer_replace_tokens.dim() == 3\n            audio_tokens = depformer_replace_tokens.squeeze(-1)\n\n        state.offsets = torch.where(state.exec_mask, state.offsets + 1, state.offsets)\n        state.offset_cpu += 1\n        positions = (state.offsets % CT)[:, None, None]\n        scatter_with_mask_(state.cache[:, :1], -1, positions,\n                           text_token[:, None, None], state.exec_mask[:, None, None])\n        if audio_tokens is not None:\n            audio_tokens = audio_tokens[:, :, None]\n            scatter_with_mask_(\n                state.cache[:, 1 : lm_model.dep_q + 1, :],\n                -1,\n                positions.expand_as(audio_tokens),\n                audio_tokens,\n                state.exec_mask[:, None, None],\n            )\n\n        if not self.support_out_of_sync and state.offset_cpu <= self.max_delay:\n            # When using out of sync exec, should not rely on this being None.\n            return None\n        B = state.cache.shape[0]\n        gen_delays_cuda = self.delays_cuda[: lm_model.dep_q + 1]\n        index = (state.offsets[:, None, None] - self.max_delay + gen_delays_cuda[:, None]) % CT\n        out = state.cache.gather(dim=2, index=index)\n        mask = (state.offsets <= self.max_delay) | ~state.exec_mask\n        out[mask, :, :] = lm_model.ungenerated_token_id\n        return out, transformer_out\n\n    @torch.no_grad()\n    def step(self, input_tokens: torch.Tensor,\n             depformer_replace_tokens: torch.Tensor | None = None) -> torch.Tensor | None:\n        out = self._step(input_tokens, depformer_replace_tokens)\n        if out is None:\n            return None\n        return out[0]\n\n    @torch.no_grad()\n    def step_with_extra_heads(\n        self,\n        input_tokens: torch.Tensor,\n        depformer_replace_tokens: torch.Tensor | None = None,\n    ) -> tuple[torch.Tensor, list[torch.Tensor]] | None:\n        out = self._step(input_tokens, depformer_replace_tokens)\n        if out is None:\n            return None\n        out, transformer_out = out\n        extra_heads = [\n            torch.nn.functional.softmax(extra_head(transformer_out), dim=-1)\n            for extra_head in self.lm_model.extra_heads\n        ]\n        return out, extra_heads\n\n    def depformer_step(\n        self,\n        text_token: torch.Tensor,\n        transformer_out: torch.Tensor,\n    ) -> torch.Tensor:\n        B, = text_token.shape\n        B_cfg = B\n        if self.cfg_coef != 1.:\n            B_cfg = 2 * B\n        prev_token = text_token\n        lm_model = self.lm_model\n        depformer_tokens: list[torch.Tensor] = []\n        assert lm_model.depformer\n        assert not lm_model.depformer.is_streaming\n        with lm_model.depformer.streaming(B_cfg):\n            assert lm_model.depformer.is_streaming\n            for cb_index in range(lm_model.dep_q):\n                input_ = prev_token[:, None, None]\n                if self.cfg_coef != 1.:\n                    input_ = input_.repeat(2, 1, 1)\n                logits = lm_model.forward_depformer(cb_index, input_, transformer_out)\n                if self.cfg_coef != 1.:\n                    logits, logits_null = logits.chunk(2)\n                    logits = logits_null + (logits - logits_null) * self.cfg_coef\n                next_token = sample_token(\n                    logits.float(),\n                    self.use_sampling,\n                    self.temp,\n                    self.top_k,\n                )\n                assert next_token.shape == (B, 1, 1)\n                next_token = next_token[:, 0, 0]  # shape is B\n                depformer_tokens.append(next_token)\n                prev_token = next_token\n\n        assert len(depformer_tokens) == lm_model.dep_q, (\n            len(depformer_tokens),\n            lm_model.dep_q,\n        )\n        out = torch.stack(depformer_tokens, dim=1)\n        assert out.shape == (B, lm_model.dep_q), out.shape\n        return out\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}