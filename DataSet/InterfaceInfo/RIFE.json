{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/ECCV2022-RIFE",
    "API_Calls": [
        {
            "Name": "call_Model",
            "Description": "call_Model",
            "Code": "import os\nimport cv2\nimport torch\nimport argparse\nfrom torch.nn import functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.set_grad_enabled(False)\nif torch.cuda.is_available():\n    torch.backends.cudnn.enabled = True\n    torch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description='Interpolation for a pair of images')\nparser.add_argument('--img', dest='img', nargs=2, required=True)\nparser.add_argument('--exp', default=4, type=int)\nparser.add_argument('--ratio', default=0, type=float, help='inference ratio between two images with 0 - 1 range')\nparser.add_argument('--rthreshold', default=0.02, type=float, help='returns image when actual ratio falls in given range threshold')\nparser.add_argument('--rmaxcycles', default=8, type=int, help='limit max number of bisectional cycles')\nparser.add_argument('--model', dest='modelDir', type=str, default='train_log', help='directory with trained model files')\n\nargs = parser.parse_args()\n\ntry:\n    try:\n        try:\n            from model.RIFE_HDv2 import Model\n            model = Model()\n            model.load_model(args.modelDir, -1)\n            print(\"Loaded v2.x HD model.\")\n        except:\n            from train_log.RIFE_HDv3 import Model\n            model = Model()\n            model.load_model(args.modelDir, -1)\n            print(\"Loaded v3.x HD model.\")\n    except:\n        from model.RIFE_HD import Model\n        model = Model()\n        model.load_model(args.modelDir, -1)\n        print(\"Loaded v1.x HD model\")\nexcept:\n    from model.RIFE import Model\n    model = Model()\n    model.load_model(args.modelDir, -1)\n    print(\"Loaded ArXiv-RIFE model\")\nmodel.eval()\nmodel.device()\n\nif args.img[0].endswith('.exr') and args.img[1].endswith('.exr'):\n    img0 = cv2.imread(args.img[0], cv2.IMREAD_COLOR | cv2.IMREAD_ANYDEPTH)\n    img1 = cv2.imread(args.img[1], cv2.IMREAD_COLOR | cv2.IMREAD_ANYDEPTH)\n    img0 = (torch.tensor(img0.transpose(2, 0, 1)).to(device)).unsqueeze(0)\n    img1 = (torch.tensor(img1.transpose(2, 0, 1)).to(device)).unsqueeze(0)\n\nelse:\n    img0 = cv2.imread(args.img[0], cv2.IMREAD_UNCHANGED)\n    img1 = cv2.imread(args.img[1], cv2.IMREAD_UNCHANGED)\n    img0 = (torch.tensor(img0.transpose(2, 0, 1)).to(device) / 255.).unsqueeze(0)\n    img1 = (torch.tensor(img1.transpose(2, 0, 1)).to(device) / 255.).unsqueeze(0)\n\nn, c, h, w = img0.shape\nph = ((h - 1) // 32 + 1) * 32\npw = ((w - 1) // 32 + 1) * 32\npadding = (0, pw - w, 0, ph - h)\nimg0 = F.pad(img0, padding)\nimg1 = F.pad(img1, padding)\n\n\nif args.ratio:\n    img_list = [img0]\n    img0_ratio = 0.0\n    img1_ratio = 1.0\n    if args.ratio <= img0_ratio + args.rthreshold / 2:\n        middle = img0\n    elif args.ratio >= img1_ratio - args.rthreshold / 2:\n        middle = img1\n    else:\n        tmp_img0 = img0\n        tmp_img1 = img1\n        for inference_cycle in range(args.rmaxcycles):\n            middle = model.inference(tmp_img0, tmp_img1)\n            middle_ratio = ( img0_ratio + img1_ratio ) / 2\n            if args.ratio - (args.rthreshold / 2) <= middle_ratio <= args.ratio + (args.rthreshold / 2):\n                break\n            if args.ratio > middle_ratio:\n                tmp_img0 = middle\n                img0_ratio = middle_ratio\n            else:\n                tmp_img1 = middle\n                img1_ratio = middle_ratio\n    img_list.append(middle)\n    img_list.append(img1)\nelse:\n    img_list = [img0, img1]\n    for i in range(args.exp):\n        tmp = []\n        for j in range(len(img_list) - 1):\n            mid = model.inference(img_list[j], img_list[j + 1])\n            tmp.append(img_list[j])\n            tmp.append(mid)\n        tmp.append(img1)\n        img_list = tmp\n\nif not os.path.exists('output'):\n    os.mkdir('output')\nfor i in range(len(img_list)):\n    if args.img[0].endswith('.exr') and args.img[1].endswith('.exr'):\n        cv2.imwrite('output/img{}.exr'.format(i), (img_list[i][0]).cpu().numpy().transpose(1, 2, 0)[:h, :w], [cv2.IMWRITE_EXR_TYPE, cv2.IMWRITE_EXR_TYPE_HALF])\n    else:\n        cv2.imwrite('output/img{}.png'.format(i), (img_list[i][0] * 255).byte().cpu().numpy().transpose(1, 2, 0)[:h, :w])\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/ECCV2022-RIFE/inference_img.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "Model",
            "Description": "Model_impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/ECCV2022-RIFE/model/RIFE.py",
            "Implementation": "import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.optim import AdamW\nimport torch.optim as optim\nimport itertools\nfrom model.warplayer import warp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom model.IFNet import *\nfrom model.IFNet_m import *\nimport torch.nn.functional as F\nfrom model.loss import *\nfrom model.laplacian import *\nfrom model.refine import *\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \nclass Model:\n    def __init__(self, local_rank=-1, arbitrary=False):\n        if arbitrary == True:\n            self.flownet = IFNet_m()\n        else:\n            self.flownet = IFNet()\n        self.device()\n        self.optimG = AdamW(self.flownet.parameters(), lr=1e-6, weight_decay=1e-3) # use large weight decay may avoid NaN loss\n        self.epe = EPE()\n        self.lap = LapLoss()\n        self.sobel = SOBEL()\n        if local_rank != -1:\n            self.flownet = DDP(self.flownet, device_ids=[local_rank], output_device=local_rank)\n\n    def train(self):\n        self.flownet.train()\n\n    def eval(self):\n        self.flownet.eval()\n\n    def device(self):\n        self.flownet.to(device)\n\n    def load_model(self, path, rank=0):\n        def convert(param):\n            return {\n            k.replace(\"module.\", \"\"): v\n                for k, v in param.items()\n                if \"module.\" in k\n            }\n            \n        if rank <= 0:\n            self.flownet.load_state_dict(convert(torch.load('{}/flownet.pkl'.format(path))))\n        \n    def save_model(self, path, rank=0):\n        if rank == 0:\n            torch.save(self.flownet.state_dict(),'{}/flownet.pkl'.format(path))\n\n    def inference(self, img0, img1, scale=1, scale_list=None, TTA=False, timestep=0.5):\n        if scale_list is None:\n            scale_list = [4, 2, 1]\n        for i in range(3):\n            scale_list[i] = scale_list[i] * 1.0 / scale\n        imgs = torch.cat((img0, img1), 1)\n        flow, mask, merged, flow_teacher, merged_teacher, loss_distill = self.flownet(imgs, scale_list, timestep=timestep)\n        if TTA == False:\n            return merged[2]\n        else:\n            flow2, mask2, merged2, flow_teacher2, merged_teacher2, loss_distill2 = self.flownet(imgs.flip(2).flip(3), scale_list, timestep=timestep)\n            return (merged[2] + merged2[2].flip(2).flip(3)) / 2\n    \n    def update(self, imgs, gt, learning_rate=0, mul=1, training=True, flow_gt=None):\n        for param_group in self.optimG.param_groups:\n            param_group['lr'] = learning_rate\n        img0 = imgs[:, :3]\n        img1 = imgs[:, 3:]\n        if training:\n            self.train()\n        else:\n            self.eval()\n        flow, mask, merged, flow_teacher, merged_teacher, loss_distill = self.flownet(torch.cat((imgs, gt), 1), scale=[4, 2, 1])\n        loss_l1 = (self.lap(merged[2], gt)).mean()\n        loss_tea = (self.lap(merged_teacher, gt)).mean()\n        if training:\n            self.optimG.zero_grad()\n            loss_G = loss_l1 + loss_tea + loss_distill * 0.01 # when training RIFEm, the weight of loss_distill should be 0.005 or 0.002\n            loss_G.backward()\n            self.optimG.step()\n        else:\n            flow_teacher = flow[2]\n        return merged[2], {\n            'merged_tea': merged_teacher,\n            'mask': mask,\n            'mask_tea': mask,\n            'flow': flow[2][:, :2],\n            'flow_tea': flow_teacher,\n            'loss_l1': loss_l1,\n            'loss_tea': loss_tea,\n            'loss_distill': loss_distill,\n            }\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}