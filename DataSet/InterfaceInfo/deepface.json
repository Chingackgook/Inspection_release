{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/deepface",
    "API_Calls": [
        {
            "Name": "service",
            "Description": "通过封装DeepFace的核心功能（特征提取、人脸验证、属性分析）实现了安全可靠的API接口，采用统一模式：捕获输入→调用DeepFace对应方法→异常处理→结构化返回结果",
            "Code": "# built-in dependencies\nimport traceback\nfrom typing import Optional, Union\n\n# 3rd party dependencies\nimport numpy as np\n\n# project dependencies\nfrom deepface import DeepFace\nfrom deepface.commons.logger import Logger\n\nlogger = Logger()\n\n\n# pylint: disable=broad-except\n\n\ndef represent(\n    img_path: Union[str, np.ndarray],\n    model_name: str,\n    detector_backend: str,\n    enforce_detection: bool,\n    align: bool,\n    anti_spoofing: bool,\n    max_faces: Optional[int] = None,\n):\n    try:\n        result = {}\n        embedding_objs = DeepFace.represent(\n            img_path=img_path,\n            model_name=model_name,\n            detector_backend=detector_backend,\n            enforce_detection=enforce_detection,\n            align=align,\n            anti_spoofing=anti_spoofing,\n            max_faces=max_faces,\n        )\n        result[\"results\"] = embedding_objs\n        return result\n    except Exception as err:\n        tb_str = traceback.format_exc()\n        logger.error(str(err))\n        logger.error(tb_str)\n        return {\"error\": f\"Exception while representing: {str(err)} - {tb_str}\"}, 400\n\n\ndef verify(\n    img1_path: Union[str, np.ndarray],\n    img2_path: Union[str, np.ndarray],\n    model_name: str,\n    detector_backend: str,\n    distance_metric: str,\n    enforce_detection: bool,\n    align: bool,\n    anti_spoofing: bool,\n):\n    try:\n        obj = DeepFace.verify(\n            img1_path=img1_path,\n            img2_path=img2_path,\n            model_name=model_name,\n            detector_backend=detector_backend,\n            distance_metric=distance_metric,\n            align=align,\n            enforce_detection=enforce_detection,\n            anti_spoofing=anti_spoofing,\n        )\n        return obj\n    except Exception as err:\n        tb_str = traceback.format_exc()\n        logger.error(str(err))\n        logger.error(tb_str)\n        return {\"error\": f\"Exception while verifying: {str(err)} - {tb_str}\"}, 400\n\n\ndef analyze(\n    img_path: Union[str, np.ndarray],\n    actions: list,\n    detector_backend: str,\n    enforce_detection: bool,\n    align: bool,\n    anti_spoofing: bool,\n):\n    try:\n        result = {}\n        demographies = DeepFace.analyze(\n            img_path=img_path,\n            actions=actions,\n            detector_backend=detector_backend,\n            enforce_detection=enforce_detection,\n            align=align,\n            silent=True,\n            anti_spoofing=anti_spoofing,\n        )\n        result[\"results\"] = demographies\n        return result\n    except Exception as err:\n        tb_str = traceback.format_exc()\n        logger.error(str(err))\n        logger.error(tb_str)\n        return {\"error\": f\"Exception while analyzing: {str(err)} - {tb_str}\"}, 400\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/deepface/deepface/api/src/modules/core/service.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "DeepFace.py",
            "Description": "DeepFace 库的核心功能模块​​，提供了一套完整的基于深度学习的人脸识别与分析解决方案，涵盖人脸验证、属性分析（年龄/性别/情绪/种族）、数据库检索、实时视频流处理和反欺诈检测等功能",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/deepface/deepface/DeepFace.py",
            "Implementation": "# common dependencies\nimport os\nimport warnings\nimport logging\nfrom typing import Any, Dict, IO, List, Union, Optional, Sequence\n\n# this has to be set before importing tensorflow\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n\n# pylint: disable=wrong-import-position\n\n# 3rd party dependencies\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n# package dependencies\nfrom deepface.commons import package_utils, folder_utils\nfrom deepface.commons.logger import Logger\nfrom deepface.modules import (\n    modeling,\n    representation,\n    verification,\n    recognition,\n    demography,\n    detection,\n    streaming,\n    preprocessing,\n)\nfrom deepface import __version__\n\nlogger = Logger()\n\n# -----------------------------------\n# configurations for dependencies\n\n# users should install tf_keras package if they are using tf 2.16 or later versions\npackage_utils.validate_for_keras3()\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\ntf_version = package_utils.get_tf_major_version()\nif tf_version == 2:\n    tf.get_logger().setLevel(logging.ERROR)\n# -----------------------------------\n\n# create required folders if necessary to store model weights\nfolder_utils.initialize_folder()\n\n\ndef build_model(model_name: str, task: str = \"facial_recognition\") -> Any:\n    \"\"\"\n    This function builds a pre-trained model\n    Args:\n        model_name (str): model identifier\n            - VGG-Face, Facenet, Facenet512, OpenFace, DeepFace, DeepID, Dlib,\n                ArcFace, SFace GhostFaceNet and Buffalo_L for face recognition\n            - Age, Gender, Emotion, Race for facial attributes\n            - opencv, mtcnn, ssd, dlib, retinaface, mediapipe, yolov8, yolov11n,\n              yolov11s, yolov11m, yunet, fastmtcnn or centerface for face detectors\n            - Fasnet for spoofing\n        task (str): facial_recognition, facial_attribute, face_detector, spoofing\n            default is facial_recognition\n    Returns:\n        built_model\n    \"\"\"\n    return modeling.build_model(task=task, model_name=model_name)\n\n\ndef verify(\n    img1_path: Union[str, np.ndarray, IO[bytes], List[float]],\n    img2_path: Union[str, np.ndarray, IO[bytes], List[float]],\n    model_name: str = \"VGG-Face\",\n    detector_backend: str = \"opencv\",\n    distance_metric: str = \"cosine\",\n    enforce_detection: bool = True,\n    align: bool = True,\n    expand_percentage: int = 0,\n    normalization: str = \"base\",\n    silent: bool = False,\n    threshold: Optional[float] = None,\n    anti_spoofing: bool = False,\n) -> Dict[str, Any]:\n    \"\"\"\n    Verify if an image pair represents the same person or different persons.\n    Args:\n        img1_path (str or np.ndarray or IO[bytes] or List[float]): Path to the first image.\n            Accepts exact image path as a string, numpy array (BGR), a file object that supports\n            at least `.read` and is opened in binary mode, base64 encoded images\n            or pre-calculated embeddings.\n\n        img2_path (str or np.ndarray or IO[bytes] or List[float]): Path to the second image.\n            Accepts exact image path as a string, numpy array (BGR), a file object that supports\n            at least `.read` and is opened in binary mode, base64 encoded images\n            or pre-calculated embeddings.\n\n        model_name (str): Model for face recognition. Options: VGG-Face, Facenet, Facenet512,\n            OpenFace, DeepFace, DeepID, Dlib, ArcFace, SFace and GhostFaceNet (default is VGG-Face).\n\n        detector_backend (string): face detector backend. Options: 'opencv', 'retinaface',\n            'mtcnn', 'ssd', 'dlib', 'mediapipe', 'yolov8', 'yolov11n', 'yolov11s', 'yolov11m',\n            'centerface' or 'skip' (default is opencv).\n\n        distance_metric (string): Metric for measuring similarity. Options: 'cosine',\n            'euclidean', 'euclidean_l2', 'angular' (default is cosine).\n\n        enforce_detection (boolean): If no face is detected in an image, raise an exception.\n            Set to False to avoid the exception for low-resolution images (default is True).\n\n        align (bool): Flag to enable face alignment (default is True).\n\n        expand_percentage (int): expand detected facial area with a percentage (default is 0).\n\n        normalization (string): Normalize the input image before feeding it to the model.\n            Options: base, raw, Facenet, Facenet2018, VGGFace, VGGFace2, ArcFace (default is base)\n\n        silent (boolean): Suppress or allow some log messages for a quieter analysis process\n            (default is False).\n\n        threshold (float): Specify a threshold to determine whether a pair represents the same\n            person or different individuals. This threshold is used for comparing distances.\n            If left unset, default pre-tuned threshold values will be applied based on the specified\n            model name and distance metric (default is None).\n\n        anti_spoofing (boolean): Flag to enable anti spoofing (default is False).\n\n    Returns:\n        result (dict): A dictionary containing verification results with following keys.\n\n        - 'verified' (bool): Indicates whether the images represent the same person (True)\n            or different persons (False).\n\n        - 'distance' (float): The distance measure between the face vectors.\n            A lower distance indicates higher similarity.\n\n        - 'threshold' (float): The maximum threshold used for verification.\n            If the distance is below this threshold, the images are considered a match.\n\n        - 'model' (str): The chosen face recognition model.\n\n        - 'distance_metric' (str): The chosen similarity metric for measuring distances.\n\n        - 'facial_areas' (dict): Rectangular regions of interest for faces in both images.\n            - 'img1': {'x': int, 'y': int, 'w': int, 'h': int}\n                    Region of interest for the first image.\n            - 'img2': {'x': int, 'y': int, 'w': int, 'h': int}\n                    Region of interest for the second image.\n\n        - 'time' (float): Time taken for the verification process in seconds.\n    \"\"\"\n\n    return verification.verify(\n        img1_path=img1_path,\n        img2_path=img2_path,\n        model_name=model_name,\n        detector_backend=detector_backend,\n        distance_metric=distance_metric,\n        enforce_detection=enforce_detection,\n        align=align,\n        expand_percentage=expand_percentage,\n        normalization=normalization,\n        silent=silent,\n        threshold=threshold,\n        anti_spoofing=anti_spoofing,\n    )\n\n\ndef analyze(\n    img_path: Union[str, np.ndarray, IO[bytes], List[str], List[np.ndarray], List[IO[bytes]]],\n    actions: Union[tuple, list] = (\"emotion\", \"age\", \"gender\", \"race\"),\n    enforce_detection: bool = True,\n    detector_backend: str = \"opencv\",\n    align: bool = True,\n    expand_percentage: int = 0,\n    silent: bool = False,\n    anti_spoofing: bool = False,\n) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:\n    \"\"\"\n    Analyze facial attributes such as age, gender, emotion, and race in the provided image.\n    Args:\n        img_path (str, np.ndarray, IO[bytes], list): The exact path to the image, a numpy array\n            in BGR format, a file object that supports at least `.read` and is opened in binary\n            mode, or a base64 encoded image. If the source image contains multiple faces,\n            the result will include information for each detected face.\n\n        actions (tuple): Attributes to analyze. The default is ('age', 'gender', 'emotion', 'race').\n            You can exclude some of these attributes from the analysis if needed.\n\n        enforce_detection (boolean): If no face is detected in an image, raise an exception.\n            Set to False to avoid the exception for low-resolution images (default is True).\n\n        detector_backend (string): face detector backend. Options: 'opencv', 'retinaface',\n            'mtcnn', 'ssd', 'dlib', 'mediapipe', 'yolov8', 'yolov11n',  'yolov11s', 'yolov11m',\n            'centerface' or 'skip' (default is opencv).\n\n        distance_metric (string): Metric for measuring similarity. Options: 'cosine',\n            'euclidean', 'euclidean_l2', 'angular' (default is cosine).\n\n        align (boolean): Perform alignment based on the eye positions (default is True).\n\n        expand_percentage (int): expand detected facial area with a percentage (default is 0).\n\n        silent (boolean): Suppress or allow some log messages for a quieter analysis process\n            (default is False).\n\n        anti_spoofing (boolean): Flag to enable anti spoofing (default is False).\n\n    Returns:\n        (List[List[Dict[str, Any]]]): A list of analysis results if received batched image,\n                                      explained below.\n\n        (List[Dict[str, Any]]): A list of dictionaries, where each dictionary represents\n           the analysis results for a detected face. Each dictionary in the list contains the\n           following keys:\n\n        - 'region' (dict): Represents the rectangular region of the detected face in the image.\n            - 'x': x-coordinate of the top-left corner of the face.\n            - 'y': y-coordinate of the top-left corner of the face.\n            - 'w': Width of the detected face region.\n            - 'h': Height of the detected face region.\n\n        - 'age' (float): Estimated age of the detected face.\n\n        - 'face_confidence' (float): Confidence score for the detected face.\n            Indicates the reliability of the face detection.\n\n        - 'dominant_gender' (str): The dominant gender in the detected face.\n            Either \"Man\" or \"Woman\".\n\n        - 'gender' (dict): Confidence scores for each gender category.\n            - 'Man': Confidence score for the male gender.\n            - 'Woman': Confidence score for the female gender.\n\n        - 'dominant_emotion' (str): The dominant emotion in the detected face.\n            Possible values include \"sad,\" \"angry,\" \"surprise,\" \"fear,\" \"happy,\"\n            \"disgust,\" and \"neutral\"\n\n        - 'emotion' (dict): Confidence scores for each emotion category.\n            - 'sad': Confidence score for sadness.\n            - 'angry': Confidence score for anger.\n            - 'surprise': Confidence score for surprise.\n            - 'fear': Confidence score for fear.\n            - 'happy': Confidence score for happiness.\n            - 'disgust': Confidence score for disgust.\n            - 'neutral': Confidence score for neutrality.\n\n        - 'dominant_race' (str): The dominant race in the detected face.\n            Possible values include \"indian,\" \"asian,\" \"latino hispanic,\"\n            \"black,\" \"middle eastern,\" and \"white.\"\n\n        - 'race' (dict): Confidence scores for each race category.\n            - 'indian': Confidence score for Indian ethnicity.\n            - 'asian': Confidence score for Asian ethnicity.\n            - 'latino hispanic': Confidence score for Latino/Hispanic ethnicity.\n            - 'black': Confidence score for Black ethnicity.\n            - 'middle eastern': Confidence score for Middle Eastern ethnicity.\n            - 'white': Confidence score for White ethnicity.\n    \"\"\"\n    return demography.analyze(\n        img_path=img_path,\n        actions=actions,\n        enforce_detection=enforce_detection,\n        detector_backend=detector_backend,\n        align=align,\n        expand_percentage=expand_percentage,\n        silent=silent,\n        anti_spoofing=anti_spoofing,\n    )\n\n\ndef find(\n    img_path: Union[str, np.ndarray, IO[bytes]],\n    db_path: str,\n    model_name: str = \"VGG-Face\",\n    distance_metric: str = \"cosine\",\n    enforce_detection: bool = True,\n    detector_backend: str = \"opencv\",\n    align: bool = True,\n    expand_percentage: int = 0,\n    threshold: Optional[float] = None,\n    normalization: str = \"base\",\n    silent: bool = False,\n    refresh_database: bool = True,\n    anti_spoofing: bool = False,\n    batched: bool = False,\n) -> Union[List[pd.DataFrame], List[List[Dict[str, Any]]]]:\n    \"\"\"\n    Identify individuals in a database\n    Args:\n        img_path (str or np.ndarray or IO[bytes]): The exact path to the image, a numpy array\n            in BGR format, a file object that supports at least `.read` and is opened in binary\n            mode, or a base64 encoded image. If the source image contains multiple\n            faces, the result will include information for each detected face.\n\n        db_path (string): Path to the folder containing image files. All detected faces\n            in the database will be considered in the decision-making process.\n\n        model_name (str): Model for face recognition. Options: VGG-Face, Facenet, Facenet512,\n            OpenFace, DeepFace, DeepID, Dlib, ArcFace, SFace and GhostFaceNet (default is VGG-Face).\n\n        distance_metric (string): Metric for measuring similarity. Options: 'cosine',\n            'euclidean', 'euclidean_l2', 'angular' (default is cosine).\n\n        enforce_detection (boolean): If no face is detected in an image, raise an exception.\n            Set to False to avoid the exception for low-resolution images (default is True).\n\n        detector_backend (string): face detector backend. Options: 'opencv', 'retinaface',\n            'mtcnn', 'ssd', 'dlib', 'mediapipe', 'yolov8', 'yolov11n', 'yolov11s', 'yolov11m',\n            'centerface' or 'skip' (default is opencv).\n\n        align (boolean): Perform alignment based on the eye positions (default is True).\n\n        expand_percentage (int): expand detected facial area with a percentage (default is 0).\n\n        threshold (float): Specify a threshold to determine whether a pair represents the same\n            person or different individuals. This threshold is used for comparing distances.\n            If left unset, default pre-tuned threshold values will be applied based on the specified\n            model name and distance metric (default is None).\n\n        normalization (string): Normalize the input image before feeding it to the model.\n            Options: base, raw, Facenet, Facenet2018, VGGFace, VGGFace2, ArcFace (default is base).\n\n        silent (boolean): Suppress or allow some log messages for a quieter analysis process\n            (default is False).\n\n        refresh_database (boolean): Synchronizes the images representation (pkl) file with the\n            directory/db files, if set to false, it will ignore any file changes inside the db_path\n            (default is True).\n\n        anti_spoofing (boolean): Flag to enable anti spoofing (default is False).\n\n    Returns:\n        results (List[pd.DataFrame] or List[List[Dict[str, Any]]]):\n            A list of pandas dataframes (if `batched=False`) or\n            a list of dicts (if `batched=True`).\n            Each dataframe or dict corresponds to the identity information for\n            an individual detected in the source image.\n\n            Note: If you have a large database and/or a source photo with many faces,\n            use `batched=True`, as it is optimized for large batch processing.\n            Please pay attention that when using `batched=True`, the function returns\n            a list of dicts (not a list of DataFrames),\n            but with the same keys as the columns in the DataFrame.\n\n            The DataFrame columns or dict keys include:\n\n            - 'identity': Identity label of the detected individual.\n\n            - 'target_x', 'target_y', 'target_w', 'target_h': Bounding box coordinates of the\n                    target face in the database.\n\n            - 'source_x', 'source_y', 'source_w', 'source_h': Bounding box coordinates of the\n                    detected face in the source image.\n\n            - 'threshold': threshold to determine a pair whether same person or different persons\n\n            - 'distance': Similarity score between the faces based on the\n                    specified model and distance metric\n    \"\"\"\n    return recognition.find(\n        img_path=img_path,\n        db_path=db_path,\n        model_name=model_name,\n        distance_metric=distance_metric,\n        enforce_detection=enforce_detection,\n        detector_backend=detector_backend,\n        align=align,\n        expand_percentage=expand_percentage,\n        threshold=threshold,\n        normalization=normalization,\n        silent=silent,\n        refresh_database=refresh_database,\n        anti_spoofing=anti_spoofing,\n        batched=batched,\n    )\n\n\ndef represent(\n    img_path: Union[str, np.ndarray, IO[bytes], Sequence[Union[str, np.ndarray, IO[bytes]]]],\n    model_name: str = \"VGG-Face\",\n    enforce_detection: bool = True,\n    detector_backend: str = \"opencv\",\n    align: bool = True,\n    expand_percentage: int = 0,\n    normalization: str = \"base\",\n    anti_spoofing: bool = False,\n    max_faces: Optional[int] = None,\n) -> Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]:\n    \"\"\"\n    Represent facial images as multi-dimensional vector embeddings.\n\n    Args:\n        img_path (str, np.ndarray, IO[bytes], or Sequence[Union[str, np.ndarray, IO[bytes]]]):\n            The exact path to the image, a numpy array\n            in BGR format, a file object that supports at least `.read` and is opened in binary\n            mode, or a base64 encoded image. If the source image contains multiple faces,\n            the result will include information for each detected face. If a sequence is provided,\n            each element should be a string or numpy array representing an image, and the function\n            will process images in batch.\n\n        model_name (str): Model for face recognition. Options: VGG-Face, Facenet, Facenet512,\n            OpenFace, DeepFace, DeepID, Dlib, ArcFace, SFace and GhostFaceNet\n            (default is VGG-Face.).\n\n        enforce_detection (boolean): If no face is detected in an image, raise an exception.\n            Default is True. Set to False to avoid the exception for low-resolution images\n            (default is True).\n\n        detector_backend (string): face detector backend. Options: 'opencv', 'retinaface',\n            'mtcnn', 'ssd', 'dlib', 'mediapipe', 'yolov8', 'yolov11n', 'yolov11s', 'yolov11m',\n            'centerface' or 'skip' (default is opencv).\n\n        align (boolean): Perform alignment based on the eye positions (default is True).\n\n        expand_percentage (int): expand detected facial area with a percentage (default is 0).\n\n        normalization (string): Normalize the input image before feeding it to the model.\n            Default is base. Options: base, raw, Facenet, Facenet2018, VGGFace, VGGFace2, ArcFace\n            (default is base).\n\n        anti_spoofing (boolean): Flag to enable anti spoofing (default is False).\n\n        max_faces (int): Set a limit on the number of faces to be processed (default is None).\n\n    Returns:\n        results (List[Dict[str, Any]] or List[Dict[str, Any]]): A list of dictionaries.\n            Result type becomes List of List of Dict if batch input passed.\n            Each containing the following fields:\n\n        - embedding (List[float]): Multidimensional vector representing facial features.\n            The number of dimensions varies based on the reference model\n            (e.g., FaceNet returns 128 dimensions, VGG-Face returns 4096 dimensions).\n\n        - facial_area (dict): Detected facial area by face detection in dictionary format.\n            Contains 'x' and 'y' as the left-corner point, and 'w' and 'h'\n            as the width and height. If `detector_backend` is set to 'skip', it represents\n            the full image area and is nonsensical.\n\n        - face_confidence (float): Confidence score of face detection. If `detector_backend` is set\n            to 'skip', the confidence will be 0 and is nonsensical.\n    \"\"\"\n    return representation.represent(\n        img_path=img_path,\n        model_name=model_name,\n        enforce_detection=enforce_detection,\n        detector_backend=detector_backend,\n        align=align,\n        expand_percentage=expand_percentage,\n        normalization=normalization,\n        anti_spoofing=anti_spoofing,\n        max_faces=max_faces,\n    )\n\n\ndef stream(\n    db_path: str = \"\",\n    model_name: str = \"VGG-Face\",\n    detector_backend: str = \"opencv\",\n    distance_metric: str = \"cosine\",\n    enable_face_analysis: bool = True,\n    source: Any = 0,\n    time_threshold: int = 5,\n    frame_threshold: int = 5,\n    anti_spoofing: bool = False,\n    output_path: Optional[str] = None,\n    debug: bool = False,\n) -> None:\n    \"\"\"\n    Run real time face recognition and facial attribute analysis\n\n    Args:\n        db_path (string): Path to the folder containing image files. All detected faces\n            in the database will be considered in the decision-making process.\n\n        model_name (str): Model for face recognition. Options: VGG-Face, Facenet, Facenet512,\n            OpenFace, DeepFace, DeepID, Dlib, ArcFace, SFace and GhostFaceNet (default is VGG-Face).\n\n        detector_backend (string): face detector backend. Options: 'opencv', 'retinaface',\n            'mtcnn', 'ssd', 'dlib', 'mediapipe', 'yolov8', 'yolov11n', 'yolov11s', 'yolov11m',\n            'centerface' or 'skip' (default is opencv).\n\n        distance_metric (string): Metric for measuring similarity. Options: 'cosine',\n            'euclidean', 'euclidean_l2', 'angular' (default is cosine).\n\n        enable_face_analysis (bool): Flag to enable face analysis (default is True).\n\n        source (Any): The source for the video stream (default is 0, which represents the\n            default camera).\n\n        time_threshold (int): The time threshold (in seconds) for face recognition (default is 5).\n\n        frame_threshold (int): The frame threshold for face recognition (default is 5).\n\n        anti_spoofing (boolean): Flag to enable anti spoofing (default is False).\n\n        output_path (str): Path to save the output video. (default is None\n            If None, no video is saved).\n\n        debug (bool): set this to True to save frame outcomes\n\n    Returns:\n        None\n    \"\"\"\n\n    time_threshold = max(time_threshold, 1)\n    frame_threshold = max(frame_threshold, 1)\n\n    streaming.analysis(\n        db_path=db_path,\n        model_name=model_name,\n        detector_backend=detector_backend,\n        distance_metric=distance_metric,\n        enable_face_analysis=enable_face_analysis,\n        source=source,\n        time_threshold=time_threshold,\n        frame_threshold=frame_threshold,\n        anti_spoofing=anti_spoofing,\n        output_path=output_path,\n        debug=debug,\n    )\n\n\ndef extract_faces(\n    img_path: Union[str, np.ndarray, IO[bytes]],\n    detector_backend: str = \"opencv\",\n    enforce_detection: bool = True,\n    align: bool = True,\n    expand_percentage: int = 0,\n    grayscale: bool = False,\n    color_face: str = \"rgb\",\n    normalize_face: bool = True,\n    anti_spoofing: bool = False,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Extract faces from a given image\n\n    Args:\n        img_path (str or np.ndarray or IO[bytes]): Path to the first image. Accepts exact image path\n            as a string, numpy array (BGR), a file object that supports at least `.read` and is\n            opened in binary mode, or base64 encoded images.\n\n        detector_backend (string): face detector backend. Options: 'opencv', 'retinaface',\n            'mtcnn', 'ssd', 'dlib', 'mediapipe', 'yolov8', 'yolov11n', 'yolov11s', 'yolov11m',\n            'centerface' or 'skip' (default is opencv).\n\n        enforce_detection (boolean): If no face is detected in an image, raise an exception.\n            Set to False to avoid the exception for low-resolution images (default is True).\n\n        align (bool): Flag to enable face alignment (default is True).\n\n        expand_percentage (int): expand detected facial area with a percentage (default is 0).\n\n        grayscale (boolean): (Deprecated) Flag to convert the output face image to grayscale\n            (default is False).\n\n        color_face (string): Color to return face image output. Options: 'rgb', 'bgr' or 'gray'\n            (default is 'rgb').\n\n        normalize_face (boolean): Flag to enable normalization (divide by 255) of the output\n            face image output face image normalization (default is True).\n\n        anti_spoofing (boolean): Flag to enable anti spoofing (default is False).\n\n    Returns:\n        results (List[Dict[str, Any]]): A list of dictionaries, where each dictionary contains:\n\n        - \"face\" (np.ndarray): The detected face as a NumPy array.\n\n        - \"facial_area\" (Dict[str, Any]): The detected face's regions as a dictionary containing:\n            - keys 'x', 'y', 'w', 'h' with int values\n            - keys 'left_eye', 'right_eye' with a tuple of 2 ints as values. left and right eyes\n                are eyes on the left and right respectively with respect to the person itself\n                instead of observer.\n\n        - \"confidence\" (float): The confidence score associated with the detected face.\n\n        - \"is_real\" (boolean): antispoofing analyze result. this key is just available in the\n            result only if anti_spoofing is set to True in input arguments.\n\n        - \"antispoof_score\" (float): score of antispoofing analyze result. this key is\n            just available in the result only if anti_spoofing is set to True in input arguments.\n    \"\"\"\n\n    return detection.extract_faces(\n        img_path=img_path,\n        detector_backend=detector_backend,\n        enforce_detection=enforce_detection,\n        align=align,\n        expand_percentage=expand_percentage,\n        grayscale=grayscale,\n        color_face=color_face,\n        normalize_face=normalize_face,\n        anti_spoofing=anti_spoofing,\n    )\n\n\ndef cli() -> None:\n    \"\"\"\n    command line interface function will be offered in this block\n    \"\"\"\n    import fire\n\n    fire.Fire()\n\n\n# deprecated function(s)\n\n\ndef detectFace(\n    img_path: Union[str, np.ndarray],\n    target_size: tuple = (224, 224),\n    detector_backend: str = \"opencv\",\n    enforce_detection: bool = True,\n    align: bool = True,\n) -> Union[np.ndarray, None]:\n    \"\"\"\n    Deprecated face detection function. Use extract_faces for same functionality.\n\n    Args:\n        img_path (str or np.ndarray): Path to the first image. Accepts exact image path\n            as a string, numpy array (BGR), or base64 encoded images.\n\n        target_size (tuple): final shape of facial image. black pixels will be\n            added to resize the image (default is (224, 224)).\n\n        detector_backend (string): face detector backend. Options: 'opencv', 'retinaface',\n            'mtcnn', 'ssd', 'dlib', 'mediapipe', 'yolov8', 'yolov11n', 'yolov11s', 'yolov11m',\n            'centerface' or 'skip' (default is opencv).\n\n        enforce_detection (boolean): If no face is detected in an image, raise an exception.\n            Set to False to avoid the exception for low-resolution images (default is True).\n\n        align (bool): Flag to enable face alignment (default is True).\n\n    Returns:\n        img (np.ndarray): detected (and aligned) facial area image as numpy array\n    \"\"\"\n    logger.warn(\"Function detectFace is deprecated. Use extract_faces instead.\")\n    face_objs = extract_faces(\n        img_path=img_path,\n        detector_backend=detector_backend,\n        grayscale=False,\n        enforce_detection=enforce_detection,\n        align=align,\n    )\n    extracted_face = None\n    if len(face_objs) > 0:\n        extracted_face = face_objs[0][\"face\"]\n        extracted_face = preprocessing.resize_image(img=extracted_face, target_size=target_size)\n    return extracted_face\n",
            "Example": [
                "from deepface import DeepFace\nresult = DeepFace.verify(img1_path = \"img1.jpg\", img2_path = \"img2.jpg\")\nobjs = DeepFace.analyze(\n  img_path = \"img1.jpg\", actions = ['age', 'gender', 'race', 'emotion']\n)\nprint(\"Is the same person: \", result[\"verified\"])\nprint(\"Age: \", objs[0][\"age\"])\n"
            ]
        }
    ]
}