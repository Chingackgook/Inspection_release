{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/semantic-kernel",
    "API_Calls": [
        {
            "Name": "ChatCompletionAgent_Call",
            "Description": "通过调用chatcompletion接口实现了一个基于​​Semantic Kernel框架​​的多轮对话系统",
            "Code": "# Copyright (c) Microsoft. All rights reserved.\n\nimport asyncio\nfrom typing import Annotated\n\nfrom semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\nfrom semantic_kernel.contents import ChatMessageContent, FunctionCallContent, FunctionResultContent\nfrom semantic_kernel.filters import AutoFunctionInvocationContext\nfrom semantic_kernel.functions import kernel_function\nfrom semantic_kernel.kernel import Kernel\n\n\"\"\"\nThe following sample demonstrates how to configure the auto\nfunction invocation filter while using a ChatCompletionAgent.\nThis allows the developer or user to view the function call content\nand the function result content.\n\"\"\"\n\n\n# Define the auto function invocation filter that will be used by the kernel\nasync def auto_function_invocation_filter(context: AutoFunctionInvocationContext, next):\n    \"\"\"A filter that will be called for each function call in the response.\"\"\"\n    # if we don't call next, it will skip this function, and go to the next one\n    await next(context)\n    if context.function.plugin_name == \"menu\":\n        context.terminate = True\n\n\n# Define a sample plugin for the sample\nclass MenuPlugin:\n    \"\"\"A sample Menu Plugin used for the concept sample.\"\"\"\n\n    @kernel_function(description=\"Provides a list of specials from the menu.\")\n    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n        return \"\"\"\n        Special Soup: Clam Chowder\n        Special Salad: Cobb Salad\n        Special Drink: Chai Tea\n        \"\"\"\n\n    @kernel_function(description=\"Provides the price of the requested menu item.\")\n    def get_item_price(\n        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n        return \"$9.99\"\n\n\ndef _create_kernel_with_chat_completionand_filter() -> Kernel:\n    \"\"\"A helper function to create a kernel with a chat completion service and a filter.\"\"\"\n    kernel = Kernel()\n    kernel.add_service(AzureChatCompletion())\n    kernel.add_filter(\"auto_function_invocation\", auto_function_invocation_filter)\n    kernel.add_plugin(plugin=MenuPlugin(), plugin_name=\"menu\")\n    return kernel\n\n\ndef _write_content(content: ChatMessageContent) -> None:\n    \"\"\"Write the content to the console based on the content type.\"\"\"\n    last_item_type = type(content.items[-1]).__name__ if content.items else \"(empty)\"\n    message_content = \"\"\n    if isinstance(last_item_type, FunctionCallContent):\n        message_content = f\"tool request = {content.items[-1].function_name}\"\n    elif isinstance(last_item_type, FunctionResultContent):\n        message_content = f\"function result = {content.items[-1].result}\"\n    else:\n        message_content = str(content.items[-1])\n    print(f\"[{last_item_type}] {content.role} : '{message_content}'\")\n\n\nasync def main():\n    # 1. Create the agent with a kernel instance that contains\n    # the auto function invocation filter and the AI service\n    agent = ChatCompletionAgent(\n        kernel=_create_kernel_with_chat_completionand_filter(),\n        name=\"Host\",\n        instructions=\"Answer questions about the menu.\",\n    )\n\n    # 2. Define the thread\n    thread: ChatHistoryAgentThread = None\n\n    user_inputs = [\n        \"Hello\",\n        \"What is the special soup?\",\n        \"What is the special drink?\",\n        \"Thank you\",\n    ]\n\n    for user_input in user_inputs:\n        print(f\"# User: '{user_input}'\")\n        # 3. Get the response from the agent\n        response = await agent.get_response(messages=user_input, thread=thread)\n        thread = response.thread\n        _write_content(response)\n\n    print(\"================================\")\n    print(\"CHAT HISTORY\")\n    print(\"================================\")\n\n    # 4. Print out the chat history to view the different types of messages\n    async for message in thread.get_messages():\n        _write_content(message)\n\n    \"\"\"\n    Sample output:\n\n    # AuthorRole.USER: 'Hello'\n    [TextContent] AuthorRole.ASSISTANT : 'Hello! How can I assist you today?'\n    # AuthorRole.USER: 'What is the special soup?'\n    [FunctionResultContent] AuthorRole.TOOL : '\n            Special Soup: Clam Chowder\n            Special Salad: Cobb Salad\n            Special Drink: Chai Tea\n            '\n    # AuthorRole.USER: 'What is the special drink?'\n    [TextContent] AuthorRole.ASSISTANT : 'The special drink is Chai Tea.'\n    # AuthorRole.USER: 'Thank you'\n    [TextContent] AuthorRole.ASSISTANT : 'You're welcome! If you have any more questions or need assistance with \n        anything else, feel free to ask!'\n    ================================\n    CHAT HISTORY\n    ================================\n    [TextContent] AuthorRole.USER : 'Hello'\n    [TextContent] AuthorRole.ASSISTANT : 'Hello! How can I assist you today?'\n    [TextContent] AuthorRole.USER : 'What is the special soup?'\n    [FunctionCallContent] AuthorRole.ASSISTANT : 'menu-get_specials({})'\n    [FunctionResultContent] AuthorRole.TOOL : '\n            Special Soup: Clam Chowder\n            Special Salad: Cobb Salad\n            Special Drink: Chai Tea\n            '\n    [TextContent] AuthorRole.USER : 'What is the special drink?'\n    [TextContent] AuthorRole.ASSISTANT : 'The special drink is Chai Tea.'\n    [TextContent] AuthorRole.USER : 'Thank you'\n    [TextContent] AuthorRole.ASSISTANT : 'You're welcome! If you have any more questions or need assistance with \n        anything else, feel free to ask!'\n    \"\"\"\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/semantic-kernel/python/samples/concepts/agents/chat_completion_agent/chat_completion_agent_function_termination.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class ChatCompletionAgent",
            "Description": "\nChatCompletionAgent定义了一个 基于聊天完成服务（如 OpenAI/Azure）的对话代理（ChatCompletionAgent），用于管理多轮对话、处理函数调用，并与大语言模型（LLM）交互",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/semantic-kernel/python/semantic_kernel/agents/chat_completion/chat_completion_agent.py",
            "Implementation": "class ChatCompletionAgent(Agent):\n    \"\"\"A Chat Completion Agent based on ChatCompletionClientBase.\"\"\"\n\n    function_choice_behavior: FunctionChoiceBehavior | None = Field(\n        default_factory=lambda: FunctionChoiceBehavior.Auto()\n    )\n    channel_type: ClassVar[type[AgentChannel] | None] = ChatHistoryChannel\n    service: ChatCompletionClientBase | None = Field(default=None, exclude=True)\n\n    def __init__(\n        self,\n        *,\n        arguments: KernelArguments | None = None,\n        description: str | None = None,\n        function_choice_behavior: FunctionChoiceBehavior | None = None,\n        id: str | None = None,\n        instructions: str | None = None,\n        kernel: \"Kernel | None\" = None,\n        name: str | None = None,\n        plugins: list[KernelPlugin | object] | dict[str, KernelPlugin | object] | None = None,\n        prompt_template_config: PromptTemplateConfig | None = None,\n        service: ChatCompletionClientBase | None = None,\n    ) -> None:\n        \"\"\"Initialize a new instance of ChatCompletionAgent.\n\n        Args:\n            arguments: The kernel arguments for the agent. Invoke method arguments take precedence over\n                the arguments provided here.\n            description: The description of the agent.\n            function_choice_behavior: The function choice behavior to determine how and which plugins are\n                advertised to the model.\n            kernel: The kernel instance. If both a kernel and a service are provided, the service will take precedence\n                if they share the same service_id or ai_model_id. Otherwise if separate, the first AI service\n                registered on the kernel will be used.\n            id: The unique identifier for the agent. If not provided,\n                a unique GUID will be generated.\n            instructions: The instructions for the agent.\n            name: The name of the agent.\n            plugins: The plugins for the agent. If plugins are included along with a kernel, any plugins\n                that already exist in the kernel will be overwritten.\n            prompt_template_config: The prompt template configuration for the agent.\n            service: The chat completion service instance. If a kernel is provided with the same service_id or\n                `ai_model_id`, the service will take precedence.\n        \"\"\"\n        args: dict[str, Any] = {\n            \"description\": description,\n        }\n        if name is not None:\n            args[\"name\"] = name\n        if id is not None:\n            args[\"id\"] = id\n        if kernel is not None:\n            args[\"kernel\"] = kernel\n        if arguments is not None:\n            args[\"arguments\"] = arguments\n\n        if instructions and prompt_template_config and instructions != prompt_template_config.template:\n            logger.info(\n                f\"Both `instructions` ({instructions}) and `prompt_template_config` \"\n                f\"({prompt_template_config.template}) were provided. Using template in `prompt_template_config` \"\n                \"and ignoring `instructions`.\"\n            )\n\n        if plugins is not None:\n            args[\"plugins\"] = plugins\n\n        if function_choice_behavior is not None:\n            args[\"function_choice_behavior\"] = function_choice_behavior\n\n        if service is not None:\n            args[\"service\"] = service\n\n        if instructions is not None:\n            args[\"instructions\"] = instructions\n        if prompt_template_config is not None:\n            args[\"prompt_template\"] = TEMPLATE_FORMAT_MAP[prompt_template_config.template_format](\n                prompt_template_config=prompt_template_config\n            )\n            if prompt_template_config.template is not None:\n                # Use the template from the prompt_template_config if it is provided\n                args[\"instructions\"] = prompt_template_config.template\n        super().__init__(**args)\n\n    @model_validator(mode=\"after\")\n    def configure_service(self) -> \"ChatCompletionAgent\":\n        \"\"\"Configure the service used by the ChatCompletionAgent.\"\"\"\n        if self.service is None:\n            return self\n        if not isinstance(self.service, ChatCompletionClientBase):\n            raise AgentInitializationException(\n                f\"Service provided for ChatCompletionAgent is not an instance of ChatCompletionClientBase. \"\n                f\"Service: {type(self.service)}\"\n            )\n        self.kernel.add_service(self.service, overwrite=True)\n        return self\n\n    async def create_channel(\n        self, chat_history: ChatHistory | None = None, thread_id: str | None = None\n    ) -> AgentChannel:\n        \"\"\"Create a ChatHistoryChannel.\n\n        Args:\n            chat_history: The chat history for the channel. If None, a new ChatHistory instance will be created.\n            thread_id: The ID of the thread. If None, a new thread will be created.\n\n        Returns:\n            An instance of AgentChannel.\n        \"\"\"\n        from semantic_kernel.agents.chat_completion.chat_completion_agent import ChatHistoryAgentThread\n\n        ChatHistoryChannel.model_rebuild()\n\n        thread = ChatHistoryAgentThread(chat_history=chat_history, thread_id=thread_id)\n\n        if thread.id is None:\n            await thread.create()\n\n        messages = [message async for message in thread.get_messages()]\n\n        return ChatHistoryChannel(messages=messages, thread=thread)\n\n    @trace_agent_get_response\n    @override\n    async def get_response(\n        self,\n        *,\n        messages: str | ChatMessageContent | list[str | ChatMessageContent] | None = None,\n        thread: AgentThread | None = None,\n        arguments: KernelArguments | None = None,\n        kernel: \"Kernel | None\" = None,\n        **kwargs: Any,\n    ) -> AgentResponseItem[ChatMessageContent]:\n        \"\"\"Get a response from the agent.\n\n        Args:\n            messages: The input chat message content either as a string, ChatMessageContent or\n                a list of strings or ChatMessageContent.\n            thread: The thread to use for agent invocation.\n            arguments: The kernel arguments.\n            kernel: The kernel instance.\n            kwargs: The keyword arguments.\n\n        Returns:\n            An AgentResponseItem of type ChatMessageContent.\n        \"\"\"\n        thread = await self._ensure_thread_exists_with_messages(\n            messages=messages,\n            thread=thread,\n            construct_thread=lambda: ChatHistoryAgentThread(),\n            expected_type=ChatHistoryAgentThread,\n        )\n        assert thread.id is not None  # nosec\n\n        chat_history = ChatHistory()\n        async for message in thread.get_messages():\n            chat_history.add_message(message)\n\n        responses: list[ChatMessageContent] = []\n        async for response in self._inner_invoke(\n            thread,\n            chat_history,\n            None,\n            arguments,\n            kernel,\n            **kwargs,\n        ):\n            responses.append(response)\n\n        if not responses:\n            raise AgentInvokeException(\"No response from agent.\")\n\n        return AgentResponseItem(message=responses[-1], thread=thread)\n\n    @trace_agent_invocation\n    @override\n    async def invoke(\n        self,\n        *,\n        messages: str | ChatMessageContent | list[str | ChatMessageContent] | None = None,\n        thread: AgentThread | None = None,\n        on_intermediate_message: Callable[[ChatMessageContent], Awaitable[None]] | None = None,\n        arguments: KernelArguments | None = None,\n        kernel: \"Kernel | None\" = None,\n        **kwargs: Any,\n    ) -> AsyncIterable[AgentResponseItem[ChatMessageContent]]:\n        \"\"\"Invoke the chat history handler.\n\n        Args:\n            messages: The input chat message content either as a string, ChatMessageContent or\n                a list of strings or ChatMessageContent.\n            thread: The thread to use for agent invocation.\n            on_intermediate_message: A callback function to handle intermediate steps of the agent's execution.\n            arguments: The kernel arguments.\n            kernel: The kernel instance.\n            kwargs: The keyword arguments.\n\n        Returns:\n            An async iterable of AgentResponseItem of type ChatMessageContent.\n        \"\"\"\n        thread = await self._ensure_thread_exists_with_messages(\n            messages=messages,\n            thread=thread,\n            construct_thread=lambda: ChatHistoryAgentThread(),\n            expected_type=ChatHistoryAgentThread,\n        )\n        assert thread.id is not None  # nosec\n\n        chat_history = ChatHistory()\n        async for message in thread.get_messages():\n            chat_history.add_message(message)\n\n        async for response in self._inner_invoke(\n            thread,\n            chat_history,\n            on_intermediate_message,\n            arguments,\n            kernel,\n            **kwargs,\n        ):\n            yield AgentResponseItem(message=response, thread=thread)\n\n    @trace_agent_invocation\n    @override\n    async def invoke_stream(\n        self,\n        *,\n        messages: str | ChatMessageContent | list[str | ChatMessageContent] | None = None,\n        thread: AgentThread | None = None,\n        on_intermediate_message: Callable[[ChatMessageContent], Awaitable[None]] | None = None,\n        arguments: KernelArguments | None = None,\n        kernel: \"Kernel | None\" = None,\n        **kwargs: Any,\n    ) -> AsyncIterable[AgentResponseItem[StreamingChatMessageContent]]:\n        \"\"\"Invoke the chat history handler in streaming mode.\n\n        Args:\n            messages: The chat message content either as a string, ChatMessageContent or\n                a list of str or ChatMessageContent.\n            thread: The thread to use for agent invocation.\n            on_intermediate_message: A callback function to handle intermediate steps of the\n                                     agent's execution as fully formed messages.\n            arguments: The kernel arguments.\n            kernel: The kernel instance.\n            kwargs: The keyword arguments.\n\n        Returns:\n            An async generator of AgentResponseItem of type StreamingChatMessageContent.\n        \"\"\"\n        thread = await self._ensure_thread_exists_with_messages(\n            messages=messages,\n            thread=thread,\n            construct_thread=lambda: ChatHistoryAgentThread(),\n            expected_type=ChatHistoryAgentThread,\n        )\n        assert thread.id is not None  # nosec\n\n        chat_history = ChatHistory()\n        async for message in thread.get_messages():\n            chat_history.add_message(message)\n\n        if arguments is None:\n            arguments = KernelArguments(**kwargs)\n        else:\n            arguments.update(kwargs)\n\n        kernel = kernel or self.kernel\n        arguments = self._merge_arguments(arguments)\n\n        chat_completion_service, settings = await self._get_chat_completion_service_and_settings(\n            kernel=kernel, arguments=arguments\n        )\n\n        # If the user hasn't provided a function choice behavior, use the agent's default.\n        if settings.function_choice_behavior is None:\n            settings.function_choice_behavior = self.function_choice_behavior\n\n        agent_chat_history = await self._prepare_agent_chat_history(\n            history=chat_history,\n            kernel=kernel,\n            arguments=arguments,\n        )\n\n        message_count_before_completion = len(agent_chat_history)\n\n        logger.debug(f\"[{type(self).__name__}] Invoking {type(chat_completion_service).__name__}.\")\n\n        responses: AsyncGenerator[list[StreamingChatMessageContent], Any] = (\n            chat_completion_service.get_streaming_chat_message_contents(\n                chat_history=agent_chat_history,\n                settings=settings,\n                kernel=kernel,\n                arguments=arguments,\n            )\n        )\n\n        logger.debug(\n            f\"[{type(self).__name__}] Invoked {type(chat_completion_service).__name__} \"\n            f\"with message count: {message_count_before_completion}.\"\n        )\n\n        role = None\n        response_builder: list[str] = []\n        async for response_list in responses:\n            for response in response_list:\n                role = response.role\n                response.name = self.name\n                response_builder.append(response.content)\n\n                if (\n                    role == AuthorRole.ASSISTANT\n                    and response.items\n                    and not any(\n                        isinstance(item, (FunctionCallContent, FunctionResultContent)) for item in response.items\n                    )\n                ):\n                    yield AgentResponseItem(message=response, thread=thread)\n\n        await self._capture_mutated_messages(\n            agent_chat_history,\n            message_count_before_completion,\n            thread,\n            on_intermediate_message,\n        )\n\n        if role != AuthorRole.TOOL:\n            # Tool messages will be automatically added to the chat history by the auto function invocation loop\n            # if it's the response (i.e. terminated by a filter), thus we need to avoid notifying the thread about\n            # them multiple times.\n            await thread.on_new_message(\n                ChatMessageContent(\n                    role=role if role else AuthorRole.ASSISTANT, content=\"\".join(response_builder), name=self.name\n                )\n            )\n\n    async def _inner_invoke(\n        self,\n        thread: ChatHistoryAgentThread,\n        history: ChatHistory,\n        on_intermediate_message: Callable[[ChatMessageContent], Awaitable[None]] | None = None,\n        arguments: KernelArguments | None = None,\n        kernel: \"Kernel | None\" = None,\n        **kwargs: Any,\n    ) -> AsyncIterable[ChatMessageContent]:\n        \"\"\"Helper method to invoke the agent with a chat history in non-streaming mode.\"\"\"\n        if arguments is None:\n            arguments = KernelArguments(**kwargs)\n        else:\n            arguments.update(kwargs)\n\n        kernel = kernel or self.kernel\n        arguments = self._merge_arguments(arguments)\n\n        chat_completion_service, settings = await self._get_chat_completion_service_and_settings(\n            kernel=kernel, arguments=arguments\n        )\n\n        # If the user hasn't provided a function choice behavior, use the agent's default.\n        if settings.function_choice_behavior is None:\n            settings.function_choice_behavior = self.function_choice_behavior\n\n        agent_chat_history = await self._prepare_agent_chat_history(\n            history=history,\n            kernel=kernel,\n            arguments=arguments,\n        )\n\n        message_count_before_completion = len(agent_chat_history)\n\n        logger.debug(f\"[{type(self).__name__}] Invoking {type(chat_completion_service).__name__}.\")\n\n        responses = await chat_completion_service.get_chat_message_contents(\n            chat_history=agent_chat_history,\n            settings=settings,\n            kernel=kernel,\n            arguments=arguments,\n        )\n\n        logger.debug(\n            f\"[{type(self).__name__}] Invoked {type(chat_completion_service).__name__} \"\n            f\"with message count: {message_count_before_completion}.\"\n        )\n\n        await self._capture_mutated_messages(\n            agent_chat_history,\n            message_count_before_completion,\n            thread,\n            on_intermediate_message,\n        )\n\n        for response in responses:\n            response.name = self.name\n            if response.role != AuthorRole.TOOL:\n                # Tool messages will be automatically added to the chat history by the auto function invocation loop\n                # if it's the response (i.e. terminated by a filter),, thus we need to avoid notifying the thread about\n                # them multiple times.\n                await thread.on_new_message(response)\n            yield response\n\n    async def _prepare_agent_chat_history(\n        self, history: ChatHistory, kernel: \"Kernel\", arguments: KernelArguments\n    ) -> ChatHistory:\n        \"\"\"Prepare the agent chat history from the input history by adding the formatted instructions.\"\"\"\n        formatted_instructions = await self.format_instructions(kernel, arguments)\n        messages = []\n        if formatted_instructions:\n            messages.append(ChatMessageContent(role=AuthorRole.SYSTEM, content=formatted_instructions, name=self.name))\n        if history.messages:\n            messages.extend(history.messages)\n\n        return ChatHistory(messages=messages)\n\n    async def _get_chat_completion_service_and_settings(\n        self, kernel: \"Kernel\", arguments: KernelArguments\n    ) -> tuple[ChatCompletionClientBase, PromptExecutionSettings]:\n        \"\"\"Get the chat completion service and settings.\"\"\"\n        chat_completion_service, settings = kernel.select_ai_service(arguments=arguments, type=ChatCompletionClientBase)\n\n        if not chat_completion_service:\n            raise KernelServiceNotFoundError(\n                \"Chat completion service not found. Check your service or kernel configuration.\"\n            )\n\n        assert isinstance(chat_completion_service, ChatCompletionClientBase)  # nosec\n        assert settings is not None  # nosec\n\n        return chat_completion_service, settings\n\n    async def _capture_mutated_messages(\n        self,\n        agent_chat_history: ChatHistory,\n        start: int,\n        thread: ChatHistoryAgentThread,\n        on_intermediate_message: Callable[[ChatMessageContent], Awaitable[None]] | None = None,\n    ) -> None:\n        \"\"\"Capture mutated messages related function calling/tools.\"\"\"\n        for message_index in range(start, len(agent_chat_history)):\n            message = agent_chat_history[message_index]  # type: ignore\n            message.name = self.name\n            await thread.on_new_message(message)\n\n            if on_intermediate_message:\n                await on_intermediate_message(message)\n",
            "Example": [
            ]
        }
    ]
}