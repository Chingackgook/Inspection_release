{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/StreamDiffusion",
    "API_Calls": [
        {
            "Name": "call_StreamDiffusionWrapper",
            "Description": "call_StreamDiffusionWrapper",
            "Code": "import os\nimport sys\nfrom typing import Literal, Dict, Optional\n\nimport fire\n\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\", \"..\"))\n\nfrom utils.wrapper import StreamDiffusionWrapper\n\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\ndef main(\n    input: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"inputs\", \"input.png\"),\n    output: str = os.path.join(CURRENT_DIR, \"..\", \"..\", \"images\", \"outputs\", \"output.png\"),\n    model_id_or_path: str = \"KBlueLeaf/kohaku-v2.1\",\n    lora_dict: Optional[Dict[str, float]] = None,\n    prompt: str = \"1girl with brown dog hair, thick glasses, smiling\",\n    negative_prompt: str = \"low quality, bad quality, blurry, low resolution\",\n    width: int = 512,\n    height: int = 512,\n    acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"xformers\",\n    use_denoising_batch: bool = True,\n    guidance_scale: float = 1.2,\n    cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n    seed: int = 2,\n    delta: float = 0.5,\n):\n    \"\"\"\n    Initializes the StreamDiffusionWrapper.\n\n    Parameters\n    ----------\n    input : str, optional\n        The input image file to load images from.\n    output : str, optional\n        The output image file to save images to.\n    model_id_or_path : str\n        The model id or path to load.\n    lora_dict : Optional[Dict[str, float]], optional\n        The lora_dict to load, by default None.\n        Keys are the LoRA names and values are the LoRA scales.\n        Example: {'LoRA_1' : 0.5 , 'LoRA_2' : 0.7 ,...}\n    prompt : str\n        The prompt to generate images from.\n    negative_prompt : str, optional\n        The negative prompt to use.\n    width : int, optional\n        The width of the image, by default 512.\n    height : int, optional\n        The height of the image, by default 512.\n    acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n        The acceleration method, by default \"tensorrt\".\n    use_denoising_batch : bool, optional\n        Whether to use denoising batch or not, by default True.\n    guidance_scale : float, optional\n        The CFG scale, by default 1.2.\n    cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n    optional\n        The cfg_type for img2img mode, by default \"self\".\n        You cannot use anything other than \"none\" for txt2img mode.\n    seed : int, optional\n        The seed, by default 2. if -1, use random seed.\n    delta : float, optional\n        The delta multiplier of virtual residual noise,\n        by default 1.0.\n    \"\"\"\n\n    if guidance_scale <= 1.0:\n        cfg_type = \"none\"\n\n    stream = StreamDiffusionWrapper(\n        model_id_or_path=model_id_or_path,\n        lora_dict=lora_dict,\n        t_index_list=[22, 32, 45],\n        frame_buffer_size=1,\n        width=width,\n        height=height,\n        warmup=10,\n        acceleration=acceleration,\n        mode=\"img2img\",\n        use_denoising_batch=use_denoising_batch,\n        cfg_type=cfg_type,\n        seed=seed,\n    )\n\n    stream.prepare(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=50,\n        guidance_scale=guidance_scale,\n        delta=delta,\n    )\n\n    image_tensor = stream.preprocess_image(input)\n\n    for _ in range(stream.batch_size - 1):\n        stream(image=image_tensor)\n\n    output_image = stream(image=image_tensor)\n    output_image.save(output)\n\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/StreamDiffusion/examples/img2img/single.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "StreamDiffusionWrapper",
            "Description": "StreamDiffusionWrapper",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/StreamDiffusion/utils/wrapper.py",
            "Implementation": "import gc\nimport os\nfrom pathlib import Path\nimport traceback\nfrom typing import List, Literal, Optional, Union, Dict\n\nimport numpy as np\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline\nfrom PIL import Image\n\nfrom streamdiffusion import StreamDiffusion\nfrom streamdiffusion.image_utils import postprocess_image\n\n\ntorch.set_grad_enabled(False)\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n\nclass StreamDiffusionWrapper:\n    def __init__(\n        self,\n        model_id_or_path: str,\n        t_index_list: List[int],\n        lora_dict: Optional[Dict[str, float]] = None,\n        mode: Literal[\"img2img\", \"txt2img\"] = \"img2img\",\n        output_type: Literal[\"pil\", \"pt\", \"np\", \"latent\"] = \"pil\",\n        lcm_lora_id: Optional[str] = None,\n        vae_id: Optional[str] = None,\n        device: Literal[\"cpu\", \"cuda\"] = \"cuda\",\n        dtype: torch.dtype = torch.float16,\n        frame_buffer_size: int = 1,\n        width: int = 512,\n        height: int = 512,\n        warmup: int = 10,\n        acceleration: Literal[\"none\", \"xformers\", \"tensorrt\"] = \"tensorrt\",\n        do_add_noise: bool = True,\n        device_ids: Optional[List[int]] = None,\n        use_lcm_lora: bool = True,\n        use_tiny_vae: bool = True,\n        enable_similar_image_filter: bool = False,\n        similar_image_filter_threshold: float = 0.98,\n        similar_image_filter_max_skip_frame: int = 10,\n        use_denoising_batch: bool = True,\n        cfg_type: Literal[\"none\", \"full\", \"self\", \"initialize\"] = \"self\",\n        seed: int = 2,\n        use_safety_checker: bool = False,\n        engine_dir: Optional[Union[str, Path]] = \"engines\",\n    ):\n        \"\"\"\n        Initializes the StreamDiffusionWrapper.\n\n        Parameters\n        ----------\n        model_id_or_path : str\n            The model id or path to load.\n        t_index_list : List[int]\n            The t_index_list to use for inference.\n        lora_dict : Optional[Dict[str, float]], optional\n            The lora_dict to load, by default None.\n            Keys are the LoRA names and values are the LoRA scales.\n            Example: {'LoRA_1' : 0.5 , 'LoRA_2' : 0.7 ,...}\n        mode : Literal[\"img2img\", \"txt2img\"], optional\n            txt2img or img2img, by default \"img2img\".\n        output_type : Literal[\"pil\", \"pt\", \"np\", \"latent\"], optional\n            The output type of image, by default \"pil\".\n        lcm_lora_id : Optional[str], optional\n            The lcm_lora_id to load, by default None.\n            If None, the default LCM-LoRA\n            (\"latent-consistency/lcm-lora-sdv1-5\") will be used.\n        vae_id : Optional[str], optional\n            The vae_id to load, by default None.\n            If None, the default TinyVAE\n            (\"madebyollin/taesd\") will be used.\n        device : Literal[\"cpu\", \"cuda\"], optional\n            The device to use for inference, by default \"cuda\".\n        dtype : torch.dtype, optional\n            The dtype for inference, by default torch.float16.\n        frame_buffer_size : int, optional\n            The frame buffer size for denoising batch, by default 1.\n        width : int, optional\n            The width of the image, by default 512.\n        height : int, optional\n            The height of the image, by default 512.\n        warmup : int, optional\n            The number of warmup steps to perform, by default 10.\n        acceleration : Literal[\"none\", \"xformers\", \"tensorrt\"], optional\n            The acceleration method, by default \"tensorrt\".\n        do_add_noise : bool, optional\n            Whether to add noise for following denoising steps or not,\n            by default True.\n        device_ids : Optional[List[int]], optional\n            The device ids to use for DataParallel, by default None.\n        use_lcm_lora : bool, optional\n            Whether to use LCM-LoRA or not, by default True.\n        use_tiny_vae : bool, optional\n            Whether to use TinyVAE or not, by default True.\n        enable_similar_image_filter : bool, optional\n            Whether to enable similar image filter or not,\n            by default False.\n        similar_image_filter_threshold : float, optional\n            The threshold for similar image filter, by default 0.98.\n        similar_image_filter_max_skip_frame : int, optional\n            The max skip frame for similar image filter, by default 10.\n        use_denoising_batch : bool, optional\n            Whether to use denoising batch or not, by default True.\n        cfg_type : Literal[\"none\", \"full\", \"self\", \"initialize\"],\n        optional\n            The cfg_type for img2img mode, by default \"self\".\n            You cannot use anything other than \"none\" for txt2img mode.\n        seed : int, optional\n            The seed, by default 2.\n        use_safety_checker : bool, optional\n            Whether to use safety checker or not, by default False.\n        \"\"\"\n        self.sd_turbo = \"turbo\" in model_id_or_path\n\n        if mode == \"txt2img\":\n            if cfg_type != \"none\":\n                raise ValueError(\n                    f\"txt2img mode accepts only cfg_type = 'none', but got {cfg_type}\"\n                )\n            if use_denoising_batch and frame_buffer_size > 1:\n                if not self.sd_turbo:\n                    raise ValueError(\n                        \"txt2img mode cannot use denoising batch with frame_buffer_size > 1.\"\n                    )\n\n        if mode == \"img2img\":\n            if not use_denoising_batch:\n                raise NotImplementedError(\n                    \"img2img mode must use denoising batch for now.\"\n                )\n\n        self.device = device\n        self.dtype = dtype\n        self.width = width\n        self.height = height\n        self.mode = mode\n        self.output_type = output_type\n        self.frame_buffer_size = frame_buffer_size\n        self.batch_size = (\n            len(t_index_list) * frame_buffer_size\n            if use_denoising_batch\n            else frame_buffer_size\n        )\n\n        self.use_denoising_batch = use_denoising_batch\n        self.use_safety_checker = use_safety_checker\n\n        self.stream: StreamDiffusion = self._load_model(\n            model_id_or_path=model_id_or_path,\n            lora_dict=lora_dict,\n            lcm_lora_id=lcm_lora_id,\n            vae_id=vae_id,\n            t_index_list=t_index_list,\n            acceleration=acceleration,\n            warmup=warmup,\n            do_add_noise=do_add_noise,\n            use_lcm_lora=use_lcm_lora,\n            use_tiny_vae=use_tiny_vae,\n            cfg_type=cfg_type,\n            seed=seed,\n            engine_dir=engine_dir,\n        )\n\n        if device_ids is not None:\n            self.stream.unet = torch.nn.DataParallel(\n                self.stream.unet, device_ids=device_ids\n            )\n\n        if enable_similar_image_filter:\n            self.stream.enable_similar_image_filter(similar_image_filter_threshold, similar_image_filter_max_skip_frame)\n\n    def prepare(\n        self,\n        prompt: str,\n        negative_prompt: str = \"\",\n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.2,\n        delta: float = 1.0,\n    ) -> None:\n        \"\"\"\n        Prepares the model for inference.\n\n        Parameters\n        ----------\n        prompt : str\n            The prompt to generate images from.\n        num_inference_steps : int, optional\n            The number of inference steps to perform, by default 50.\n        guidance_scale : float, optional\n            The guidance scale to use, by default 1.2.\n        delta : float, optional\n            The delta multiplier of virtual residual noise,\n            by default 1.0.\n        \"\"\"\n        self.stream.prepare(\n            prompt,\n            negative_prompt,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            delta=delta,\n        )\n\n    def __call__(\n        self,\n        image: Optional[Union[str, Image.Image, torch.Tensor]] = None,\n        prompt: Optional[str] = None,\n    ) -> Union[Image.Image, List[Image.Image]]:\n        \"\"\"\n        Performs img2img or txt2img based on the mode.\n\n        Parameters\n        ----------\n        image : Optional[Union[str, Image.Image, torch.Tensor]]\n            The image to generate from.\n        prompt : Optional[str]\n            The prompt to generate images from.\n\n        Returns\n        -------\n        Union[Image.Image, List[Image.Image]]\n            The generated image.\n        \"\"\"\n        if self.mode == \"img2img\":\n            return self.img2img(image, prompt)\n        else:\n            return self.txt2img(prompt)",
            "Examples": [
                "\n"
            ]
        }
    ]
}