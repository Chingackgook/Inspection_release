{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/CosyVoice",
    "API_Calls": [
        {
            "Name": "CosyVoice2_generate",
            "Code": "#from modelscope import snapshot_download\n#snapshot_download('iic/CosyVoice2-0.5B', local_dir='pretrained_models/CosyVoice2-0.5B')\nimport sys\nsys.path.append('third_party/Matcha-TTS')\nfrom cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\nfrom cosyvoice.utils.file_utils import load_wav\nimport torchaudio\n\ncosyvoice = CosyVoice2('pretrained_models/CosyVoice2-0.5B', load_jit=False, load_trt=False, fp16=False, use_flow_cache=False)\n\n# NOTE if you want to reproduce the results on https://funaudiollm.github.io/cosyvoice2, please add text_frontend=False during inference\n# zero_shot usage\nprompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)\nfor i, j in enumerate(cosyvoice.inference_zero_shot('收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '希望你以后能够做的比我还好呦。', prompt_speech_16k, stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n\n# save zero_shot spk for future usage\nassert cosyvoice.add_zero_shot_spk('希望你以后能够做的比我还好呦。', prompt_speech_16k, 'my_zero_shot_spk') is True\nfor i, j in enumerate(cosyvoice.inference_zero_shot('收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '', '', zero_shot_spk_id='my_zero_shot_spk', stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\ncosyvoice.save_spkinfo()\n\n# fine grained control, for supported control, check cosyvoice/tokenizer/tokenizer.py#L248\nfor i, j in enumerate(cosyvoice.inference_cross_lingual('在他讲述那个荒诞故事的过程中，他突然[laughter]停下来，因为他自己也被逗笑了[laughter]。', prompt_speech_16k, stream=False)):\n    torchaudio.save('fine_grained_control_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n\n# instruct usage\nfor i, j in enumerate(cosyvoice.inference_instruct2('收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '用四川话说这句话', prompt_speech_16k, stream=False)):\n    torchaudio.save('instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n\n# bistream usage, you can use generator as input, this is useful when using text llm model as input\n# NOTE you should still have some basic sentence split logic because llm can not handle arbitrary sentence length\ndef text_generator():\n    yield '收到好友从远方寄来的生日礼物，'\n    yield '那份意外的惊喜与深深的祝福'\n    yield '让我心中充满了甜蜜的快乐，'\n    yield '笑容如花儿般绽放。'\nfor i, j in enumerate(cosyvoice.inference_zero_shot(text_generator(), '希望你以后能够做的比我还好呦。', prompt_speech_16k, stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n",
            "Description": "这是一个语音合成的例子，使用了CosyVoice2模型进行语音合成。代码中包含了如何加载模型、进行语音合成、保存合成结果等操作。",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/CosyVoice/generate.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "CosyVoice2(CosyVoice)",
            "Description": "这是两个上层类，一个是CosyVoice，一个是CosyVoice2，都是用来进行语音合成的。CosyVoice2是CosyVoice的升级版，支持更多的功能和更好的性能，我们后续的测试是基于CosyVoice2的。",
            "Implementation": "import os\nimport time\nfrom typing import Generator\nfrom tqdm import tqdm\nfrom hyperpyyaml import load_hyperpyyaml\nfrom modelscope import snapshot_download\nimport torch\nfrom cosyvoice.cli.frontend import CosyVoiceFrontEnd\nfrom cosyvoice.cli.model import CosyVoiceModel, CosyVoice2Model\nfrom cosyvoice.utils.file_utils import logging\nfrom cosyvoice.utils.class_utils import get_model_type\n\n\nclass CosyVoice:\n\n    def __init__(self, model_dir, load_jit=False, load_trt=False, fp16=False):\n        self.instruct = True if '-Instruct' in model_dir else False\n        self.model_dir = model_dir\n        self.fp16 = fp16\n        if not os.path.exists(model_dir):\n            model_dir = snapshot_download(model_dir)\n        hyper_yaml_path = '{}/cosyvoice.yaml'.format(model_dir)\n        if not os.path.exists(hyper_yaml_path):\n            raise ValueError('{} not found!'.format(hyper_yaml_path))\n        with open(hyper_yaml_path, 'r') as f:\n            configs = load_hyperpyyaml(f)\n        assert get_model_type(configs) != CosyVoice2Model, 'do not use {} for CosyVoice initialization!'.format(model_dir)\n        self.frontend = CosyVoiceFrontEnd(configs['get_tokenizer'],\n                                          configs['feat_extractor'],\n                                          '{}/campplus.onnx'.format(model_dir),\n                                          '{}/speech_tokenizer_v1.onnx'.format(model_dir),\n                                          '{}/spk2info.pt'.format(model_dir),\n                                          configs['allowed_special'])\n        self.sample_rate = configs['sample_rate']\n        if torch.cuda.is_available() is False and (load_jit is True or load_trt is True or fp16 is True):\n            load_jit, load_trt, fp16 = False, False, False\n            logging.warning('no cuda device, set load_jit/load_trt/fp16 to False')\n        self.model = CosyVoiceModel(configs['llm'], configs['flow'], configs['hift'], fp16)\n        self.model.load('{}/llm.pt'.format(model_dir),\n                        '{}/flow.pt'.format(model_dir),\n                        '{}/hift.pt'.format(model_dir))\n        if load_jit:\n            self.model.load_jit('{}/llm.text_encoder.{}.zip'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'),\n                                '{}/llm.llm.{}.zip'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'),\n                                '{}/flow.encoder.{}.zip'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'))\n        if load_trt:\n            self.model.load_trt('{}/flow.decoder.estimator.{}.mygpu.plan'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'),\n                                '{}/flow.decoder.estimator.fp32.onnx'.format(model_dir),\n                                self.fp16)\n        del configs\n\n    def list_available_spks(self):\n        spks = list(self.frontend.spk2info.keys())\n        return spks\n\n    def add_zero_shot_spk(self, prompt_text, prompt_speech_16k, zero_shot_spk_id):\n        assert zero_shot_spk_id != '', 'do not use empty zero_shot_spk_id'\n        model_input = self.frontend.frontend_zero_shot('', prompt_text, prompt_speech_16k, self.sample_rate, '')\n        del model_input['text']\n        del model_input['text_len']\n        self.frontend.spk2info[zero_shot_spk_id] = model_input\n        return True\n\n    def save_spkinfo(self):\n        torch.save(self.frontend.spk2info, '{}/spk2info.pt'.format(self.model_dir))\n\n    def inference_sft(self, tts_text, spk_id, stream=False, speed=1.0, text_frontend=True):\n        for i in tqdm(self.frontend.text_normalize(tts_text, split=True, text_frontend=text_frontend)):\n            model_input = self.frontend.frontend_sft(i, spk_id)\n            start_time = time.time()\n            logging.info('synthesis text {}'.format(i))\n            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n                speech_len = model_output['tts_speech'].shape[1] / self.sample_rate\n                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n                yield model_output\n                start_time = time.time()\n\n    def inference_zero_shot(self, tts_text, prompt_text, prompt_speech_16k, zero_shot_spk_id='', stream=False, speed=1.0, text_frontend=True):\n        prompt_text = self.frontend.text_normalize(prompt_text, split=False, text_frontend=text_frontend)\n        for i in tqdm(self.frontend.text_normalize(tts_text, split=True, text_frontend=text_frontend)):\n            if (not isinstance(i, Generator)) and len(i) < 0.5 * len(prompt_text):\n                logging.warning('synthesis text {} too short than prompt text {}, this may lead to bad performance'.format(i, prompt_text))\n            model_input = self.frontend.frontend_zero_shot(i, prompt_text, prompt_speech_16k, self.sample_rate, zero_shot_spk_id)\n            start_time = time.time()\n            logging.info('synthesis text {}'.format(i))\n            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n                speech_len = model_output['tts_speech'].shape[1] / self.sample_rate\n                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n                yield model_output\n                start_time = time.time()\n\n    def inference_cross_lingual(self, tts_text, prompt_speech_16k, zero_shot_spk_id='', stream=False, speed=1.0, text_frontend=True):\n        for i in tqdm(self.frontend.text_normalize(tts_text, split=True, text_frontend=text_frontend)):\n            model_input = self.frontend.frontend_cross_lingual(i, prompt_speech_16k, self.sample_rate, zero_shot_spk_id)\n            start_time = time.time()\n            logging.info('synthesis text {}'.format(i))\n            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n                speech_len = model_output['tts_speech'].shape[1] / self.sample_rate\n                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n                yield model_output\n                start_time = time.time()\n\n    def inference_instruct(self, tts_text, spk_id, instruct_text, stream=False, speed=1.0, text_frontend=True):\n        assert isinstance(self.model, CosyVoiceModel), 'inference_instruct is only implemented for CosyVoice!'\n        if self.instruct is False:\n            raise ValueError('{} do not support instruct inference'.format(self.model_dir))\n        instruct_text = self.frontend.text_normalize(instruct_text, split=False, text_frontend=text_frontend)\n        for i in tqdm(self.frontend.text_normalize(tts_text, split=True, text_frontend=text_frontend)):\n            model_input = self.frontend.frontend_instruct(i, spk_id, instruct_text)\n            start_time = time.time()\n            logging.info('synthesis text {}'.format(i))\n            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n                speech_len = model_output['tts_speech'].shape[1] / self.sample_rate\n                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n                yield model_output\n                start_time = time.time()\n\n    def inference_vc(self, source_speech_16k, prompt_speech_16k, stream=False, speed=1.0):\n        model_input = self.frontend.frontend_vc(source_speech_16k, prompt_speech_16k, self.sample_rate)\n        start_time = time.time()\n        for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n            speech_len = model_output['tts_speech'].shape[1] / self.sample_rate\n            logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n            yield model_output\n            start_time = time.time()\n\n\nclass CosyVoice2(CosyVoice):\n\n    def __init__(self, model_dir, load_jit=False, load_trt=False, fp16=False, use_flow_cache=False):\n        self.instruct = True if '-Instruct' in model_dir else False\n        self.model_dir = model_dir\n        self.fp16 = fp16\n        if not os.path.exists(model_dir):\n            model_dir = snapshot_download(model_dir)\n        hyper_yaml_path = '{}/cosyvoice2.yaml'.format(model_dir)\n        if not os.path.exists(hyper_yaml_path):\n            raise ValueError('{} not found!'.format(hyper_yaml_path))\n        with open(hyper_yaml_path, 'r') as f:\n            configs = load_hyperpyyaml(f, overrides={'qwen_pretrain_path': os.path.join(model_dir, 'CosyVoice-BlankEN')})\n        assert get_model_type(configs) == CosyVoice2Model, 'do not use {} for CosyVoice2 initialization!'.format(model_dir)\n        self.frontend = CosyVoiceFrontEnd(configs['get_tokenizer'],\n                                          configs['feat_extractor'],\n                                          '{}/campplus.onnx'.format(model_dir),\n                                          '{}/speech_tokenizer_v2.onnx'.format(model_dir),\n                                          '{}/spk2info.pt'.format(model_dir),\n                                          configs['allowed_special'])\n        self.sample_rate = configs['sample_rate']\n        if torch.cuda.is_available() is False and (load_jit is True or load_trt is True or fp16 is True):\n            load_jit, load_trt, fp16 = False, False, False\n            logging.warning('no cuda device, set load_jit/load_trt/fp16 to False')\n        self.model = CosyVoice2Model(configs['llm'], configs['flow'], configs['hift'], fp16, use_flow_cache)\n        self.model.load('{}/llm.pt'.format(model_dir),\n                        '{}/flow.pt'.format(model_dir) if use_flow_cache is False else '{}/flow.cache.pt'.format(model_dir),\n                        '{}/hift.pt'.format(model_dir))\n        if load_jit:\n            self.model.load_jit('{}/flow.encoder.{}.zip'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'))\n        if load_trt:\n            self.model.load_trt('{}/flow.decoder.estimator.{}.mygpu.plan'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'),\n                                '{}/flow.decoder.estimator.fp32.onnx'.format(model_dir),\n                                self.fp16)\n        del configs\n\n    def inference_instruct(self, *args, **kwargs):\n        raise NotImplementedError('inference_instruct is not implemented for CosyVoice2!')\n\n    def inference_instruct2(self, tts_text, instruct_text, prompt_speech_16k, zero_shot_spk_id='', stream=False, speed=1.0, text_frontend=True):\n        assert isinstance(self.model, CosyVoice2Model), 'inference_instruct2 is only implemented for CosyVoice2!'\n        for i in tqdm(self.frontend.text_normalize(tts_text, split=True, text_frontend=text_frontend)):\n            model_input = self.frontend.frontend_instruct2(i, instruct_text, prompt_speech_16k, self.sample_rate, zero_shot_spk_id)\n            start_time = time.time()\n            logging.info('synthesis text {}'.format(i))\n            for model_output in self.model.tts(**model_input, stream=stream, speed=speed):\n                speech_len = model_output['tts_speech'].shape[1] / self.sample_rate\n                logging.info('yield speech len {}, rtf {}'.format(speech_len, (time.time() - start_time) / speech_len))\n                yield model_output\n                start_time = time.time()",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/CosyVoice/cosyvoice/cli/cosyvoice.py",
            "Examples": [
                {
                    "Description": "这是一个语音合成的例子，使用了CosyVoice2模型进行语音合成。代码中包含了如何加载模型、进行语音合成、保存合成结果等操作。",
                    "Path": "/mnt/autor_name/haoTingDeWenJianJia/CosyVoice/generate.py",
                    "Code": "#from modelscope import snapshot_download\n#snapshot_download('iic/CosyVoice2-0.5B', local_dir='pretrained_models/CosyVoice2-0.5B')\nimport sys\nsys.path.append('third_party/Matcha-TTS')\nfrom cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\nfrom cosyvoice.utils.file_utils import load_wav\nimport torchaudio\n\ncosyvoice = CosyVoice2('pretrained_models/CosyVoice2-0.5B', load_jit=False, load_trt=False, fp16=False, use_flow_cache=False)\n\n# NOTE if you want to reproduce the results on https://funaudiollm.github.io/cosyvoice2, please add text_frontend=False during inference\n# zero_shot usage\nprompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)\nfor i, j in enumerate(cosyvoice.inference_zero_shot('收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '希望你以后能够做的比我还好呦。', prompt_speech_16k, stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n\n# save zero_shot spk for future usage\nassert cosyvoice.add_zero_shot_spk('希望你以后能够做的比我还好呦。', prompt_speech_16k, 'my_zero_shot_spk') is True\nfor i, j in enumerate(cosyvoice.inference_zero_shot('收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '', '', zero_shot_spk_id='my_zero_shot_spk', stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\ncosyvoice.save_spkinfo()\n\n# fine grained control, for supported control, check cosyvoice/tokenizer/tokenizer.py#L248\nfor i, j in enumerate(cosyvoice.inference_cross_lingual('在他讲述那个荒诞故事的过程中，他突然[laughter]停下来，因为他自己也被逗笑了[laughter]。', prompt_speech_16k, stream=False)):\n    torchaudio.save('fine_grained_control_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n\n# instruct usage\nfor i, j in enumerate(cosyvoice.inference_instruct2('收到好友从远方寄来的生日礼物，那份意外的惊喜与深深的祝福让我心中充满了甜蜜的快乐，笑容如花儿般绽放。', '用四川话说这句话', prompt_speech_16k, stream=False)):\n    torchaudio.save('instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n\n# bistream usage, you can use generator as input, this is useful when using text llm model as input\n# NOTE you should still have some basic sentence split logic because llm can not handle arbitrary sentence length\ndef text_generator():\n    yield '收到好友从远方寄来的生日礼物，'\n    yield '那份意外的惊喜与深深的祝福'\n    yield '让我心中充满了甜蜜的快乐，'\n    yield '笑容如花儿般绽放。'\nfor i, j in enumerate(cosyvoice.inference_zero_shot(text_generator(), '希望你以后能够做的比我还好呦。', prompt_speech_16k, stream=False)):\n    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)"
                }
            ]
        }
    ]
}