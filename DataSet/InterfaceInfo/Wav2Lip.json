{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/Wav2Lip",
    "API_Calls": [
        {
            "Name": "call_Wav2Lip",
            "Description": "call Wav2Lip to load the model and process the video frames.",
            "Code": "from os import listdir, path\nimport numpy as np\nimport scipy, cv2, os, sys, argparse, audio\nimport json, subprocess, random, string\nfrom tqdm import tqdm\nfrom glob import glob\nimport torch, face_detection\nfrom models import Wav2Lip\nimport platform\n\nparser = argparse.ArgumentParser(description='Inference code to lip-sync videos in the wild using Wav2Lip models')\n\nparser.add_argument('--checkpoint_path', type=str, default='wav2lip_gan.pth',\n\t\t\t\t\thelp='Name of saved checkpoint to load weights from')\n\nparser.add_argument('--face', type=str, \n\t\t\t\t\thelp='Filepath of video/image that contains faces to use', required=True)\nparser.add_argument('--audio', type=str, \n\t\t\t\t\thelp='Filepath of video/audio file to use as raw audio source', required=True)\nparser.add_argument('--outfile', type=str, help='Video path to save result. See default for an e.g.', \n\t\t\t\t\t\t\t\tdefault='results/result_voice.mp4')\n\nparser.add_argument('--static', type=bool, \n\t\t\t\t\thelp='If True, then use only first video frame for inference', default=False)\nparser.add_argument('--fps', type=float, help='Can be specified only if input is a static image (default: 25)', \n\t\t\t\t\tdefault=25., required=False)\n\nparser.add_argument('--pads', nargs='+', type=int, default=[0, 10, 0, 0], \n\t\t\t\t\thelp='Padding (top, bottom, left, right). Please adjust to include chin at least')\n\nparser.add_argument('--face_det_batch_size', type=int, \n\t\t\t\t\thelp='Batch size for face detection', default=16)\nparser.add_argument('--wav2lip_batch_size', type=int, help='Batch size for Wav2Lip model(s)', default=128)\n\nparser.add_argument('--resize_factor', default=1, type=int, \n\t\t\thelp='Reduce the resolution by this factor. Sometimes, best results are obtained at 480p or 720p')\n\nparser.add_argument('--crop', nargs='+', type=int, default=[0, -1, 0, -1], \n\t\t\t\t\thelp='Crop video to a smaller region (top, bottom, left, right). Applied after resize_factor and rotate arg. ' \n\t\t\t\t\t'Useful if multiple face present. -1 implies the value will be auto-inferred based on height, width')\n\nparser.add_argument('--box', nargs='+', type=int, default=[-1, -1, -1, -1], \n\t\t\t\t\thelp='Specify a constant bounding box for the face. Use only as a last resort if the face is not detected.'\n\t\t\t\t\t'Also, might work only if the face is not moving around much. Syntax: (top, bottom, left, right).')\n\nparser.add_argument('--rotate', default=False, action='store_true',\n\t\t\t\t\thelp='Sometimes videos taken from a phone can be flipped 90deg. If true, will flip video right by 90deg.'\n\t\t\t\t\t'Use if you get a flipped result, despite feeding a normal looking video')\n\nparser.add_argument('--nosmooth', default=False, action='store_true',\n\t\t\t\t\thelp='Prevent smoothing face detections over a short temporal window')\n\nargs = parser.parse_args()\nargs.img_size = 96\n\nif os.path.isfile(args.face) and args.face.split('.')[1] in ['jpg', 'png', 'jpeg']:\n\targs.static = True\n\ndef get_smoothened_boxes(boxes, T):\n\tfor i in range(len(boxes)):\n\t\tif i + T > len(boxes):\n\t\t\twindow = boxes[len(boxes) - T:]\n\t\telse:\n\t\t\twindow = boxes[i : i + T]\n\t\tboxes[i] = np.mean(window, axis=0)\n\treturn boxes\n\ndef face_detect(images):\n\tdetector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n\t\t\t\t\t\t\t\t\t\t\tflip_input=False, device=device)\n\n\tbatch_size = args.face_det_batch_size\n\t\n\twhile 1:\n\t\tpredictions = []\n\t\ttry:\n\t\t\tfor i in tqdm(range(0, len(images), batch_size)):\n\t\t\t\tpredictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n\t\texcept RuntimeError:\n\t\t\tif batch_size == 1: \n\t\t\t\traise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n\t\t\tbatch_size //= 2\n\t\t\tprint('Recovering from OOM error; New batch size: {}'.format(batch_size))\n\t\t\tcontinue\n\t\tbreak\n\n\tresults = []\n\tpady1, pady2, padx1, padx2 = args.pads\n\tfor rect, image in zip(predictions, images):\n\t\tif rect is None:\n\t\t\tcv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n\t\t\traise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n\n\t\ty1 = max(0, rect[1] - pady1)\n\t\ty2 = min(image.shape[0], rect[3] + pady2)\n\t\tx1 = max(0, rect[0] - padx1)\n\t\tx2 = min(image.shape[1], rect[2] + padx2)\n\t\t\n\t\tresults.append([x1, y1, x2, y2])\n\n\tboxes = np.array(results)\n\tif not args.nosmooth: boxes = get_smoothened_boxes(boxes, T=5)\n\tresults = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n\n\tdel detector\n\treturn results \n\ndef datagen(frames, mels):\n\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n\n\tif args.box[0] == -1:\n\t\tif not args.static:\n\t\t\tface_det_results = face_detect(frames) # BGR2RGB for CNN face detection\n\t\telse:\n\t\t\tface_det_results = face_detect([frames[0]])\n\telse:\n\t\tprint('Using the specified bounding box instead of face detection...')\n\t\ty1, y2, x1, x2 = args.box\n\t\tface_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames]\n\n\tfor i, m in enumerate(mels):\n\t\tidx = 0 if args.static else i%len(frames)\n\t\tframe_to_save = frames[idx].copy()\n\t\tface, coords = face_det_results[idx].copy()\n\n\t\tface = cv2.resize(face, (args.img_size, args.img_size))\n\t\t\t\n\t\timg_batch.append(face)\n\t\tmel_batch.append(m)\n\t\tframe_batch.append(frame_to_save)\n\t\tcoords_batch.append(coords)\n\n\t\tif len(img_batch) >= args.wav2lip_batch_size:\n\t\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n\n\t\t\timg_masked = img_batch.copy()\n\t\t\timg_masked[:, args.img_size//2:] = 0\n\n\t\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n\t\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n\n\t\t\tyield img_batch, mel_batch, frame_batch, coords_batch\n\t\t\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n\n\tif len(img_batch) > 0:\n\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n\n\t\timg_masked = img_batch.copy()\n\t\timg_masked[:, args.img_size//2:] = 0\n\n\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n\n\t\tyield img_batch, mel_batch, frame_batch, coords_batch\n\nmel_step_size = 16\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} for inference.'.format(device))\n\ndef _load(checkpoint_path):\n\tif device == 'cuda':\n\t\tcheckpoint = torch.load(checkpoint_path)\n\telse:\n\t\tcheckpoint = torch.load(checkpoint_path,\n\t\t\t\t\t\t\t\tmap_location=lambda storage, loc: storage)\n\treturn checkpoint\n\ndef load_model(path):\n\tmodel = Wav2Lip()\n\tprint(\"Load checkpoint from: {}\".format(path))\n\tcheckpoint = _load(path)\n\ts = checkpoint[\"state_dict\"]\n\tnew_s = {}\n\tfor k, v in s.items():\n\t\tnew_s[k.replace('module.', '')] = v\n\tmodel.load_state_dict(new_s)\n\n\tmodel = model.to(device)\n\treturn model.eval()\n\ndef main():\n\tif not os.path.isfile(args.face):\n\t\traise ValueError('--face argument must be a valid path to video/image file')\n\n\telif args.face.split('.')[1] in ['jpg', 'png', 'jpeg']:\n\t\tfull_frames = [cv2.imread(args.face)]\n\t\tfps = args.fps\n\n\telse:\n\t\tvideo_stream = cv2.VideoCapture(args.face)\n\t\tfps = video_stream.get(cv2.CAP_PROP_FPS)\n\n\t\tprint('Reading video frames...')\n\n\t\tfull_frames = []\n\t\twhile 1:\n\t\t\tstill_reading, frame = video_stream.read()\n\t\t\tif not still_reading:\n\t\t\t\tvideo_stream.release()\n\t\t\t\tbreak\n\t\t\tif args.resize_factor > 1:\n\t\t\t\tframe = cv2.resize(frame, (frame.shape[1]//args.resize_factor, frame.shape[0]//args.resize_factor))\n\n\t\t\tif args.rotate:\n\t\t\t\tframe = cv2.rotate(frame, cv2.cv2.ROTATE_90_CLOCKWISE)\n\n\t\t\ty1, y2, x1, x2 = args.crop\n\t\t\tif x2 == -1: x2 = frame.shape[1]\n\t\t\tif y2 == -1: y2 = frame.shape[0]\n\n\t\t\tframe = frame[y1:y2, x1:x2]\n\n\t\t\tfull_frames.append(frame)\n\n\tprint (\"Number of frames available for inference: \"+str(len(full_frames)))\n\n\tif not args.audio.endswith('.wav'):\n\t\tprint('Extracting raw audio...')\n\t\tcommand = 'ffmpeg -y -i {} -strict -2 {}'.format(args.audio, 'temp/temp.wav')\n\n\t\tsubprocess.call(command, shell=True)\n\t\targs.audio = 'temp/temp.wav'\n\n\twav = audio.load_wav(args.audio, 16000)\n\tmel = audio.melspectrogram(wav)\n\tprint(mel.shape)\n\n\tif np.isnan(mel.reshape(-1)).sum() > 0:\n\t\traise ValueError('Mel contains nan! Using a TTS voice? Add a small epsilon noise to the wav file and try again')\n\n\tmel_chunks = []\n\tmel_idx_multiplier = 80./fps \n\ti = 0\n\twhile 1:\n\t\tstart_idx = int(i * mel_idx_multiplier)\n\t\tif start_idx + mel_step_size > len(mel[0]):\n\t\t\tmel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n\t\t\tbreak\n\t\tmel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n\t\ti += 1\n\n\tprint(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n\n\tfull_frames = full_frames[:len(mel_chunks)]\n\n\tbatch_size = args.wav2lip_batch_size\n\tgen = datagen(full_frames.copy(), mel_chunks)\n\n\tfor i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, \n\t\t\t\t\t\t\t\t\t\t\ttotal=int(np.ceil(float(len(mel_chunks))/batch_size)))):\n\t\tif i == 0:\n\t\t\tmodel = load_model(args.checkpoint_path)\n\t\t\tprint (\"Model loaded\")\n\n\t\t\tframe_h, frame_w = full_frames[0].shape[:-1]\n\t\t\tout = cv2.VideoWriter('temp/result.avi', \n\t\t\t\t\t\t\t\t\tcv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n\n\t\timg_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n\t\tmel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n\n\t\twith torch.no_grad():\n\t\t\tpred = model(mel_batch, img_batch)\n\n\t\tpred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n\t\t\n\t\tfor p, f, c in zip(pred, frames, coords):\n\t\t\ty1, y2, x1, x2 = c\n\t\t\tp = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n\n\t\t\tf[y1:y2, x1:x2] = p\n\t\t\tout.write(f)\n\n\tout.release()\n\n\tcommand = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(args.audio, 'temp/result.avi', args.outfile)\n\tsubprocess.call(command, shell=platform.system() != 'Windows')\n\nif __name__ == '__main__':\n\tmain()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Wav2Lip/inference.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "Wav2Lip",
            "Description": "Wav2Lip impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Wav2Lip/models/wav2lip.py",
            "Implementation": "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport math\n\nfrom .conv import Conv2dTranspose, Conv2d, nonorm_Conv2d\n\nclass Wav2Lip(nn.Module):\n    def __init__(self):\n        super(Wav2Lip, self).__init__()\n\n        self.face_encoder_blocks = nn.ModuleList([\n            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96\n\n            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),\n\n            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),\n\n            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),\n\n            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),\n\n            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n            \n            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n\n        self.audio_encoder = nn.Sequential(\n            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n\n        self.face_decoder_blocks = nn.ModuleList([\n            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),\n\n            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n\n            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6\n\n            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),\n            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12\n\n            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24\n\n            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), \n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48\n\n            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96\n\n        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\n            nn.Sigmoid()) \n\n    def forward(self, audio_sequences, face_sequences):\n        # audio_sequences = (B, T, 1, 80, 16)\n        B = audio_sequences.size(0)\n\n        input_dim_size = len(face_sequences.size())\n        if input_dim_size > 4:\n            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)\n            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n\n        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1\n\n        feats = []\n        x = face_sequences\n        for f in self.face_encoder_blocks:\n            x = f(x)\n            feats.append(x)\n\n        x = audio_embedding\n        for f in self.face_decoder_blocks:\n            x = f(x)\n            try:\n                x = torch.cat((x, feats[-1]), dim=1)\n            except Exception as e:\n                print(x.size())\n                print(feats[-1].size())\n                raise e\n            \n            feats.pop()\n\n        x = self.output_block(x)\n\n        if input_dim_size > 4:\n            x = torch.split(x, B, dim=0) # [(B, C, H, W)]\n            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)\n\n        else:\n            outputs = x\n            \n        return outputs",
            "Examples": [
                "\n"
            ]
        }
    ]
}