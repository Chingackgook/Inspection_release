{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/EasyOCR",
    "API_Calls": [
        {
            "Name": "Reader_call",
            "Description": "easyocr's CLI program",
            "Code": "import argparse\nimport easyocr\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Process EasyOCR.\")\n    parser.add_argument(\n        \"-l\",\n        \"--lang\",\n        nargs='+',\n        required=True,\n        type=str,\n        help=\"for languages\",\n    )\n    parser.add_argument(\n        \"--gpu\",\n        type=bool,\n        choices=[True, False],\n        default=True,\n        help=\"Using GPU (default: True)\",\n    )\n    parser.add_argument(\n        \"--model_storage_directory\",\n        type=str,\n        default=None,\n        help=\"Directory for model (.pth) file\",\n    )\n    parser.add_argument(\n        \"--user_network_directory\",\n        type=str,\n        default=None,\n        help=\"Directory for custom network files\",\n    )\n    parser.add_argument(\n        \"--recog_network\",\n        type=str,\n        default='standard',\n        help=\"Recognition networks\",\n    )\n    parser.add_argument(\n        \"--download_enabled\",\n        type=bool,\n        choices=[True, False],\n        default=True,\n        help=\"Enable Download\",\n    )\n    parser.add_argument(\n        \"--detector\",\n        type=bool,\n        choices=[True, False],\n        default=True,\n        help=\"Initialize text detector module\",\n    )\n    parser.add_argument(\n        \"--recognizer\",\n        type=bool,\n        choices=[True, False],\n        default=True,\n        help=\"Initialize text recognizer module\",\n    )\n    parser.add_argument(\n        \"--verbose\",\n        type=bool,\n        choices=[True, False],\n        default=True,\n        help=\"Print detail/warning\",\n    )\n    parser.add_argument(\n        \"--quantize\",\n        type=bool,\n        choices=[True, False],\n        default=True,\n        help=\"Use dynamic quantization\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--file\",\n        required=True,\n        type=str,\n        help=\"input file\",\n    )\n    parser.add_argument(\n        \"--decoder\",\n        type=str,\n        choices=[\"greedy\", 'beamsearch', 'wordbeamsearch'],\n        default='greedy',\n        help=\"decoder algorithm\",\n    )\n    parser.add_argument(\n        \"--beamWidth\",\n        type=int,\n        default=5,\n        help=\"size of beam search\",\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        default=1,\n        help=\"batch_size\",\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=int,\n        default=0,\n        help=\"number of processing cpu cores\",\n    )\n    parser.add_argument(\n        \"--allowlist\",\n        type=str,\n        default=None,\n        help=\"Force EasyOCR to recognize only subset of characters\",\n    )\n    parser.add_argument(\n        \"--blocklist\",\n        type=str,\n        default=None,\n        help=\"Block subset of character. This argument will be ignored if allowlist is given.\",\n    )\n    parser.add_argument(\n        \"--detail\",\n        type=int,\n        choices=[0, 1],\n        default=1,\n        help=\"simple output (default: 1)\",\n    )\n    parser.add_argument(\n        \"--rotation_info\",\n        type=list,\n        default=None,\n        help=\"Allow EasyOCR to rotate each text box and return the one with the best confident score. Eligible values are 90, 180 and 270. For example, try [90, 180 ,270] for all possible text orientations.\",\n    )\n    parser.add_argument(\n        \"--paragraph\",\n        type=bool,\n        choices=[True, False],\n        default=False,\n        help=\"Combine result into paragraph\",\n    )\n    parser.add_argument(\n        \"--min_size\",\n        type=int,\n        default=20,\n        help=\"Filter text box smaller than minimum value in pixel\",\n    )\n    parser.add_argument(\n        \"--contrast_ths\",\n        type=float,\n        default=0.1,\n        help=\"Text box with contrast lower than this value will be passed into model 2 times. First is with original image and second with contrast adjusted to 'adjust_contrast' value. The one with more confident level will be returned as a result.\",\n    )\n    parser.add_argument(\n        \"--adjust_contrast\",\n        type=float,\n        default=0.5,\n        help=\"target contrast level for low contrast text box\",\n    )\n    parser.add_argument(\n        \"--text_threshold\",\n        type=float,\n        default=0.7,\n        help=\"Text confidence threshold\",\n    )\n    parser.add_argument(\n        \"--low_text\",\n        type=float,\n        default=0.4,\n        help=\"Text low-bound score\",\n    )\n    parser.add_argument(\n        \"--link_threshold\",\n        type=float,\n        default=0.4,\n        help=\"Link confidence threshold\",\n    )\n    parser.add_argument(\n        \"--canvas_size\",\n        type=int,\n        default=2560,\n        help=\"Maximum image size. Image bigger than this value will be resized down.\",\n    )\n    parser.add_argument(\n        \"--mag_ratio\",\n        type=float,\n        default=1.,\n        help=\"Image magnification ratio\",\n    )\n    parser.add_argument(\n        \"--slope_ths\",\n        type=float,\n        default=0.1,\n        help=\"Maximum slope (delta y/delta x) to considered merging. Low value means tiled boxes will not be merged.\",\n    )\n    parser.add_argument(\n        \"--ycenter_ths\",\n        type=float,\n        default=0.5,\n        help=\"Maximum shift in y direction. Boxes with different level should not be merged.\",\n    )\n    parser.add_argument(\n        \"--height_ths\",\n        type=float,\n        default=0.5,\n        help=\"Maximum different in box height. Boxes with very different text size should not be merged. \",\n    )\n    parser.add_argument(\n        \"--width_ths\",\n        type=float,\n        default=0.5,\n        help=\"Maximum horizontal distance to merge boxes.\",\n    )\n    parser.add_argument(\n        \"--y_ths\",\n        type=float,\n        default=0.5,\n        help=\"Maximum vertical distance to merge boxes (when paragraph = True).\",\n    )\n    parser.add_argument(\n        \"--x_ths\",\n        type=float,\n        default=1.0,\n        help=\"Maximum horizontal distance to merge boxes (when paragraph = True).\",\n    )\n    parser.add_argument(\n        \"--add_margin\",\n        type=float,\n        default=0.1,\n        help=\"Extend bounding boxes in all direction by certain value. This is important for language with complex script (E.g. Thai).\",\n    )\n    parser.add_argument(\n        \"--output_format\",\n        type=str,\n        choices=[\"standard\", 'dict', 'json'],\n        default='standard',\n        help=\"output format.\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    reader = easyocr.Reader(lang_list=args.lang,\\\n                            gpu=args.gpu,\\\n                            model_storage_directory=args.model_storage_directory,\\\n                            user_network_directory=args.user_network_directory,\\\n                            recog_network=args.recog_network,\\\n                            download_enabled=args.download_enabled,\\\n                            detector=args.detector,\\\n                            recognizer=args.recognizer,\\\n                            verbose=args.verbose,\\\n                            quantize=args.quantize)\n    for line in reader.readtext(args.file,\\\n                                decoder=args.decoder,\\\n                                beamWidth=args.beamWidth,\\\n                                batch_size=args.batch_size,\\\n                                workers=args.workers,\\\n                                allowlist=args.allowlist,\\\n                                blocklist=args.blocklist,\\\n                                detail=args.detail,\\\n                                rotation_info=args.rotation_info,\\\n                                paragraph=args.paragraph,\\\n                                min_size=args.min_size,\\\n                                contrast_ths=args.contrast_ths,\\\n                                adjust_contrast=args.adjust_contrast,\\\n                                text_threshold=args.text_threshold,\\\n                                low_text=args.low_text,\\\n                                link_threshold=args.link_threshold,\\\n                                canvas_size=args.canvas_size,\\\n                                mag_ratio=args.mag_ratio,\\\n                                slope_ths=args.slope_ths,\\\n                                ycenter_ths=args.ycenter_ths,\\\n                                height_ths=args.height_ths,\\\n                                width_ths=args.width_ths,\\\n                                y_ths=args.y_ths,\\\n                                x_ths=args.x_ths,\\\n                                add_margin=args.add_margin,\\\n                                output_format=args.output_format):\n        print(line)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/EasyOCR/easyocr/cli.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class Reader(object)",
            "Description": "EasyOCR是一个OCR（光学字符识别）库，支持多种语言的文本识别。它使用深度学习模型来检测和识别图像中的文本。该库提供了简单易用的API，可以处理各种图像格式，并支持GPU加速。",
            "Implementation": "# -*- coding: utf-8 -*-\n\nfrom easyocr.recognition import get_recognizer, get_text\nfrom easyocr.utils import group_text_box, get_image_list, calculate_md5, get_paragraph,\\\n                   download_and_unzip, printProgressBar, diff, reformat_input,\\\n                   make_rotated_img_list, set_result_with_confidence,\\\n                   reformat_input_batched, merge_to_free\nfrom easyocr.config import *\nfrom bidi import get_display\nimport numpy as np\nimport cv2\nimport torch\nimport os\nimport sys\nfrom PIL import Image\nfrom logging import getLogger\nimport yaml\nimport json\n\nif sys.version_info[0] == 2:\n    from io import open\n    from six.moves.urllib.request import urlretrieve\n    from pathlib2 import Path\nelse:\n    from urllib.request import urlretrieve\n    from pathlib import Path\n\nLOGGER = getLogger(__name__)\n\nclass Reader(object):\n\n    def __init__(self, lang_list, gpu=True, model_storage_directory=None,\n                 user_network_directory=None, detect_network=\"craft\", \n                 recog_network='standard', download_enabled=True, \n                 detector=True, recognizer=True, verbose=True, \n                 quantize=True, cudnn_benchmark=False):\n        \"\"\"Create an EasyOCR Reader\n\n        Parameters:\n            lang_list (list): Language codes (ISO 639) for languages to be recognized during analysis.\n\n            gpu (bool): Enable GPU support (default)\n\n            model_storage_directory (string): Path to directory for model data. If not specified,\n            models will be read from a directory as defined by the environment variable\n            EASYOCR_MODULE_PATH (preferred), MODULE_PATH (if defined), or ~/.EasyOCR/.\n\n            user_network_directory (string): Path to directory for custom network architecture.\n            If not specified, it is as defined by the environment variable\n            EASYOCR_MODULE_PATH (preferred), MODULE_PATH (if defined), or ~/.EasyOCR/.\n\n            download_enabled (bool): Enabled downloading of model data via HTTP (default).\n        \"\"\"\n        self.verbose = verbose\n        self.download_enabled = download_enabled\n\n        self.model_storage_directory = MODULE_PATH + '/model'\n        if model_storage_directory:\n            self.model_storage_directory = model_storage_directory\n        Path(self.model_storage_directory).mkdir(parents=True, exist_ok=True)\n\n        self.user_network_directory = MODULE_PATH + '/user_network'\n        if user_network_directory:\n            self.user_network_directory = user_network_directory\n        Path(self.user_network_directory).mkdir(parents=True, exist_ok=True)\n        sys.path.append(self.user_network_directory)\n\n        if gpu is False:\n            self.device = 'cpu'\n            if verbose:\n                LOGGER.warning('Using CPU. Note: This module is much faster with a GPU.')\n        elif gpu is True:\n            if torch.cuda.is_available():\n                self.device = 'cuda'\n            elif torch.backends.mps.is_available():\n                self.device = 'mps'\n            else:\n                self.device = 'cpu'\n                if verbose:\n                    LOGGER.warning('Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.')\n        else:\n            self.device = gpu\n\n        self.detection_models = detection_models\n        self.recognition_models = recognition_models\n\n        # check and download detection model\n        self.support_detection_network = ['craft', 'dbnet18']\n        self.quantize=quantize, \n        self.cudnn_benchmark=cudnn_benchmark\n        if detector:\n            detector_path = self.getDetectorPath(detect_network)\n        \n        # recognition model\n        separator_list = {}\n\n        if recog_network in ['standard'] + [model for model in recognition_models['gen1']] + [model for model in recognition_models['gen2']]:\n            if recog_network in [model for model in recognition_models['gen1']]:\n                model = recognition_models['gen1'][recog_network]\n                recog_network = 'generation1'\n                self.model_lang = model['model_script']\n            elif recog_network in [model for model in recognition_models['gen2']]:\n                model = recognition_models['gen2'][recog_network]\n                recog_network = 'generation2'\n                self.model_lang = model['model_script']\n            else: # auto-detect\n                unknown_lang = set(lang_list) - set(all_lang_list)\n                if unknown_lang != set():\n                    raise ValueError(unknown_lang, 'is not supported')\n                # choose recognition model\n                if lang_list == ['en']:\n                    self.setModelLanguage('english', lang_list, ['en'], '[\"en\"]')\n                    model = recognition_models['gen2']['english_g2']\n                    recog_network = 'generation2'\n                elif 'th' in lang_list:\n                    self.setModelLanguage('thai', lang_list, ['th','en'], '[\"th\",\"en\"]')\n                    model = recognition_models['gen1']['thai_g1']\n                    recog_network = 'generation1'\n                elif 'ch_tra' in lang_list:\n                    self.setModelLanguage('chinese_tra', lang_list, ['ch_tra','en'], '[\"ch_tra\",\"en\"]')\n                    model = recognition_models['gen1']['zh_tra_g1']\n                    recog_network = 'generation1'\n                elif 'ch_sim' in lang_list:\n                    self.setModelLanguage('chinese_sim', lang_list, ['ch_sim','en'], '[\"ch_sim\",\"en\"]')\n                    model = recognition_models['gen2']['zh_sim_g2']\n                    recog_network = 'generation2'\n                elif 'ja' in lang_list:\n                    self.setModelLanguage('japanese', lang_list, ['ja','en'], '[\"ja\",\"en\"]')\n                    model = recognition_models['gen2']['japanese_g2']\n                    recog_network = 'generation2'\n                elif 'ko' in lang_list:\n                    self.setModelLanguage('korean', lang_list, ['ko','en'], '[\"ko\",\"en\"]')\n                    model = recognition_models['gen2']['korean_g2']\n                    recog_network = 'generation2'\n                elif 'ta' in lang_list:\n                    self.setModelLanguage('tamil', lang_list, ['ta','en'], '[\"ta\",\"en\"]')\n                    model = recognition_models['gen1']['tamil_g1']\n                    recog_network = 'generation1'\n                elif 'te' in lang_list:\n                    self.setModelLanguage('telugu', lang_list, ['te','en'], '[\"te\",\"en\"]')\n                    model = recognition_models['gen2']['telugu_g2']\n                    recog_network = 'generation2'\n                elif 'kn' in lang_list:\n                    self.setModelLanguage('kannada', lang_list, ['kn','en'], '[\"kn\",\"en\"]')\n                    model = recognition_models['gen2']['kannada_g2']\n                    recog_network = 'generation2'\n                elif set(lang_list) & set(bengali_lang_list):\n                    self.setModelLanguage('bengali', lang_list, bengali_lang_list+['en'], '[\"bn\",\"as\",\"en\"]')\n                    model = recognition_models['gen1']['bengali_g1']\n                    recog_network = 'generation1'\n                elif set(lang_list) & set(arabic_lang_list):\n                    self.setModelLanguage('arabic', lang_list, arabic_lang_list+['en'], '[\"ar\",\"fa\",\"ur\",\"ug\",\"en\"]')\n                    model = recognition_models['gen1']['arabic_g1']\n                    recog_network = 'generation1'\n                elif set(lang_list) & set(devanagari_lang_list):\n                    self.setModelLanguage('devanagari', lang_list, devanagari_lang_list+['en'], '[\"hi\",\"mr\",\"ne\",\"en\"]')\n                    model = recognition_models['gen1']['devanagari_g1']\n                    recog_network = 'generation1'\n                elif set(lang_list) & set(cyrillic_lang_list):\n                    self.setModelLanguage('cyrillic', lang_list, cyrillic_lang_list+['en'],\n                                          '[\"ru\",\"rs_cyrillic\",\"be\",\"bg\",\"uk\",\"mn\",\"en\"]')\n                    model = recognition_models['gen2']['cyrillic_g2']\n                    recog_network = 'generation2'\n                else:\n                    self.model_lang = 'latin'\n                    model = recognition_models['gen2']['latin_g2']\n                    recog_network = 'generation2'\n            self.character = model['characters']\n\n            model_path = os.path.join(self.model_storage_directory, model['filename'])\n            # check recognition model file\n            if recognizer:\n                if os.path.isfile(model_path) == False:\n                    if not self.download_enabled:\n                        raise FileNotFoundError(\"Missing %s and downloads disabled\" % model_path)\n                    LOGGER.warning('Downloading recognition model, please wait. '\n                                   'This may take several minutes depending upon your network connection.')\n                    download_and_unzip(model['url'], model['filename'], self.model_storage_directory, verbose)\n                    assert calculate_md5(model_path) == model['md5sum'], corrupt_msg\n                    LOGGER.info('Download complete.')\n                elif calculate_md5(model_path) != model['md5sum']:\n                    if not self.download_enabled:\n                        raise FileNotFoundError(\"MD5 mismatch for %s and downloads disabled\" % model_path)\n                    LOGGER.warning(corrupt_msg)\n                    os.remove(model_path)\n                    LOGGER.warning('Re-downloading the recognition model, please wait. '\n                                   'This may take several minutes depending upon your network connection.')\n                    download_and_unzip(model['url'], model['filename'], self.model_storage_directory, verbose)\n                    assert calculate_md5(model_path) == model['md5sum'], corrupt_msg\n                    LOGGER.info('Download complete')\n            self.setLanguageList(lang_list, model)\n\n        else: # user-defined model\n            with open(os.path.join(self.user_network_directory, recog_network+ '.yaml'), encoding='utf8') as file:\n                recog_config = yaml.load(file, Loader=yaml.FullLoader)\n            \n            global imgH # if custom model, save this variable. (from *.yaml)\n            if recog_config['imgH']:\n                imgH = recog_config['imgH']\n                \n            available_lang = recog_config['lang_list']\n            self.setModelLanguage(recog_network, lang_list, available_lang, str(available_lang))\n            #char_file = os.path.join(self.user_network_directory, recog_network+ '.txt')\n            self.character = recog_config['character_list']\n            model_file = recog_network+ '.pth'\n            model_path = os.path.join(self.model_storage_directory, model_file)\n            self.setLanguageList(lang_list, recog_config)\n\n        dict_list = {}\n        for lang in lang_list:\n            dict_list[lang] = os.path.join(BASE_PATH, 'dict', lang + \".txt\")\n\n        if detector:\n            self.detector = self.initDetector(detector_path)\n            \n        if recognizer:\n            if recog_network == 'generation1':\n                network_params = {\n                    'input_channel': 1,\n                    'output_channel': 512,\n                    'hidden_size': 512\n                    }\n            elif recog_network == 'generation2':\n                network_params = {\n                    'input_channel': 1,\n                    'output_channel': 256,\n                    'hidden_size': 256\n                    }\n            else:\n                network_params = recog_config['network_params']\n            self.recognizer, self.converter = get_recognizer(recog_network, network_params,\\\n                                                         self.character, separator_list,\\\n                                                         dict_list, model_path, device = self.device, quantize=quantize)\n\n    def getDetectorPath(self, detect_network):\n        if detect_network in self.support_detection_network:\n            self.detect_network = detect_network\n            if self.detect_network == 'craft':\n                from easyocr.detection import get_detector, get_textbox\n            elif self.detect_network in ['dbnet18']:\n                from easyocr.detection_db import get_detector, get_textbox\n            else:\n                raise RuntimeError(\"Unsupport detector network. Support networks are craft and dbnet18.\")\n            self.get_textbox = get_textbox\n            self.get_detector = get_detector\n            corrupt_msg = 'MD5 hash mismatch, possible file corruption'\n            detector_path = os.path.join(self.model_storage_directory, self.detection_models[self.detect_network]['filename'])\n            if os.path.isfile(detector_path) == False:\n                if not self.download_enabled:\n                    raise FileNotFoundError(\"Missing %s and downloads disabled\" % detector_path)\n                LOGGER.warning('Downloading detection model, please wait. '\n                               'This may take several minutes depending upon your network connection.')\n                download_and_unzip(self.detection_models[self.detect_network]['url'], self.detection_models[self.detect_network]['filename'], self.model_storage_directory, self.verbose)\n                assert calculate_md5(detector_path) == self.detection_models[self.detect_network]['md5sum'], corrupt_msg\n                LOGGER.info('Download complete')\n            elif calculate_md5(detector_path) != self.detection_models[self.detect_network]['md5sum']:\n                if not self.download_enabled:\n                    raise FileNotFoundError(\"MD5 mismatch for %s and downloads disabled\" % detector_path)\n                LOGGER.warning(corrupt_msg)\n                os.remove(detector_path)\n                LOGGER.warning('Re-downloading the detection model, please wait. '\n                               'This may take several minutes depending upon your network connection.')\n                download_and_unzip(self.detection_models[self.detect_network]['url'], self.detection_models[self.detect_network]['filename'], self.model_storage_directory, self.verbose)\n                assert calculate_md5(detector_path) == self.detection_models[self.detect_network]['md5sum'], corrupt_msg\n        else:\n            raise RuntimeError(\"Unsupport detector network. Support networks are {}.\".format(', '.join(self.support_detection_network)))\n        \n        return detector_path\n\n    def initDetector(self, detector_path):\n        return self.get_detector(detector_path, \n                                 device = self.device, \n                                 quantize = self.quantize, \n                                 cudnn_benchmark = self.cudnn_benchmark\n                                 )\n    \n    def setDetector(self, detect_network):\n        detector_path = self.getDetectorPath(detect_network)\n        self.detector = self.initDetector(detector_path)\n    \n    def setModelLanguage(self, language, lang_list, list_lang, list_lang_string):\n        self.model_lang = language\n        if set(lang_list) - set(list_lang) != set():\n            if language == 'ch_tra' or language == 'ch_sim':\n                language = 'chinese'\n            raise ValueError(language.capitalize() + ' is only compatible with English, try lang_list=' + list_lang_string)\n\n    def getChar(self, fileName):\n        char_file = os.path.join(BASE_PATH, 'character', fileName)\n        with open(char_file, \"r\", encoding=\"utf-8-sig\") as input_file:\n            list = input_file.read().splitlines()\n            char = ''.join(list)\n        return char\n\n    def setLanguageList(self, lang_list, model):\n        self.lang_char = []\n        for lang in lang_list:\n            char_file = os.path.join(BASE_PATH, 'character', lang + \"_char.txt\")\n            with open(char_file, \"r\", encoding = \"utf-8-sig\") as input_file:\n                char_list =  input_file.read().splitlines()\n            self.lang_char += char_list\n        if model.get('symbols'):\n            symbol = model['symbols']\n        elif model.get('character_list'):\n            symbol = model['character_list']\n        else:\n            symbol = '0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ '\n        self.lang_char = set(self.lang_char).union(set(symbol))\n        self.lang_char = ''.join(self.lang_char)\n\n    def detect(self, img, min_size = 20, text_threshold = 0.7, low_text = 0.4,\\\n               link_threshold = 0.4,canvas_size = 2560, mag_ratio = 1.,\\\n               slope_ths = 0.1, ycenter_ths = 0.5, height_ths = 0.5,\\\n               width_ths = 0.5, add_margin = 0.1, reformat=True, optimal_num_chars=None,\n               threshold = 0.2, bbox_min_score = 0.2, bbox_min_size = 3, max_candidates = 0,\n               ):\n\n        if reformat:\n            img, img_cv_grey = reformat_input(img)\n\n        text_box_list = self.get_textbox(self.detector, \n                                    img, \n                                    canvas_size = canvas_size, \n                                    mag_ratio = mag_ratio,\n                                    text_threshold = text_threshold, \n                                    link_threshold = link_threshold, \n                                    low_text = low_text,\n                                    poly = False, \n                                    device = self.device, \n                                    optimal_num_chars = optimal_num_chars,\n                                    threshold = threshold, \n                                    bbox_min_score = bbox_min_score, \n                                    bbox_min_size = bbox_min_size, \n                                    max_candidates = max_candidates,\n                                    )\n\n        horizontal_list_agg, free_list_agg = [], []\n        for text_box in text_box_list:\n            horizontal_list, free_list = group_text_box(text_box, slope_ths,\n                                                        ycenter_ths, height_ths,\n                                                        width_ths, add_margin,\n                                                        (optimal_num_chars is None))\n            if min_size:\n                horizontal_list = [i for i in horizontal_list if max(\n                    i[1] - i[0], i[3] - i[2]) > min_size]\n                free_list = [i for i in free_list if max(\n                    diff([c[0] for c in i]), diff([c[1] for c in i])) > min_size]\n            horizontal_list_agg.append(horizontal_list)\n            free_list_agg.append(free_list)\n\n        return horizontal_list_agg, free_list_agg\n\n    def recognize(self, img_cv_grey, horizontal_list=None, free_list=None,\\\n                  decoder = 'greedy', beamWidth= 5, batch_size = 1,\\\n                  workers = 0, allowlist = None, blocklist = None, detail = 1,\\\n                  rotation_info = None,paragraph = False,\\\n                  contrast_ths = 0.1,adjust_contrast = 0.5, filter_ths = 0.003,\\\n                  y_ths = 0.5, x_ths = 1.0, reformat=True, output_format='standard'):\n\n        if reformat:\n            img, img_cv_grey = reformat_input(img_cv_grey)\n\n        if allowlist:\n            ignore_char = ''.join(set(self.character)-set(allowlist))\n        elif blocklist:\n            ignore_char = ''.join(set(blocklist))\n        else:\n            ignore_char = ''.join(set(self.character)-set(self.lang_char))\n\n        if self.model_lang in ['chinese_tra','chinese_sim']: decoder = 'greedy'\n\n        if (horizontal_list==None) and (free_list==None):\n            y_max, x_max = img_cv_grey.shape\n            horizontal_list = [[0, x_max, 0, y_max]]\n            free_list = []\n\n        # without gpu/parallelization, it is faster to process image one by one\n        if ((batch_size == 1) or (self.device == 'cpu')) and not rotation_info:\n            result = []\n            for bbox in horizontal_list:\n                h_list = [bbox]\n                f_list = []\n                image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height = imgH)\n                result0 = get_text(self.character, imgH, int(max_width), self.recognizer, self.converter, image_list,\\\n                              ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths,\\\n                              workers, self.device)\n                result += result0\n            for bbox in free_list:\n                h_list = []\n                f_list = [bbox]\n                image_list, max_width = get_image_list(h_list, f_list, img_cv_grey, model_height = imgH)\n                result0 = get_text(self.character, imgH, int(max_width), self.recognizer, self.converter, image_list,\\\n                              ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths,\\\n                              workers, self.device)\n                result += result0\n        # default mode will try to process multiple boxes at the same time\n        else:\n            image_list, max_width = get_image_list(horizontal_list, free_list, img_cv_grey, model_height = imgH)\n            image_len = len(image_list)\n            if rotation_info and image_list:\n                image_list = make_rotated_img_list(rotation_info, image_list)\n                max_width = max(max_width, imgH)\n\n            result = get_text(self.character, imgH, int(max_width), self.recognizer, self.converter, image_list,\\\n                          ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths,\\\n                          workers, self.device)\n\n            if rotation_info and (horizontal_list+free_list):\n                # Reshape result to be a list of lists, each row being for \n                # one of the rotations (first row being no rotation)\n                result = set_result_with_confidence(\n                    [result[image_len*i:image_len*(i+1)] for i in range(len(rotation_info) + 1)])\n\n        if self.model_lang == 'arabic':\n            direction_mode = 'rtl'\n            result = [list(item) for item in result]\n            for item in result:\n                item[1] = get_display(item[1])\n        else:\n            direction_mode = 'ltr'\n\n        if paragraph:\n            result = get_paragraph(result, x_ths=x_ths, y_ths=y_ths, mode = direction_mode)\n\n        if detail == 0:\n            return [item[1] for item in result]\n        elif output_format == 'dict':\n            if paragraph:\n                return [ {'boxes':item[0],'text':item[1]} for item in result]    \n            return [ {'boxes':item[0],'text':item[1],'confident':item[2]} for item in result]\n        elif output_format == 'json':\n            if paragraph:\n                return [json.dumps({'boxes':[list(map(int, lst)) for lst in item[0]],'text':item[1]}, ensure_ascii=False) for item in result]\n            return [json.dumps({'boxes':[list(map(int, lst)) for lst in item[0]],'text':item[1],'confident':item[2]}, ensure_ascii=False) for item in result]\n        elif output_format == 'free_merge':\n            return merge_to_free(result, free_list)\n        else:\n            return result\n\n    def readtext(self, image, decoder = 'greedy', beamWidth= 5, batch_size = 1,\\\n                 workers = 0, allowlist = None, blocklist = None, detail = 1,\\\n                 rotation_info = None, paragraph = False, min_size = 20,\\\n                 contrast_ths = 0.1,adjust_contrast = 0.5, filter_ths = 0.003,\\\n                 text_threshold = 0.7, low_text = 0.4, link_threshold = 0.4,\\\n                 canvas_size = 2560, mag_ratio = 1.,\\\n                 slope_ths = 0.1, ycenter_ths = 0.5, height_ths = 0.5,\\\n                 width_ths = 0.5, y_ths = 0.5, x_ths = 1.0, add_margin = 0.1, \n                 threshold = 0.2, bbox_min_score = 0.2, bbox_min_size = 3, max_candidates = 0,\n                 output_format='standard'):\n        '''\n        Parameters:\n        image: file path or numpy-array or a byte stream object\n        '''\n        img, img_cv_grey = reformat_input(image)\n\n        horizontal_list, free_list = self.detect(img, \n                                                 min_size = min_size, text_threshold = text_threshold,\\\n                                                 low_text = low_text, link_threshold = link_threshold,\\\n                                                 canvas_size = canvas_size, mag_ratio = mag_ratio,\\\n                                                 slope_ths = slope_ths, ycenter_ths = ycenter_ths,\\\n                                                 height_ths = height_ths, width_ths= width_ths,\\\n                                                 add_margin = add_margin, reformat = False,\\\n                                                 threshold = threshold, bbox_min_score = bbox_min_score,\\\n                                                 bbox_min_size = bbox_min_size, max_candidates = max_candidates\n                                                 )\n        # get the 1st result from hor & free list as self.detect returns a list of depth 3\n        horizontal_list, free_list = horizontal_list[0], free_list[0]\n        result = self.recognize(img_cv_grey, horizontal_list, free_list,\\\n                                decoder, beamWidth, batch_size,\\\n                                workers, allowlist, blocklist, detail, rotation_info,\\\n                                paragraph, contrast_ths, adjust_contrast,\\\n                                filter_ths, y_ths, x_ths, False, output_format)\n\n        return result\n    \n    def readtextlang(self, image, decoder = 'greedy', beamWidth= 5, batch_size = 1,\\\n                 workers = 0, allowlist = None, blocklist = None, detail = 1,\\\n                 rotation_info = None, paragraph = False, min_size = 20,\\\n                 contrast_ths = 0.1,adjust_contrast = 0.5, filter_ths = 0.003,\\\n                 text_threshold = 0.7, low_text = 0.4, link_threshold = 0.4,\\\n                 canvas_size = 2560, mag_ratio = 1.,\\\n                 slope_ths = 0.1, ycenter_ths = 0.5, height_ths = 0.5,\\\n                 width_ths = 0.5, y_ths = 0.5, x_ths = 1.0, add_margin = 0.1, \n                 threshold = 0.2, bbox_min_score = 0.2, bbox_min_size = 3, max_candidates = 0,\n                 output_format='standard'):\n        '''\n        Parameters:\n        image: file path or numpy-array or a byte stream object\n        '''\n        img, img_cv_grey = reformat_input(image)\n\n        horizontal_list, free_list = self.detect(img, \n                                                 min_size = min_size, text_threshold = text_threshold,\\\n                                                 low_text = low_text, link_threshold = link_threshold,\\\n                                                 canvas_size = canvas_size, mag_ratio = mag_ratio,\\\n                                                 slope_ths = slope_ths, ycenter_ths = ycenter_ths,\\\n                                                 height_ths = height_ths, width_ths= width_ths,\\\n                                                 add_margin = add_margin, reformat = False,\\\n                                                 threshold = threshold, bbox_min_score = bbox_min_score,\\\n                                                 bbox_min_size = bbox_min_size, max_candidates = max_candidates\n                                                 )\n        # get the 1st result from hor & free list as self.detect returns a list of depth 3\n        horizontal_list, free_list = horizontal_list[0], free_list[0]\n        result = self.recognize(img_cv_grey, horizontal_list, free_list,\\\n                                decoder, beamWidth, batch_size,\\\n                                workers, allowlist, blocklist, detail, rotation_info,\\\n                                paragraph, contrast_ths, adjust_contrast,\\\n                                filter_ths, y_ths, x_ths, False, output_format)\n       \n        char = []\n        directory = 'characters/'\n        for i in range(len(result)):\n            char.append(result[i][1])\n        \n        def search(arr,x):\n            g = False\n            for i in range(len(arr)):\n                if arr[i]==x:\n                    g = True\n                    return 1\n            if g == False:\n                return -1\n        def tupleadd(i):\n            a = result[i]\n            b = a + (filename[0:2],)\n            return b\n        \n        for filename in os.listdir(directory):\n            if filename.endswith(\".txt\"):\n                with open ('characters/'+ filename,'rt',encoding=\"utf8\") as myfile:  \n                    chartrs = str(myfile.read().splitlines()).replace('\\n','') \n                    for i in range(len(char)):\n                        res = search(chartrs,char[i])\n                        if res != -1:\n                            if filename[0:2]==\"en\" or filename[0:2]==\"ch\":\n                                print(tupleadd(i))\n\n    def readtext_batched(self, image, n_width=None, n_height=None,\\\n                         decoder = 'greedy', beamWidth= 5, batch_size = 1,\\\n                         workers = 0, allowlist = None, blocklist = None, detail = 1,\\\n                         rotation_info = None, paragraph = False, min_size = 20,\\\n                         contrast_ths = 0.1,adjust_contrast = 0.5, filter_ths = 0.003,\\\n                         text_threshold = 0.7, low_text = 0.4, link_threshold = 0.4,\\\n                         canvas_size = 2560, mag_ratio = 1.,\\\n                         slope_ths = 0.1, ycenter_ths = 0.5, height_ths = 0.5,\\\n                         width_ths = 0.5, y_ths = 0.5, x_ths = 1.0, add_margin = 0.1, \n                         threshold = 0.2, bbox_min_score = 0.2, bbox_min_size = 3, max_candidates = 0,\n                         output_format='standard'):\n        '''\n        Parameters:\n        image: file path or numpy-array or a byte stream object\n        When sending a list of images, they all must of the same size,\n        the following parameters will automatically resize if they are not None\n        n_width: int, new width\n        n_height: int, new height\n        '''\n        img, img_cv_grey = reformat_input_batched(image, n_width, n_height)\n\n        horizontal_list_agg, free_list_agg = self.detect(img, \n                                                 min_size = min_size, text_threshold = text_threshold,\\\n                                                 low_text = low_text, link_threshold = link_threshold,\\\n                                                 canvas_size = canvas_size, mag_ratio = mag_ratio,\\\n                                                 slope_ths = slope_ths, ycenter_ths = ycenter_ths,\\\n                                                 height_ths = height_ths, width_ths= width_ths,\\\n                                                 add_margin = add_margin, reformat = False,\\\n                                                 threshold = threshold, bbox_min_score = bbox_min_score,\\\n                                                 bbox_min_size = bbox_min_size, max_candidates = max_candidates\n                                                 )\n        result_agg = []\n        # put img_cv_grey in a list if its a single img\n        img_cv_grey = [img_cv_grey] if len(img_cv_grey.shape) == 2 else img_cv_grey\n        for grey_img, horizontal_list, free_list in zip(img_cv_grey, horizontal_list_agg, free_list_agg):\n            result_agg.append(self.recognize(grey_img, horizontal_list, free_list,\\\n                                            decoder, beamWidth, batch_size,\\\n                                            workers, allowlist, blocklist, detail, rotation_info,\\\n                                            paragraph, contrast_ths, adjust_contrast,\\\n                                            filter_ths, y_ths, x_ths, False, output_format))\n\n        return result_agg\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/EasyOCR/easyocr/easyocr.py",
            "Examples": [
                {
                    "Description": "简单的OCR识别",
                    "Code": "import easyocr\nreader = easyocr.Reader(['ch_sim','en']) # this needs to run only once to load the model into memory\nresult = reader.readtext('/mnt/autor_name/haoTingDeWenJianJia/EasyOCR/examples/chinese.jpg')\nprint(result)"
                }
            ]
        }
    ]
}