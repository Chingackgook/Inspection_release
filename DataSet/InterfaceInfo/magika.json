{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/magika/python/src",
    "API_Calls": [
        {
            "Name": "call_Magika",
            "Description": "call Magika to analyze the file type",
            "Code": "#!/usr/bin/env python3\n# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport importlib.metadata\nimport json\nimport logging\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple\n\nimport click\n\nfrom magika import Magika, MagikaError, PredictionMode, colors\nfrom magika.logger import get_logger\nfrom magika.types import ContentTypeLabel, MagikaResult\nfrom magika.types.overwrite_reason import OverwriteReason\n\nVERSION = importlib.metadata.version(\"magika\")\n\nCONTACT_EMAIL = \"magika-dev@google.com\"\n\nCONTEXT_SETTINGS = dict(help_option_names=[\"-h\", \"--help\"])\n\nHELP_EPILOG = f\"\"\"\nMagika version: \"{VERSION}\"\\f\nDefault model: \"{Magika._get_default_model_name()}\"\n\nSend any feedback to {CONTACT_EMAIL} or via GitHub issues.\n\"\"\"\n\n\n@click.command(\n    context_settings=CONTEXT_SETTINGS,\n    epilog=HELP_EPILOG,\n)\n@click.argument(\n    \"file\",\n    type=click.Path(exists=False, readable=False, path_type=Path),\n    required=False,\n    nargs=-1,\n)\n@click.option(\n    \"-r\",\n    \"--recursive\",\n    is_flag=True,\n    help='When passing this option, magika scans every file within directories, instead of outputting \"directory\"',\n)\n@click.option(\"--json\", \"json_output\", is_flag=True, help=\"Output in JSON format.\")\n@click.option(\"--jsonl\", \"jsonl_output\", is_flag=True, help=\"Output in JSONL format.\")\n@click.option(\n    \"-i\",\n    \"--mime-type\",\n    \"mime_output\",\n    is_flag=True,\n    help=\"Output the MIME type instead of a verbose content type description.\",\n)\n@click.option(\n    \"-l\",\n    \"--label\",\n    \"label_output\",\n    is_flag=True,\n    help=\"Output a simple label instead of a verbose content type description. Use --list-output-content-types for the list of supported output.\",\n)\n@click.option(\n    \"-c\",\n    \"--compatibility-mode\",\n    \"magic_compatibility_mode\",\n    is_flag=True,\n    help=\"Compatibility mode: output is as close as possible to `file` and colors are disabled.\",\n)\n@click.option(\n    \"-s\",\n    \"--output-score\",\n    is_flag=True,\n    help=\"Output the prediction's score in addition to the content type.\",\n)\n@click.option(\n    \"-m\",\n    \"--prediction-mode\",\n    \"prediction_mode_str\",\n    type=click.Choice(PredictionMode.get_valid_prediction_modes(), case_sensitive=True),\n    default=PredictionMode.HIGH_CONFIDENCE,\n)\n@click.option(\n    \"--batch-size\", default=32, help=\"How many files to process in one batch.\"\n)\n@click.option(\n    \"--no-dereference\",\n    is_flag=True,\n    help=\"This option causes symlinks not to be followed. By default, symlinks are dereferenced.\",\n)\n@click.option(\n    \"--colors/--no-colors\",\n    \"with_colors\",\n    is_flag=True,\n    default=True,\n    help=\"Enable/disable use of colors.\",\n)\n@click.option(\"-v\", \"--verbose\", is_flag=True, help=\"Enable more verbose output.\")\n@click.option(\"-vv\", \"--debug\", is_flag=True, help=\"Enable debug logging.\")\n@click.option(\n    \"--version\", \"output_version\", is_flag=True, help=\"Print the version and exit.\"\n)\n@click.option(\n    \"--model-dir\",\n    type=click.Path(\n        exists=True, file_okay=False, dir_okay=True, resolve_path=True, path_type=Path\n    ),\n    help=\"Use a custom model.\",\n)\ndef main(\n    file: List[Path],\n    recursive: bool,\n    json_output: bool,\n    jsonl_output: bool,\n    mime_output: bool,\n    label_output: bool,\n    magic_compatibility_mode: bool,\n    output_score: bool,\n    prediction_mode_str: str,\n    batch_size: int,\n    no_dereference: bool,\n    with_colors: bool,\n    verbose: bool,\n    debug: bool,\n    output_version: bool,\n    model_dir: Optional[Path],\n) -> None:\n    \"\"\"\n    Magika - Determine type of FILEs with deep-learning.\n    \"\"\"\n\n    # click uses the name of the variable to determine how it will show up in\n    # the --help. Since we don't like to see \"file_paths\" in the help, we name\n    # the argument \"file\" (which is ugly) and we re-assign it as soon as we can.\n    files_paths = file\n\n    if magic_compatibility_mode:\n        # In compatibility mode we disable colors.\n        with_colors = False\n\n    _l = get_logger(use_colors=with_colors)\n\n    if verbose:\n        _l.setLevel(logging.INFO)\n    if debug:\n        _l.setLevel(logging.DEBUG)\n\n    if output_version:\n        _l.raw_print_to_stdout(\"Magika python client\")\n        _l.raw_print_to_stdout(f\"Magika version: {VERSION}\")\n        _l.raw_print_to_stdout(f\"Default model: {Magika._get_default_model_name()}\")\n        sys.exit(0)\n\n    if len(files_paths) == 0:\n        _l.error(\"You need to pass at least one path, or - to read from stdin.\")\n        sys.exit(1)\n\n    read_from_stdin = False\n    for p in files_paths:\n        if str(p) == \"-\":\n            read_from_stdin = True\n        elif not p.exists():\n            _l.error(f'File or directory \"{str(p)}\" does not exist.')\n            sys.exit(1)\n    if read_from_stdin:\n        if len(files_paths) > 1:\n            _l.error('If you pass \"-\", you cannot pass anything else.')\n            sys.exit(1)\n        if recursive:\n            _l.error('If you pass \"-\", recursive scan is not meaningful.')\n            sys.exit(1)\n\n    if batch_size <= 0 or batch_size > 512:\n        _l.error(\"Batch size needs to be greater than 0 and less or equal than 512.\")\n        sys.exit(1)\n\n    if json_output and jsonl_output:\n        _l.error(\"You should use either --json or --jsonl, not both.\")\n        sys.exit(1)\n\n    if int(mime_output) + int(label_output) + int(magic_compatibility_mode) > 1:\n        _l.error(\"You should use only one of --mime, --label, --compatibility-mode.\")\n        sys.exit(1)\n\n    if recursive:\n        # recursively enumerate files within directories\n        expanded_paths = []\n        for p in files_paths:\n            if p.exists():\n                if p.is_file():\n                    expanded_paths.append(p)\n                elif p.is_dir():\n                    expanded_paths.extend(sorted(p.rglob(\"*\")))\n            elif str(p) == \"-\":\n                # this is \"read from stdin\", that's OK\n                pass\n            else:\n                _l.error(f'File or directory \"{str(p)}\" does not exist.')\n                sys.exit(1)\n        # the resulting list may still include some directories; thus, we filter them out.\n        files_paths: List[Path] = list(filter(lambda x: not x.is_dir(), expanded_paths))  # type: ignore[no-redef]\n\n    _l.info(f\"Considering {len(files_paths)} files\")\n    _l.debug(f\"Files: {files_paths}\")\n\n    # Select an alternative model checking: 1) CLI option, 2) env variable.\n    # If none of these is set, model_dir is left to None, and the Magika module\n    # will use the default model.\n    if model_dir is None:\n        model_dir_str = os.environ.get(\"MAGIKA_MODEL_DIR\")\n        if model_dir_str is not None and model_dir_str.strip() != \"\":\n            model_dir = Path(model_dir_str)\n\n    try:\n        magika = Magika(\n            model_dir=model_dir,\n            prediction_mode=PredictionMode(prediction_mode_str),\n            no_dereference=no_dereference,\n            verbose=verbose,\n            debug=debug,\n            use_colors=with_colors,\n        )\n    except MagikaError as mr:\n        _l.error(str(mr))\n        sys.exit(1)\n\n    start_color = \"\"\n    end_color = \"\"\n\n    color_by_group = {\n        \"document\": colors.LIGHT_PURPLE,\n        \"executable\": colors.LIGHT_GREEN,\n        \"archive\": colors.LIGHT_RED,\n        \"audio\": colors.YELLOW,\n        \"image\": colors.YELLOW,\n        \"video\": colors.YELLOW,\n        \"code\": colors.LIGHT_BLUE,\n    }\n\n    # updated only when we need to output in JSON format\n    all_predictions: List[Tuple[Path, MagikaResult]] = []\n\n    batches_num = len(files_paths) // batch_size\n    if len(files_paths) % batch_size != 0:\n        batches_num += 1\n    for batch_idx in range(batches_num):\n        batch_files_paths = files_paths[\n            batch_idx * batch_size : (batch_idx + 1) * batch_size\n        ]\n\n        if should_read_from_stdin(files_paths):\n            batch_predictions = [get_magika_result_from_stdin(magika)]\n        else:\n            batch_predictions = magika.identify_paths(batch_files_paths)\n\n        if json_output:\n            # we do not stream the output for JSON output\n            all_predictions.extend(zip(batch_files_paths, batch_predictions))\n        elif jsonl_output:\n            for file_path, result in zip(batch_files_paths, batch_predictions):\n                _l.raw_print_to_stdout(json.dumps(result.asdict()))\n        else:\n            for file_path, result in zip(batch_files_paths, batch_predictions):\n                if result.ok:\n                    if mime_output:\n                        # If the user requested the MIME type, we use the mime type\n                        # regardless of the compatibility mode.\n                        output = result.prediction.output.mime_type\n                    elif label_output:\n                        output = str(result.prediction.output.label)\n                    else:  # human-readable description\n                        output = f\"{result.prediction.output.description} ({result.prediction.output.group})\"\n\n                        if (\n                            result.prediction.dl.label != ContentTypeLabel.UNDEFINED\n                            and result.prediction.dl.label\n                            != result.prediction.output.label\n                            and result.prediction.overwrite_reason\n                            == OverwriteReason.LOW_CONFIDENCE\n                        ):\n                            # It seems that we had a low-confidence prediction\n                            # from the model. Let's warn the user about our best\n                            # bet.\n                            output += (\n                                \" [Low-confidence model best-guess: \"\n                                f\"{result.prediction.dl.description} ({result.prediction.dl.group}), \"\n                                f\"score={result.prediction.score}]\"\n                            )\n\n                    if with_colors:\n                        start_color = color_by_group.get(\n                            result.prediction.output.group, colors.WHITE\n                        )\n                        end_color = colors.RESET\n                else:\n                    output = result.status\n                    start_color = \"\"\n                    end_color = \"\"\n\n                if output_score and result.ok:\n                    score = int(result.prediction.score * 100)\n                    _l.raw_print_to_stdout(\n                        f\"{start_color}{file_path}: {output} {score}%{end_color}\"\n                    )\n                else:\n                    _l.raw_print_to_stdout(\n                        f\"{start_color}{file_path}: {output}{end_color}\"\n                    )\n\n    if json_output:\n        _l.raw_print_to_stdout(\n            json.dumps(\n                [result.asdict() for (_, result) in all_predictions],\n                indent=4,\n            )\n        )\n\n\ndef should_read_from_stdin(files_paths: List[Path]) -> bool:\n    return len(files_paths) == 1 and str(files_paths[0]) == \"-\"\n\n\ndef get_magika_result_from_stdin(magika: Magika) -> MagikaResult:\n    content = sys.stdin.buffer.read()\n    result = magika.identify_bytes(content)\n    return result\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/magika/python/src/magika/cli/magika_client.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "Magika",
            "Description": "Magika impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/magika/python/src/magika/magika.py",
            "Implementation": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport io\nimport json\nimport logging\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import BinaryIO, Dict, List, Optional, Sequence, Set, Tuple, Union\n\nimport numpy as np\nimport numpy.typing as npt\nimport onnxruntime as rt\n\nfrom magika.logger import get_logger\nfrom magika.types import (\n    ContentTypeInfo,\n    ContentTypeLabel,\n    MagikaError,\n    MagikaPrediction,\n    MagikaResult,\n    ModelConfig,\n    ModelFeatures,\n    ModelOutput,\n    OverwriteReason,\n    PredictionMode,\n    Seekable,\n    Status,\n)\n\n_DEFAULT_MODEL_NAME = \"standard_v3_3\"\n\n\nclass Magika:\n    def __init__(\n        self,\n        model_dir: Optional[Path] = None,\n        prediction_mode: PredictionMode = PredictionMode.HIGH_CONFIDENCE,\n        no_dereference: bool = False,\n        verbose: bool = False,\n        debug: bool = False,\n        use_colors: bool = False,\n    ) -> None:\n        self._log = get_logger(use_colors=use_colors)\n\n        if verbose:\n            self._log.setLevel(logging.INFO)\n\n        if debug:\n            self._log.setLevel(logging.DEBUG)\n\n        if model_dir is not None:\n            self._model_dir = model_dir\n        else:\n            # use default model\n            self._model_dir = (\n                Path(__file__).parent / \"models\" / self._get_default_model_name()\n            )\n\n        self._model_path = self._model_dir / \"model.onnx\"\n        self._model_config_path = self._model_dir / \"config.min.json\"\n\n        if not self._model_dir.is_dir():\n            raise MagikaError(f\"model dir not found at {str(self._model_dir)}\")\n        if not self._model_path.is_file():\n            raise MagikaError(f\"model not found at {str(self._model_path)}\")\n        if not self._model_config_path.is_file():\n            raise MagikaError(\n                f\"model config not found at {str(self._model_config_path)}\"\n            )\n\n        self._model_config: ModelConfig = Magika._load_model_config(\n            self._model_config_path\n        )\n\n        self._target_labels_space_np = np.array(\n            list(map(str, self._model_config.target_labels_space))\n        )\n\n        self._prediction_mode = prediction_mode\n\n        self._no_dereference = no_dereference\n\n        content_types_kb_path = (\n            Path(__file__).parent / \"config\" / \"content_types_kb.min.json\"\n        )\n        self._cts_infos = Magika._load_content_types_kb(content_types_kb_path)\n\n        self._onnx_session = self._init_onnx_session()\n\n    def __repr__(self) -> str:\n        return str(self)\n\n    def __str__(self) -> str:\n        return f'Magika(module_version=\"{self.get_module_version()}\", model_name=\"{self.get_model_name()}\")'\n\n    def get_module_version(self) -> str:\n        return str(__import__(self.__module__).__version__)\n\n    def get_model_name(self) -> str:\n        return self._model_dir.name\n\n    def identify_path(self, path: Union[str, os.PathLike]) -> MagikaResult:\n        \"\"\"Identify the content type of a file given its path.\"\"\"\n\n        if isinstance(path, str) or isinstance(path, os.PathLike):\n            path = Path(path)\n        else:\n            raise TypeError(\n                f\"Path '{path}' is invalid: input path should be of type `Union[str, os.PathLike]`\"\n            )\n\n        return self._get_result_from_path(path)\n\n    def identify_paths(\n        self, paths: Sequence[Union[str, os.PathLike]]\n    ) -> List[MagikaResult]:\n        \"\"\"Identify the content type of a list of files given their paths.\"\"\"\n\n        if not isinstance(paths, Sequence):\n            raise TypeError(\"Input paths should be of type Sequence[Path]\")\n\n        paths_ = []\n        for path in paths:\n            if isinstance(path, str) or isinstance(path, os.PathLike):\n                paths_.append(Path(path))\n            else:\n                raise TypeError(\n                    f\"Input '{path}' is invalid: input path should be of type `Union[str, os.PathLike]`\"\n                )\n\n        return self._get_results_from_paths(paths_)\n\n    def identify_bytes(self, content: bytes) -> MagikaResult:\n        \"\"\"Identify the content type of raw bytes.\"\"\"\n\n        if not isinstance(content, bytes):\n            raise TypeError(\n                f\"Input content should be of type 'bytes', not {type(content)}.\"\n            )\n\n        return self._get_result_from_seekable(Seekable(io.BytesIO(content)))\n\n    def identify_stream(self, stream: BinaryIO) -> MagikaResult:\n        \"\"\"Identify the content type of a BinaryIO stream. Note that this method will\n        seek() around the stream.\"\"\"\n\n        if not isinstance(stream, io.IOBase) or not stream.readable():  # type: ignore[unreachable]\n            raise TypeError(\"Input stream must be a readable BinaryIO object.\")\n\n        # Explicitly test for the most common error so that we can return an\n        # helpful error message.\n        if isinstance(stream, io.TextIOBase):  # type: ignore[unreachable]\n            raise TypeError(\n                \"Input stream must be opened in bytes mode, not in text mode.\"\n            )\n\n        if not isinstance(stream, io.BufferedIOBase):\n            raise TypeError(\"Input stream must be a readable BinaryIO object.\")\n\n        if (\n            not hasattr(stream, \"seek\")\n            or not hasattr(stream, \"read\")\n            or not hasattr(stream, \"tell\")\n        ):\n            raise TypeError(\"Input stream must have seek, read, and tell methods.\")\n\n        try:\n            current_position = stream.tell()\n            result = self._get_result_from_seekable(Seekable(stream))\n        finally:\n            # seek to the previous position even in case of exceptions\n            stream.seek(current_position)\n        return result\n\n    def get_output_content_types(self) -> List[ContentTypeLabel]:\n        \"\"\"This method returns the list of all possible output content types of\n        the module. I.e., all possible values for\n        `MagikaResult.prediction.output.label`.  This considers the list of\n        possible outputs from the model itself, but also keeps into account\n        additional configuration such as `override_map` and special content\n        types such as `ContentTypeLabel.EMPTY` or `ContentTypeLabel.SYMLINK`.\n        \"\"\"\n\n        target_labels_space = self._model_config.target_labels_space\n        overwrite_map = self._model_config.overwrite_map\n\n        output_content_types: Set[ContentTypeLabel] = {\n            ContentTypeLabel.DIRECTORY,\n            ContentTypeLabel.EMPTY,\n            ContentTypeLabel.SYMLINK,\n            ContentTypeLabel.TXT,\n            ContentTypeLabel.UNKNOWN,\n        }\n        for ct in target_labels_space:\n            # Check if we would overwrite this target label; if not, use the\n            # target label itself.\n            output_ct = overwrite_map.get(ct, ct)\n            output_content_types.add(output_ct)\n\n        return sorted(output_content_types)\n\n    def get_model_content_types(self) -> List[ContentTypeLabel]:\n        \"\"\"This method returns the list of all possible output of the underlying\n        model. I.e., all possible values for `MagikaResult.prediction.dl.label`.\n        Note that, in general, the list of \"model outputs\" is different than the\n        \"tool outputs\" as in some cases the model is not even used, or the\n        model's output is overwritten due to a low-confidence score, or other\n        reasons.  This API is useful mostly for debugging purposes; the vast\n        majority of client should use `get_output_content_types()`.\n        \"\"\"\n\n        model_content_types: Set[ContentTypeLabel] = {\n            ContentTypeLabel.UNDEFINED,\n        }\n        model_content_types.update(self._model_config.target_labels_space)\n        return sorted(model_content_types)\n\n    @staticmethod\n    def _get_default_model_name() -> str:\n        \"\"\"This returns the default model name.\n\n        We make this method static so that it can be used by external\n        clients/tests without the need to instantiate a Magika object.\n        \"\"\"\n\n        return _DEFAULT_MODEL_NAME\n\n    @staticmethod\n    def _load_content_types_kb(\n        content_types_kb_json_path: Path,\n    ) -> Dict[ContentTypeLabel, ContentTypeInfo]:\n        TXT_MIME_TYPE = \"text/plain\"\n        UNKNOWN_MIME_TYPE = \"application/octet-stream\"\n        UNKNOWN_GROUP = \"unknown\"\n\n        out = {}\n        for ct_name, ct_info in json.loads(\n            content_types_kb_json_path.read_text()\n        ).items():\n            is_text = ct_info[\"is_text\"]\n            if is_text:\n                default_mime_type = TXT_MIME_TYPE\n            else:\n                default_mime_type = UNKNOWN_MIME_TYPE\n            mime_type = (\n                default_mime_type\n                if ct_info[\"mime_type\"] is None\n                else ct_info[\"mime_type\"]\n            )\n            group = UNKNOWN_GROUP if ct_info[\"group\"] is None else ct_info[\"group\"]\n            description = (\n                ct_name if ct_info[\"description\"] is None else ct_info[\"description\"]\n            )\n            extensions = ct_info[\"extensions\"]\n            out[ContentTypeLabel(ct_name)] = ContentTypeInfo(\n                label=ContentTypeLabel(ct_name),\n                mime_type=mime_type,\n                group=group,\n                description=description,\n                extensions=extensions,\n                is_text=is_text,\n            )\n        return out\n\n    @staticmethod\n    def _load_model_config(model_config_path: Path) -> ModelConfig:\n        config = json.loads(model_config_path.read_text())\n\n        return ModelConfig(\n            beg_size=config[\"beg_size\"],\n            mid_size=config[\"mid_size\"],\n            end_size=config[\"end_size\"],\n            use_inputs_at_offsets=config[\"use_inputs_at_offsets\"],\n            medium_confidence_threshold=config[\"medium_confidence_threshold\"],\n            min_file_size_for_dl=config[\"min_file_size_for_dl\"],\n            padding_token=config[\"padding_token\"],\n            block_size=config[\"block_size\"],\n            target_labels_space=[\n                ContentTypeLabel(ct_str) for ct_str in config[\"target_labels_space\"]\n            ],\n            thresholds={\n                ContentTypeLabel(k): v for k, v in config[\"thresholds\"].items()\n            },\n            overwrite_map={\n                ContentTypeLabel(k): ContentTypeLabel(v)\n                for k, v in config[\"overwrite_map\"].items()\n            },\n        )\n\n    def _init_onnx_session(self) -> rt.InferenceSession:\n        start_time = time.time()\n        rt.disable_telemetry_events()\n\n        onnx_session = rt.InferenceSession(\n            self._model_path,\n            providers=[\"CPUExecutionProvider\"],\n        )\n        elapsed_time = 1000 * (time.time() - start_time)\n        self._log.debug(\n            f'ONNX DL model \"{self._model_path}\" loaded in {elapsed_time:.03f} ms'\n        )\n        return onnx_session\n\n    def _get_ct_info(self, content_type: ContentTypeLabel) -> ContentTypeInfo:\n        return self._cts_infos[content_type]\n\n    def _get_results_from_paths(self, paths: List[Path]) -> List[MagikaResult]:\n        \"\"\"Given a list of paths, returns a list of MagikaResult objects, which\n        contain relevant information, such as: file path, the output of the DL\n        model, the confidence score, the output of the tool, and associated\n        metadata. The order of the predictions matches the order of the input\n        paths.\"\"\"\n\n        # We do a first pass on all files: we collect features for the files\n        # that need to be analyzed with the DL model, and we already determine\n        # the output for the remaining ones.\n\n        # We use a \"str\" instead of Path because it makes it easier later on to\n        # serialize.\n        all_outputs: Dict[str, MagikaResult] = {}  # {path: <output>, ...}\n\n        # We use a list and not the dict because that's what we need later on\n        # for inference.\n        all_features: List[Tuple[Path, ModelFeatures]] = []\n\n        self._log.debug(\n            f\"Processing input files and extracting features for {len(paths)} samples\"\n        )\n        start_time = time.time()\n        for path in paths:\n            output, features = self._get_result_or_features_from_path(path)\n            if output is not None:\n                all_outputs[str(path)] = output\n            else:\n                assert features is not None\n                all_features.append((path, features))\n        elapsed_time = 1000 * (time.time() - start_time)\n        self._log.debug(f\"First pass and features extracted in {elapsed_time:.03f} ms\")\n\n        # Get the outputs via DL for the files that need it.\n        for path_str, result in self._get_results_from_features(all_features).items():\n            all_outputs[path_str] = result\n\n        # Finally, we collect the predictions in a final list, sorted by the\n        # initial paths list (and not by insertion order).\n        sorted_outputs = []\n        for path in paths:\n            sorted_outputs.append(all_outputs[str(path)])\n        return sorted_outputs\n\n    def _get_result_from_path(self, path: Path) -> MagikaResult:\n        return self._get_results_from_paths([path])[0]\n\n    def _get_result_from_seekable(self, seekable: Seekable) -> MagikaResult:\n        result, features = self._get_result_or_features_from_seekable(seekable)\n        if result is not None:\n            return result\n        assert features is not None\n        return self._get_results_from_features([(Path(\"-\"), features)])[\"-\"]\n\n    @staticmethod\n    def _extract_features_from_seekable(\n        seekable: Seekable,\n        beg_size: int,\n        mid_size: int,\n        end_size: int,\n        padding_token: int,\n        block_size: int,\n        use_inputs_at_offsets: bool,\n    ) -> ModelFeatures:\n        \"\"\"Extract features from an input seekable.\n\n        This implements features extraction v2 from a seekable, which is an\n        abstraction about anything that has a size and that can be \"read_at\" a\n        specific offset, such as a file or a buffer. This is implemented so that\n        we do not need to load the entire file in memory or scan the entire\n        buffer.\n\n        High-level overview on what we do:\n        - We read (at most) `block_size` bytes from the beginning and from the\n        end.\n        - We normalize these bytes by stripping whitespaces.\n        - We consider `beg_size` and `end_size` bytes as `beg` and `end`\n        features. If we don't have enough bytes, we use `padding_token` as\n        padding.\n\n        See comments below for the specifics and handling of corner cases.\n\n        NOTE: This implementation does not support extraction of `mid` features\n        and `use_inputs_at_offsets`.\n        \"\"\"\n\n        assert beg_size < block_size\n        assert mid_size == 0\n        assert end_size < block_size\n        assert not use_inputs_at_offsets\n\n        # we read at most block_size bytes\n        bytes_num_to_read = min(block_size, seekable.size)\n\n        if beg_size > 0:\n            # Read at most `block_size` bytes from the beginning; `lstrip()``\n            # them (or `strip()` them if the file size is less or equal than\n            # `block_size`); take at most `beg_size` bytes, and optionally pad\n            # them with `padding_token` to get to a list of `beg_size` integers.\n            beg_content = seekable.read_at(0, bytes_num_to_read)\n            beg_content = beg_content.lstrip()\n            beg_ints = Magika._get_beg_ints_with_padding(\n                beg_content, beg_size, padding_token\n            )\n        else:\n            beg_ints = []\n\n        if end_size > 0:\n            # Read at most `block_size` bytes from the end; `rstrip()`` them (or\n            # `strip()` them if the file size is less or equal than\n            # `block_size`); take at most `end_size` bytes (from the end), and\n            # optionally pad them (at the beginning) with `padding_token` to get\n            # to a list of `end_size` integers.\n            end_content = seekable.read_at(\n                seekable.size - bytes_num_to_read, bytes_num_to_read\n            )\n            end_content = end_content.rstrip()\n            end_ints = Magika._get_end_ints_with_padding(\n                end_content, end_size, padding_token\n            )\n        else:\n            end_ints = []\n\n        return ModelFeatures(\n            beg=beg_ints,\n            mid=[],\n            end=end_ints,\n            offset_0x8000_0x8007=[],\n            offset_0x8800_0x8807=[],\n            offset_0x9000_0x9007=[],\n            offset_0x9800_0x9807=[],\n        )\n\n    @staticmethod\n    def _get_beg_ints_with_padding(\n        beg_content: bytes, beg_size: int, padding_token: int\n    ) -> List[int]:\n        \"\"\"Take an (already-stripped) buffer as input and extract beg ints.\n        This returns a list of integers whose length is exactly beg_size. If\n        the buffer is bigger than required, take only the initial portion. If\n        the buffer is shorter, add padding at the end.\n        \"\"\"\n\n        if beg_size < len(beg_content):\n            # we don't need so many bytes\n            beg_content = beg_content[0:beg_size]\n\n        beg_ints = list(map(int, beg_content))\n\n        if len(beg_ints) < beg_size:\n            # we don't have enough ints, add padding\n            beg_ints = beg_ints + ([padding_token] * (beg_size - len(beg_ints)))\n\n        assert len(beg_ints) == beg_size\n\n        return beg_ints\n\n    @staticmethod\n    def _get_end_ints_with_padding(\n        end_content: bytes, end_size: int, padding_token: int\n    ) -> List[int]:\n        \"\"\"Take an (already-stripped) buffer as input and extract end ints. This\n        returns a list of integers whose length is exactly end_size.  If the\n        buffer is bigger than required, take only the last portion. If the\n        buffer is shorter, add padding at the beginning.\n        \"\"\"\n\n        if end_size < len(end_content):\n            # we don't need so many bytes\n            end_content = end_content[len(end_content) - end_size : len(end_content)]\n\n        end_ints = list(map(int, end_content))\n\n        if len(end_ints) < end_size:\n            # we don't have enough ints, add padding\n            end_ints = ([padding_token] * (end_size - len(end_ints))) + end_ints\n\n        assert len(end_ints) == end_size\n\n        return end_ints\n\n    def _get_model_outputs_from_features(\n        self, all_features: List[Tuple[Path, ModelFeatures]]\n    ) -> List[Tuple[Path, ModelOutput]]:\n        raw_preds = self._get_raw_predictions(all_features)\n        top_preds_idxs = np.argmax(raw_preds, axis=1)\n        preds_content_types_labels = self._target_labels_space_np[top_preds_idxs]\n        scores = np.max(raw_preds, axis=1)\n\n        return [\n            (path, ModelOutput(label=ContentTypeLabel(label), score=float(score)))\n            for (path, _), label, score in zip(\n                all_features, preds_content_types_labels, scores\n            )\n        ]\n\n    def _get_results_from_features(\n        self, all_features: List[Tuple[Path, ModelFeatures]]\n    ) -> Dict[str, MagikaResult]:\n        # We now do inference for those files that need it.\n\n        if len(all_features) == 0:\n            # nothing to be done\n            return {}\n\n        results: Dict[str, MagikaResult] = {}\n\n        for path, model_output in self._get_model_outputs_from_features(all_features):\n            # In additional to the content type label from the DL model, we\n            # also allow for other logic to overwrite such result. For\n            # debugging and information purposes, the JSON output stores\n            # both the raw DL model output and the final output we return to\n            # the user.\n\n            output_label, overwrite_reason = (\n                self._get_output_label_from_dl_label_and_score(\n                    model_output.label, model_output.score\n                )\n            )\n\n            results[str(path)] = self._get_result_from_labels_and_score(\n                path=path,\n                dl_label=model_output.label,\n                output_label=output_label,\n                score=model_output.score,\n                overwrite_reason=overwrite_reason,\n            )\n\n        return results\n\n    def _get_output_label_from_dl_label_and_score(\n        self, dl_label: ContentTypeLabel, score: float\n    ) -> Tuple[ContentTypeLabel, OverwriteReason]:\n        overwrite_reason = OverwriteReason.NONE\n\n        # Overwrite dl_label if specified in the overwrite_map model config.\n        output_label = self._model_config.overwrite_map.get(dl_label, dl_label)\n        if output_label != dl_label:\n            overwrite_reason = OverwriteReason.OVERWRITE_MAP\n\n        # The following code checks whether the score is \"high enough\", where\n        # \"high enough\" depends on the selected prediction mode. If the score is\n        # high enough, we return the (potentially ovewritten) model prediction;\n        # if it is not, we return a generic content type, such as TXT or\n        # UNKNOWN.\n        if self._prediction_mode == PredictionMode.BEST_GUESS:\n            # We take the (potentially overwritten) model prediction, no matter\n            # what the score is.\n            pass\n        elif (\n            self._prediction_mode == PredictionMode.HIGH_CONFIDENCE\n            and score\n            >= self._model_config.thresholds.get(\n                dl_label, self._model_config.medium_confidence_threshold\n            )\n        ):\n            # The model score is higher than the per-content-type\n            # high-confidence threshold, so we keep it (note that the model\n            # prediction may have been overwritten).\n            pass\n        elif (\n            self._prediction_mode == PredictionMode.MEDIUM_CONFIDENCE\n            and score >= self._model_config.medium_confidence_threshold\n        ):\n            # The model score is higher than the generic medium-confidence\n            # threshold, so we keep it (note that the model prediction may have\n            # been overwritten).\n            pass\n        else:\n            # We are not in a condition to trust the model, we opt to return\n            # generic labels. Note that here we use an implicit assumption that\n            # the model has, at the very least, got the binary vs. text category\n            # right. This allows us to pick between unknown and txt without the\n            # need to read or scan the file bytes once again.\n            overwrite_reason = OverwriteReason.LOW_CONFIDENCE\n            if self._get_ct_info(output_label).is_text:\n                output_label = ContentTypeLabel.TXT\n            else:\n                output_label = ContentTypeLabel.UNKNOWN\n            if dl_label == output_label:\n                # overwrite_reason is useful to convey to clients why the output\n                # predicted is different than the model predicted type; if those\n                # two are the same, the model predicted type has not actually\n                # been overwritten, so we set this to NONE.\n                overwrite_reason = OverwriteReason.NONE\n\n        return output_label, overwrite_reason\n\n    def _get_result_from_labels_and_score(\n        self,\n        path: Path,\n        dl_label: ContentTypeLabel,\n        output_label: ContentTypeLabel,\n        score: float,\n        overwrite_reason: OverwriteReason = OverwriteReason.NONE,\n    ) -> MagikaResult:\n        return MagikaResult(\n            path=path,\n            prediction=MagikaPrediction(\n                dl=self._get_ct_info(dl_label),\n                output=self._get_ct_info(output_label),\n                score=score,\n                overwrite_reason=overwrite_reason,\n            ),\n        )\n\n    def _get_result_or_features_from_path(\n        self, path: Path\n    ) -> Tuple[Optional[MagikaResult], Optional[ModelFeatures]]:\n        \"\"\"\n        Given a path, we return either a MagikaOutput or a MagikaFeatures.\n\n        There are some files and corner cases for which we do not need to use\n        deep learning to get the output; in these cases, we already return a\n        MagikaOutput object.\n\n        For some other files, we do need to use deep learning, in which case we\n        return a MagikaFeatures object. Note that for now we just collect the\n        features instead of already performing inference because we want to use\n        batching.\n        \"\"\"\n\n        if self._no_dereference and path.is_symlink():\n            result = self._get_result_from_labels_and_score(\n                path=path,\n                dl_label=ContentTypeLabel.UNDEFINED,\n                output_label=ContentTypeLabel.SYMLINK,\n                score=1.0,\n            )\n            return result, None\n\n        if not path.exists():\n            return MagikaResult(path=path, status=Status.FILE_NOT_FOUND_ERROR), None\n\n        if path.is_file():\n            if not os.access(path, os.R_OK):\n                return MagikaResult(path=path, status=Status.PERMISSION_ERROR), None\n\n            else:\n                # There are no additional path-specific corner cases, we can\n                # treat the input path as a stream.\n                with open(path, \"rb\") as stream:\n                    return self._get_result_or_features_from_seekable(\n                        Seekable(stream), path\n                    )\n\n        elif path.is_dir():\n            result = self._get_result_from_labels_and_score(\n                path=path,\n                dl_label=ContentTypeLabel.UNDEFINED,\n                output_label=ContentTypeLabel.DIRECTORY,\n                score=1.0,\n            )\n            return result, None\n\n        else:\n            result = self._get_result_from_labels_and_score(\n                path=path,\n                dl_label=ContentTypeLabel.UNDEFINED,\n                output_label=ContentTypeLabel.UNKNOWN,\n                score=1.0,\n            )\n            return result, None\n\n        raise Exception(\"unreachable\")\n\n    def _get_result_or_features_from_seekable(\n        self, seekable: Seekable, path: Path = Path(\"-\")\n    ) -> Tuple[Optional[MagikaResult], Optional[ModelFeatures]]:\n        \"\"\"\n        Given a Seekable object (which is a wrapper of BinaryIO), we return\n        either a MagikaOutput or a MagikaFeatures.\n\n        There are some corner cases for which we do not need to use deep\n        learning to get the output; in these cases, we return directly a\n        MagikaOutput object.\n\n        For all other cases, we do need to use deep learning, in which case we\n        return a MagikaFeatures object. Note that for now we just collect the\n        features instead of already performing inference because we want to use\n        batching.\n        \"\"\"\n\n        if seekable.size == 0:\n            result = self._get_result_from_labels_and_score(\n                path=path,\n                dl_label=ContentTypeLabel.UNDEFINED,\n                output_label=ContentTypeLabel.EMPTY,\n                score=1.0,\n            )\n            return result, None\n\n        elif seekable.size < self._model_config.min_file_size_for_dl:\n            content = seekable.read_at(0, seekable.size)\n            result = self._get_result_from_few_bytes(content, path=path)\n            return result, None\n\n        else:\n            file_features = Magika._extract_features_from_seekable(\n                seekable,\n                self._model_config.beg_size,\n                self._model_config.mid_size,\n                self._model_config.end_size,\n                self._model_config.padding_token,\n                self._model_config.block_size,\n                self._model_config.use_inputs_at_offsets,\n            )\n            # Check whether we have enough bytes for a meaningful\n            # detection, and not just padding.\n            if (\n                file_features.beg[self._model_config.min_file_size_for_dl - 1]\n                == self._model_config.padding_token\n            ):\n                # If the n-th token is padding, then it means that,\n                # post-stripping, we do not have enough meaningful\n                # bytes.\n                bytes_to_read = min(seekable.size, self._model_config.block_size)\n                content = seekable.read_at(0, bytes_to_read)\n                result = self._get_result_from_few_bytes(content, path=path)\n                return result, None\n\n            else:\n                # We have enough bytes, return the features for a model\n                # prediction.\n                return None, file_features\n\n        raise Exception(\"unreachable\")\n\n    def _get_result_from_few_bytes(\n        self, content: bytes, path: Path = Path(\"-\")\n    ) -> MagikaResult:\n        assert len(content) <= 4 * self._model_config.block_size\n        label = self._get_label_from_few_bytes(content)\n        return self._get_result_from_labels_and_score(\n            path=path,\n            dl_label=ContentTypeLabel.UNDEFINED,\n            output_label=label,\n            score=1.0,\n        )\n\n    def _get_label_from_few_bytes(self, content: bytes) -> ContentTypeLabel:\n        try:\n            label = ContentTypeLabel.TXT\n            _ = content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            label = ContentTypeLabel.UNKNOWN\n        return label\n\n    def _get_raw_predictions(\n        self, features: List[Tuple[Path, ModelFeatures]]\n    ) -> npt.NDArray:\n        \"\"\"\n        Given a list of (path, features), return a (files_num, features_size)\n        matrix encoding the predictions.\n        \"\"\"\n\n        start_time = time.time()\n        X_bytes = []\n        for _, fs in features:\n            sample_bytes = []\n            if self._model_config.beg_size > 0:\n                sample_bytes.extend(fs.beg[: self._model_config.beg_size])\n            if self._model_config.mid_size > 0:\n                sample_bytes.extend(fs.mid[: self._model_config.mid_size])\n            if self._model_config.end_size > 0:\n                sample_bytes.extend(fs.end[-self._model_config.end_size :])\n            X_bytes.append(sample_bytes)\n        X = np.array(X_bytes, dtype=np.int32)\n        elapsed_time = 1000 * (time.time() - start_time)\n        self._log.debug(f\"DL input prepared in {elapsed_time:.03f} ms\")\n\n        raw_predictions_list = []\n        samples_num = X.shape[0]\n\n        max_internal_batch_size = 1000\n        batches_num = samples_num // max_internal_batch_size\n        if samples_num % max_internal_batch_size != 0:\n            batches_num += 1\n\n        for batch_idx in range(batches_num):\n            self._log.debug(\n                f\"Getting raw predictions for (internal) batch {batch_idx+1}/{batches_num}\"\n            )\n            start_idx = batch_idx * max_internal_batch_size\n            end_idx = min((batch_idx + 1) * max_internal_batch_size, samples_num)\n\n            start_time = time.time()\n            batch_raw_predictions = self._onnx_session.run(\n                [\"target_label\"], {\"bytes\": X[start_idx:end_idx, :]}\n            )[0]\n            elapsed_time = 1000 * (time.time() - start_time)\n            self._log.debug(f\"DL raw prediction in {elapsed_time:.03f} ms\")\n\n            raw_predictions_list.append(batch_raw_predictions)\n        return np.concatenate(raw_predictions_list)\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}