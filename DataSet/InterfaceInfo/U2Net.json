{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/U-2-Net",
    "API_Calls": [
        {
            "Name": "call_u2net",
            "Description": "call_u2net",
            "Code": "import cv2\nimport torch\nfrom model import U2NET\nfrom torch.autograd import Variable\nimport numpy as np\nfrom glob import glob\nimport os\n\ndef detect_single_face(face_cascade,img):\n    # Convert into grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Detect faces\n    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n    if(len(faces)==0):\n        print(\"Warming: no face detection, the portrait u2net will run on the whole image!\")\n        return None\n\n    # filter to keep the largest face\n    wh = 0\n    idx = 0\n    for i in range(0,len(faces)):\n        (x,y,w,h) = faces[i]\n        if(wh<w*h):\n            idx = i\n            wh = w*h\n\n    return faces[idx]\n\n# crop, pad and resize face region to 512x512 resolution\ndef crop_face(img, face):\n\n    # no face detected, return the whole image and the inference will run on the whole image\n    if(face is None):\n        return img\n    (x, y, w, h) = face\n\n    height,width = img.shape[0:2]\n\n    # crop the face with a bigger bbox\n    hmw = h - w\n    # hpad = int(h/2)+1\n    # wpad = int(w/2)+1\n\n    l,r,t,b = 0,0,0,0\n    lpad = int(float(w)*0.4)\n    left = x-lpad\n    if(left<0):\n        l = lpad-x\n        left = 0\n\n    rpad = int(float(w)*0.4)\n    right = x+w+rpad\n    if(right>width):\n        r = right-width\n        right = width\n\n    tpad = int(float(h)*0.6)\n    top = y - tpad\n    if(top<0):\n        t = tpad-y\n        top = 0\n\n    bpad  = int(float(h)*0.2)\n    bottom = y+h+bpad\n    if(bottom>height):\n        b = bottom-height\n        bottom = height\n\n\n    im_face = img[top:bottom,left:right]\n    if(len(im_face.shape)==2):\n        im_face = np.repeat(im_face[:,:,np.newaxis],(1,1,3))\n\n    im_face = np.pad(im_face,((t,b),(l,r),(0,0)),mode='constant',constant_values=((255,255),(255,255),(255,255)))\n\n    # pad to achieve image with square shape for avoding face deformation after resizing\n    hf,wf = im_face.shape[0:2]\n    if(hf-2>wf):\n        wfp = int((hf-wf)/2)\n        im_face = np.pad(im_face,((0,0),(wfp,wfp),(0,0)),mode='constant',constant_values=((255,255),(255,255),(255,255)))\n    elif(wf-2>hf):\n        hfp = int((wf-hf)/2)\n        im_face = np.pad(im_face,((hfp,hfp),(0,0),(0,0)),mode='constant',constant_values=((255,255),(255,255),(255,255)))\n\n    # resize to have 512x512 resolution\n    im_face = cv2.resize(im_face, (512,512), interpolation = cv2.INTER_AREA)\n\n    return im_face\n\ndef normPRED(d):\n    ma = torch.max(d)\n    mi = torch.min(d)\n\n    dn = (d-mi)/(ma-mi)\n\n    return dn\n\ndef inference(net,input):\n\n    # normalize the input\n    tmpImg = np.zeros((input.shape[0],input.shape[1],3))\n    input = input/np.max(input)\n\n    tmpImg[:,:,0] = (input[:,:,2]-0.406)/0.225\n    tmpImg[:,:,1] = (input[:,:,1]-0.456)/0.224\n    tmpImg[:,:,2] = (input[:,:,0]-0.485)/0.229\n\n    # convert BGR to RGB\n    tmpImg = tmpImg.transpose((2, 0, 1))\n    tmpImg = tmpImg[np.newaxis,:,:,:]\n    tmpImg = torch.from_numpy(tmpImg)\n\n    # convert numpy array to torch tensor\n    tmpImg = tmpImg.type(torch.FloatTensor)\n\n    if torch.cuda.is_available():\n        tmpImg = Variable(tmpImg.cuda())\n    else:\n        tmpImg = Variable(tmpImg)\n\n    # inference\n    d1,d2,d3,d4,d5,d6,d7= net(tmpImg)\n\n    # normalization\n    pred = 1.0 - d1[:,0,:,:]\n    pred = normPRED(pred)\n\n    # convert torch tensor to numpy array\n    pred = pred.squeeze()\n    pred = pred.cpu().data.numpy()\n\n    del d1,d2,d3,d4,d5,d6,d7\n\n    return pred\n\ndef main():\n\n    # get the image path list for inference\n    im_list = glob('./test_data/test_portrait_images/your_portrait_im/*')\n    print(\"Number of images: \",len(im_list))\n    # indicate the output directory\n    out_dir = './test_data/test_portrait_images/your_portrait_results'\n    if(not os.path.exists(out_dir)):\n        os.mkdir(out_dir)\n\n    # Load the cascade face detection model\n    face_cascade = cv2.CascadeClassifier('./saved_models/face_detection_cv2/haarcascade_frontalface_default.xml')\n    # u2net_portrait path\n    model_dir = './saved_models/u2net_portrait/u2net_portrait.pth'\n\n    # load u2net_portrait model\n    net = U2NET(3,1)\n    net.load_state_dict(torch.load(model_dir))\n    if torch.cuda.is_available():\n        net.cuda()\n    net.eval()\n\n    # do the inference one-by-one\n    for i in range(0,len(im_list)):\n        print(\"--------------------------\")\n        print(\"inferencing \", i, \"/\", len(im_list), im_list[i])\n\n        # load each image\n        img = cv2.imread(im_list[i])\n        height,width = img.shape[0:2]\n        face = detect_single_face(face_cascade,img)\n        im_face = crop_face(img, face)\n        im_portrait = inference(net,im_face)\n\n        # save the output\n        cv2.imwrite(out_dir+\"/\"+im_list[i].split('/')[-1][0:-4]+'.png',(im_portrait*255).astype(np.uint8))\n\nif __name__ == '__main__':\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/U-2-Net/u2net_portrait_demo.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "U2NET",
            "Description": "U2NET",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/U-2-Net/model/u2net.py",
            "Implementation": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass U2NET(nn.Module):\n\n    def __init__(self,in_ch=3,out_ch=1):\n        super(U2NET,self).__init__()\n\n        self.stage1 = RSU7(in_ch,32,64)\n        self.pool12 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage2 = RSU6(64,32,128)\n        self.pool23 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage3 = RSU5(128,64,256)\n        self.pool34 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage4 = RSU4(256,128,512)\n        self.pool45 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage5 = RSU4F(512,256,512)\n        self.pool56 = nn.MaxPool2d(2,stride=2,ceil_mode=True)\n\n        self.stage6 = RSU4F(512,256,512)\n\n        # decoder\n        self.stage5d = RSU4F(1024,256,512)\n        self.stage4d = RSU4(1024,128,256)\n        self.stage3d = RSU5(512,64,128)\n        self.stage2d = RSU6(256,32,64)\n        self.stage1d = RSU7(128,16,64)\n\n        self.side1 = nn.Conv2d(64,out_ch,3,padding=1)\n        self.side2 = nn.Conv2d(64,out_ch,3,padding=1)\n        self.side3 = nn.Conv2d(128,out_ch,3,padding=1)\n        self.side4 = nn.Conv2d(256,out_ch,3,padding=1)\n        self.side5 = nn.Conv2d(512,out_ch,3,padding=1)\n        self.side6 = nn.Conv2d(512,out_ch,3,padding=1)\n\n        self.outconv = nn.Conv2d(6*out_ch,out_ch,1)\n\n    def forward(self,x):\n\n        hx = x\n\n        #stage 1\n        hx1 = self.stage1(hx)\n        hx = self.pool12(hx1)\n\n        #stage 2\n        hx2 = self.stage2(hx)\n        hx = self.pool23(hx2)\n\n        #stage 3\n        hx3 = self.stage3(hx)\n        hx = self.pool34(hx3)\n\n        #stage 4\n        hx4 = self.stage4(hx)\n        hx = self.pool45(hx4)\n\n        #stage 5\n        hx5 = self.stage5(hx)\n        hx = self.pool56(hx5)\n\n        #stage 6\n        hx6 = self.stage6(hx)\n        hx6up = _upsample_like(hx6,hx5)\n\n        #-------------------- decoder --------------------\n        hx5d = self.stage5d(torch.cat((hx6up,hx5),1))\n        hx5dup = _upsample_like(hx5d,hx4)\n\n        hx4d = self.stage4d(torch.cat((hx5dup,hx4),1))\n        hx4dup = _upsample_like(hx4d,hx3)\n\n        hx3d = self.stage3d(torch.cat((hx4dup,hx3),1))\n        hx3dup = _upsample_like(hx3d,hx2)\n\n        hx2d = self.stage2d(torch.cat((hx3dup,hx2),1))\n        hx2dup = _upsample_like(hx2d,hx1)\n\n        hx1d = self.stage1d(torch.cat((hx2dup,hx1),1))\n\n\n        #side output\n        d1 = self.side1(hx1d)\n\n        d2 = self.side2(hx2d)\n        d2 = _upsample_like(d2,d1)\n\n        d3 = self.side3(hx3d)\n        d3 = _upsample_like(d3,d1)\n\n        d4 = self.side4(hx4d)\n        d4 = _upsample_like(d4,d1)\n\n        d5 = self.side5(hx5d)\n        d5 = _upsample_like(d5,d1)\n\n        d6 = self.side6(hx6)\n        d6 = _upsample_like(d6,d1)\n\n        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),1))\n\n        return F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)",
            "Examples": [
                "\n"
            ]
        }
    ]
}