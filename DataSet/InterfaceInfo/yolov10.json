{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/yolov10",
    "API_Calls": [
        {
            "Name": "generate_divided_data",
            "Description": "该处调用 SAM 的目的是利用其强大的分割能力，将只有目标检测标签的数据集转换为包含分割标签的数据集，从而为模型训练提供更丰富的信息，提升模型在分割任务上的性能。",
            "Code": "# Ultralytics YOLO 🚀, AGPL-3.0 license\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\n\nfrom ultralytics.utils import LOGGER, TQDM\nfrom ultralytics.utils.files import increment_path\n\ndef yolo_bbox2segment(im_dir, save_dir=None, sam_model=\"sam_b.pt\"):\n    \"\"\"\n    Converts existing object detection dataset (bounding boxes) to segmentation dataset or oriented bounding box (OBB)\n    in YOLO format. Generates segmentation data using SAM auto-annotator as needed.\n\n    Args:\n        im_dir (str | Path): Path to image directory to convert.\n        save_dir (str | Path): Path to save the generated labels, labels will be saved\n            into `labels-segment` in the same directory level of `im_dir` if save_dir is None. Default: None.\n        sam_model (str): Segmentation model to use for intermediate segmentation data; optional.\n\n    Notes:\n        The input directory structure assumed for dataset:\n\n            - im_dir\n                ├─ 001.jpg\n                ├─ ..\n                └─ NNN.jpg\n            - labels\n                ├─ 001.txt\n                ├─ ..\n                └─ NNN.txt\n    \"\"\"\n    from ultralytics.data import YOLODataset\n    from ultralytics.utils.ops import xywh2xyxy\n    from ultralytics.utils import LOGGER\n    from ultralytics import SAM\n    from tqdm import tqdm\n\n    # NOTE: add placeholder to pass class index check\n    dataset = YOLODataset(im_dir, data=dict(names=list(range(1000))))\n    if len(dataset.labels[0][\"segments\"]) > 0:  # if it's segment data\n        LOGGER.info(\"Segmentation labels detected, no need to generate new ones!\")\n        return\n\n    LOGGER.info(\"Detection labels detected, generating segment labels by SAM model!\")\n    sam_model = SAM(sam_model)\n    for l in tqdm(dataset.labels, total=len(dataset.labels), desc=\"Generating segment labels\"):\n        h, w = l[\"shape\"]\n        boxes = l[\"bboxes\"]\n        if len(boxes) == 0:  # skip empty labels\n            continue\n        boxes[:, [0, 2]] *= w\n        boxes[:, [1, 3]] *= h\n        im = cv2.imread(l[\"im_file\"])\n        sam_results = sam_model(im, bboxes=xywh2xyxy(boxes), verbose=False, save=False)\n        l[\"segments\"] = sam_results[0].masks.xyn\n\n    save_dir = Path(save_dir) if save_dir else Path(im_dir).parent / \"labels-segment\"\n    save_dir.mkdir(parents=True, exist_ok=True)\n    for l in dataset.labels:\n        texts = []\n        lb_name = Path(l[\"im_file\"]).with_suffix(\".txt\").name\n        txt_file = save_dir / lb_name\n        cls = l[\"cls\"]\n        for i, s in enumerate(l[\"segments\"]):\n            line = (int(cls[i]), *s.reshape(-1))\n            texts.append((\"%g \" * len(line)).rstrip() % line)\n        if texts:\n            with open(txt_file, \"a\") as f:\n                f.writelines(text + \"\\n\" for text in texts)\n    LOGGER.info(f\"Generated segment labels saved in {save_dir}\")\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/yolov10/ultralytics/data/converter.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class_SAM",
            "Description": "SAM 类旨在实现可提示的实时图像分割。在进行预测时，predict 方法可对图像或视频源进行分割预测，支持实时流处理，还能接收边界框、点和标签等提示信息，同时会设置默认的配置参数并将提示信息封装到字典中，最后调用父类的 predict 方法完成预测。",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/yolov10/ultralytics/models/sam/model.py",
            "Implementation": "\n\nfrom pathlib import Path\n\nfrom ultralytics.engine.model import Model\nfrom ultralytics.utils.torch_utils import model_info\nfrom .build import build_sam\nfrom .predict import Predictor\n\nimport inspect\nimport sys\nfrom typing import Union\nimport numpy as np\nimport torch\nfrom ultralytics.cfg import TASK2DATA, get_cfg, get_save_dir\nfrom ultralytics.hub.utils import HUB_WEB_ROOT\nfrom ultralytics.nn.tasks import attempt_load_one_weight, guess_model_task, nn, yaml_model_load\nfrom ultralytics.utils import ASSETS, DEFAULT_CFG_DICT, LOGGER, RANK, SETTINGS, callbacks, checks, emojis, yaml_load\n\n\n\nclass Model(nn.Module):\n    \n\n    def __init__(\n        self,\n        model: Union[str, Path] = \"yolov8n.pt\",\n        task: str = None,\n        verbose: bool = False,\n    ) -> None:\n        \n        super().__init__()\n        self.callbacks = callbacks.get_default_callbacks()\n        self.predictor = None  # reuse predictor\n        self.model = None  # model object\n        self.trainer = None  # trainer object\n        self.ckpt = None  # if loaded from *.pt\n        self.cfg = None  # if loaded from *.yaml\n        self.ckpt_path = None\n        self.overrides = {}  # overrides for trainer object\n        self.metrics = None  # validation/training metrics\n        self.session = None  # HUB session\n        self.task = task  # task type\n        model = str(model).strip()\n\n        # Check if Ultralytics HUB model from https://hub.ultralytics.com\n        if self.is_hub_model(model):\n            # Fetch model from HUB\n            checks.check_requirements(\"hub-sdk>=0.0.6\")\n            self.session = self._get_hub_session(model)\n            model = self.session.model_file\n\n        # Check if Triton Server model\n        elif self.is_triton_model(model):\n            self.model_name = self.model = model\n            self.task = task\n            return\n\n        # Load or create new YOLO model\n        if Path(model).suffix in (\".yaml\", \".yml\"):\n            self._new(model, task=task, verbose=verbose)\n        else:\n            self._load(model, task=task)\n\n    def __call__(\n        self,\n        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,\n        stream: bool = False,\n        **kwargs,\n    ) -> list:\n        \n        return self.predict(source, stream, **kwargs)\n\n    @staticmethod\n    def _get_hub_session(model: str):\n        \"\"\"Creates a session for Hub Training.\"\"\"\n        from ultralytics.hub.session import HUBTrainingSession\n\n        session = HUBTrainingSession(model)\n        return session if session.client.authenticated else None\n\n    @staticmethod\n    def is_triton_model(model: str) -> bool:\n        \"\"\"Is model a Triton Server URL string, i.e. <scheme>://<netloc>/<endpoint>/<task_name>\"\"\"\n        from urllib.parse import urlsplit\n\n        url = urlsplit(model)\n        return url.netloc and url.path and url.scheme in {\"http\", \"grpc\"}\n\n    @staticmethod\n    def is_hub_model(model: str) -> bool:\n        \"\"\"Check if the provided model is a HUB model.\"\"\"\n        return any(\n            (\n                model.startswith(f\"{HUB_WEB_ROOT}/models/\"),  # i.e. https://hub.ultralytics.com/models/MODEL_ID\n                [len(x) for x in model.split(\"_\")] == [42, 20],  # APIKEY_MODEL\n                len(model) == 20 and not Path(model).exists() and all(x not in model for x in \"./\\\\\"),  # MODEL\n            )\n        )\n\n    def _new(self, cfg: str, task=None, model=None, verbose=False) -> None:\n        \n        cfg_dict = yaml_model_load(cfg)\n        self.cfg = cfg\n        self.task = task or guess_model_task(cfg_dict)\n        self.model = (model or self._smart_load(\"model\"))(cfg_dict, verbose=verbose and RANK == -1)  # build model\n        self.overrides[\"model\"] = self.cfg\n        self.overrides[\"task\"] = self.task\n\n        # Below added to allow export from YAMLs\n        self.model.args = {**DEFAULT_CFG_DICT, **self.overrides}  # combine default and model args (prefer model args)\n        self.model.task = self.task\n        self.model_name = cfg\n\n    def _load(self, weights: str, task=None) -> None:\n        \n        if weights.lower().startswith((\"https://\", \"http://\", \"rtsp://\", \"rtmp://\", \"tcp://\")):\n            weights = checks.check_file(weights)  # automatically download and return local filename\n        weights = checks.check_model_file_from_stem(weights)  # add suffix, i.e. yolov8n -> yolov8n.pt\n\n        if Path(weights).suffix == \".pt\":\n            self.model, self.ckpt = attempt_load_one_weight(weights)\n            self.task = self.model.args[\"task\"]\n            self.overrides = self.model.args = self._reset_ckpt_args(self.model.args)\n            self.ckpt_path = self.model.pt_path\n        else:\n            weights = checks.check_file(weights)  # runs in all cases, not redundant with above call\n            self.model, self.ckpt = weights, None\n            self.task = task or guess_model_task(weights)\n            self.ckpt_path = weights\n        self.overrides[\"model\"] = weights\n        self.overrides[\"task\"] = self.task\n        self.model_name = weights\n\n    def _check_is_pytorch_model(self) -> None:\n        \"\"\"Raises TypeError is model is not a PyTorch model.\"\"\"\n        pt_str = isinstance(self.model, (str, Path)) and Path(self.model).suffix == \".pt\"\n        pt_module = isinstance(self.model, nn.Module)\n        if not (pt_module or pt_str):\n            raise TypeError(\n                f\"model='{self.model}' should be a *.pt PyTorch model to run this method, but is a different format. \"\n                f\"PyTorch models can train, val, predict and export, i.e. 'model.train(data=...)', but exported \"\n                f\"formats like ONNX, TensorRT etc. only support 'predict' and 'val' modes, \"\n                f\"i.e. 'yolo predict model=yolov8n.onnx'.\\nTo run CUDA or MPS inference please pass the device \"\n                f\"argument directly in your inference command, i.e. 'model.predict(source=..., device=0)'\"\n            )\n\n    def reset_weights(self) -> \"Model\":\n        \n        self._check_is_pytorch_model()\n        for m in self.model.modules():\n            if hasattr(m, \"reset_parameters\"):\n                m.reset_parameters()\n        for p in self.model.parameters():\n            p.requires_grad = True\n        return self\n\n    def load(self, weights: Union[str, Path] = \"yolov8n.pt\") -> \"Model\":\n        \n        self._check_is_pytorch_model()\n        if isinstance(weights, (str, Path)):\n            weights, self.ckpt = attempt_load_one_weight(weights)\n        self.model.load(weights)\n        return self\n\n    def save(self, filename: Union[str, Path] = \"saved_model.pt\", use_dill=True) -> None:\n        \n        self._check_is_pytorch_model()\n        from ultralytics import __version__\n        from datetime import datetime\n\n        updates = {\n            \"date\": datetime.now().isoformat(),\n            \"version\": __version__,\n            \"license\": \"AGPL-3.0 License (https://ultralytics.com/license)\",\n            \"docs\": \"https://docs.ultralytics.com\",\n        }\n        torch.save({**self.ckpt, **updates}, filename, use_dill=use_dill)\n\n    def info(self, detailed: bool = False, verbose: bool = True):\n        \n        self._check_is_pytorch_model()\n        return self.model.info(detailed=detailed, verbose=verbose)\n\n    def fuse(self):\n        \n        self._check_is_pytorch_model()\n        self.model.fuse()\n\n    def embed(\n        self,\n        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,\n        stream: bool = False,\n        **kwargs,\n    ) -> list:\n        \n        if not kwargs.get(\"embed\"):\n            kwargs[\"embed\"] = [len(self.model.model) - 2]  # embed second-to-last layer if no indices passed\n        return self.predict(source, stream, **kwargs)\n\n    def predict(\n        self,\n        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,\n        stream: bool = False,\n        predictor=None,\n        **kwargs,\n    ) -> list:\n        \n        if source is None:\n            source = ASSETS\n            LOGGER.warning(f\"WARNING ⚠️ 'source' is missing. Using 'source={source}'.\")\n\n        is_cli = (sys.argv[0].endswith(\"yolo\") or sys.argv[0].endswith(\"ultralytics\")) and any(\n            x in sys.argv for x in (\"predict\", \"track\", \"mode=predict\", \"mode=track\")\n        )\n\n        custom = {\"conf\": 0.25, \"batch\": 1, \"save\": is_cli, \"mode\": \"predict\"}  # method defaults\n        args = {**self.overrides, **custom, **kwargs}  # highest priority args on the right\n        prompts = args.pop(\"prompts\", None)  # for SAM-type models\n\n        if not self.predictor:\n            self.predictor = predictor or self._smart_load(\"predictor\")(overrides=args, _callbacks=self.callbacks)\n            self.predictor.setup_model(model=self.model, verbose=is_cli)\n        else:  # only update args if predictor is already setup\n            self.predictor.args = get_cfg(self.predictor.args, args)\n            if \"project\" in args or \"name\" in args:\n                self.predictor.save_dir = get_save_dir(self.predictor.args)\n        if prompts and hasattr(self.predictor, \"set_prompts\"):  # for SAM-type models\n            self.predictor.set_prompts(prompts)\n        return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n\n    def track(\n        self,\n        source: Union[str, Path, int, list, tuple, np.ndarray, torch.Tensor] = None,\n        stream: bool = False,\n        persist: bool = False,\n        **kwargs,\n    ) -> list:\n        \n        if not hasattr(self.predictor, \"trackers\"):\n            from ultralytics.trackers import register_tracker\n\n            register_tracker(self, persist)\n        kwargs[\"conf\"] = kwargs.get(\"conf\") or 0.1  # ByteTrack-based method needs low confidence predictions as input\n        kwargs[\"batch\"] = kwargs.get(\"batch\") or 1  # batch-size 1 for tracking in videos\n        kwargs[\"mode\"] = \"track\"\n        return self.predict(source=source, stream=stream, **kwargs)\n\n    def val(\n        self,\n        validator=None,\n        **kwargs,\n    ):\n        \n        custom = {\"rect\": True}  # method defaults\n        args = {**self.overrides, **custom, **kwargs, \"mode\": \"val\"}  # highest priority args on the right\n\n        validator = (validator or self._smart_load(\"validator\"))(args=args, _callbacks=self.callbacks)\n        validator(model=self.model)\n        self.metrics = validator.metrics\n        return validator.metrics\n\n    def benchmark(\n        self,\n        **kwargs,\n    ):\n        \n        self._check_is_pytorch_model()\n        from ultralytics.utils.benchmarks import benchmark\n\n        custom = {\"verbose\": False}  # method defaults\n        args = {**DEFAULT_CFG_DICT, **self.model.args, **custom, **kwargs, \"mode\": \"benchmark\"}\n        return benchmark(\n            model=self,\n            data=kwargs.get(\"data\"),  # if no 'data' argument passed set data=None for default datasets\n            imgsz=args[\"imgsz\"],\n            half=args[\"half\"],\n            int8=args[\"int8\"],\n            device=args[\"device\"],\n            verbose=kwargs.get(\"verbose\"),\n        )\n\n    def export(\n        self,\n        **kwargs,\n    ):\n        \n        self._check_is_pytorch_model()\n        from .exporter import Exporter\n\n        custom = {\"imgsz\": self.model.args[\"imgsz\"], \"batch\": 1, \"data\": None, \"verbose\": False}  # method defaults\n        args = {**self.overrides, **custom, **kwargs, \"mode\": \"export\"}  # highest priority args on the right\n        return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n\n    def train(\n        self,\n        trainer=None,\n        **kwargs,\n    ):\n        \n        self._check_is_pytorch_model()\n        if hasattr(self.session, \"model\") and self.session.model.id:  # Ultralytics HUB session with loaded model\n            if any(kwargs):\n                LOGGER.warning(\"WARNING ⚠️ using HUB training arguments, ignoring local training arguments.\")\n            kwargs = self.session.train_args  # overwrite kwargs\n\n        checks.check_pip_update_available()\n\n        overrides = yaml_load(checks.check_yaml(kwargs[\"cfg\"])) if kwargs.get(\"cfg\") else self.overrides\n        custom = {\"data\": DEFAULT_CFG_DICT[\"data\"] or TASK2DATA[self.task]}  # method defaults\n        args = {**overrides, **custom, **kwargs, \"mode\": \"train\"}  # highest priority args on the right\n        if args.get(\"resume\"):\n            args[\"resume\"] = self.ckpt_path\n\n        self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n        if not args.get(\"resume\"):  # manually set model only if not resuming\n            self.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)\n            self.model = self.trainer.model\n\n            if SETTINGS[\"hub\"] is True and not self.session:\n                # Create a model in HUB\n                try:\n                    self.session = self._get_hub_session(self.model_name)\n                    if self.session:\n                        self.session.create_model(args)\n                        # Check model was created\n                        if not getattr(self.session.model, \"id\", None):\n                            self.session = None\n                except (PermissionError, ModuleNotFoundError):\n                    # Ignore PermissionError and ModuleNotFoundError which indicates hub-sdk not installed\n                    pass\n\n        self.trainer.hub_session = self.session  # attach optional HUB session\n        self.trainer.train()\n        # Update model and cfg after training\n        if RANK in (-1, 0):\n            ckpt = self.trainer.best if self.trainer.best.exists() else self.trainer.last\n            self.model, _ = attempt_load_one_weight(ckpt)\n            self.overrides = self.model.args\n            self.metrics = getattr(self.trainer.validator, \"metrics\", None)  # TODO: no metrics returned by DDP\n        return self.metrics\n\n    def tune(\n        self,\n        use_ray=False,\n        iterations=10,\n        *args,\n        **kwargs,\n    ):\n        \n        self._check_is_pytorch_model()\n        if use_ray:\n            from ultralytics.utils.tuner import run_ray_tune\n\n            return run_ray_tune(self, max_samples=iterations, *args, **kwargs)\n        else:\n            from .tuner import Tuner\n\n            custom = {}  # method defaults\n            args = {**self.overrides, **custom, **kwargs, \"mode\": \"train\"}  # highest priority args on the right\n            return Tuner(args=args, _callbacks=self.callbacks)(model=self, iterations=iterations)\n\n    def _apply(self, fn) -> \"Model\":\n        \"\"\"Apply to(), cpu(), cuda(), half(), float() to model tensors that are not parameters or registered buffers.\"\"\"\n        self._check_is_pytorch_model()\n        self = super()._apply(fn)  # noqa\n        self.predictor = None  # reset predictor as device may have changed\n        self.overrides[\"device\"] = self.device  # was str(self.device) i.e. device(type='cuda', index=0) -> 'cuda:0'\n        return self\n\n    @property\n    def names(self) -> list:\n        \"\"\"\n        Retrieves the class names associated with the loaded model.\n\n        This property returns the class names if they are defined in the model. It checks the class names for validity\n        using the 'check_class_names' function from the ultralytics.nn.autobackend module.\n\n        Returns:\n            (list | None): The class names of the model if available, otherwise None.\n        \"\"\"\n        from ultralytics.nn.autobackend import check_class_names\n\n        return check_class_names(self.model.names) if hasattr(self.model, \"names\") else None\n\n    @property\n    def device(self) -> torch.device:\n        \"\"\"\n        Retrieves the device on which the model's parameters are allocated.\n\n        This property is used to determine whether the model's parameters are on CPU or GPU. It only applies to models\n        that are instances of nn.Module.\n\n        Returns:\n            (torch.device | None): The device (CPU/GPU) of the model if it is a PyTorch model, otherwise None.\n        \"\"\"\n        return next(self.model.parameters()).device if isinstance(self.model, nn.Module) else None\n\n    @property\n    def transforms(self):\n        \"\"\"\n        Retrieves the transformations applied to the input data of the loaded model.\n\n        This property returns the transformations if they are defined in the model.\n\n        Returns:\n            (object | None): The transform object of the model if available, otherwise None.\n        \"\"\"\n        return self.model.transforms if hasattr(self.model, \"transforms\") else None\n\n    def add_callback(self, event: str, func) -> None:\n        \"\"\"\n        Adds a callback function for a specified event.\n\n        This method allows the user to register a custom callback function that is triggered on a specific event during\n        model training or inference.\n\n        Args:\n            event (str): The name of the event to attach the callback to.\n            func (callable): The callback function to be registered.\n\n        Raises:\n            ValueError: If the event name is not recognized.\n        \"\"\"\n        self.callbacks[event].append(func)\n\n    def clear_callback(self, event: str) -> None:\n        \"\"\"\n        Clears all callback functions registered for a specified event.\n\n        This method removes all custom and default callback functions associated with the given event.\n\n        Args:\n            event (str): The name of the event for which to clear the callbacks.\n\n        Raises:\n            ValueError: If the event name is not recognized.\n        \"\"\"\n        self.callbacks[event] = []\n\n    def reset_callbacks(self) -> None:\n        \"\"\"\n        Resets all callbacks to their default functions.\n\n        This method reinstates the default callback functions for all events, removing any custom callbacks that were\n        added previously.\n        \"\"\"\n        for event in callbacks.default_callbacks.keys():\n            self.callbacks[event] = [callbacks.default_callbacks[event][0]]\n\n    @staticmethod\n    def _reset_ckpt_args(args: dict) -> dict:\n        \"\"\"Reset arguments when loading a PyTorch model.\"\"\"\n        include = {\"imgsz\", \"data\", \"task\", \"single_cls\"}  # only remember these arguments when loading a PyTorch model\n        return {k: v for k, v in args.items() if k in include}\n\n    # def __getattr__(self, attr):\n    #    \"\"\"Raises error if object has no requested attribute.\"\"\"\n    #    name = self.__class__.__name__\n    #    raise AttributeError(f\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\")\n\n    def _smart_load(self, key: str):\n        \"\"\"Load model/trainer/validator/predictor.\"\"\"\n        try:\n            return self.task_map[self.task][key]\n        except Exception as e:\n            name = self.__class__.__name__\n            mode = inspect.stack()[1][3]  # get the function name.\n            raise NotImplementedError(\n                emojis(f\"WARNING ⚠️ '{name}' model does not support '{mode}' mode for '{self.task}' task yet.\")\n            ) from e\n\n    @property\n    def task_map(self) -> dict:\n        \"\"\"\n        Map head to model, trainer, validator, and predictor classes.\n\n        Returns:\n            task_map (dict): The map of model task to mode classes.\n        \"\"\"\n        raise NotImplementedError(\"Please provide task map for your model!\")\n\n\n\nclass SAM(Model):\n    \"\"\"\n    SAM (Segment Anything Model) interface class.\n\n    SAM is designed for promptable real-time image segmentation. It can be used with a variety of prompts such as\n    bounding boxes, points, or labels. The model has capabilities for zero-shot performance and is trained on the SA-1B\n    dataset.\n    \"\"\"\n\n    def __init__(self, model=\"sam_b.pt\") -> None:\n        \"\"\"\n        Initializes the SAM model with a pre-trained model file.\n\n        Args:\n            model (str): Path to the pre-trained SAM model file. File should have a .pt or .pth extension.\n\n        Raises:\n            NotImplementedError: If the model file extension is not .pt or .pth.\n        \"\"\"\n        if model and Path(model).suffix not in (\".pt\", \".pth\"):\n            raise NotImplementedError(\"SAM prediction requires pre-trained *.pt or *.pth model.\")\n        super().__init__(model=model, task=\"segment\")\n\n    def _load(self, weights: str, task=None):\n        \"\"\"\n        Loads the specified weights into the SAM model.\n\n        Args:\n            weights (str): Path to the weights file.\n            task (str, optional): Task name. Defaults to None.\n        \"\"\"\n        self.model = build_sam(weights)\n\n    def predict(self, source, stream=False, bboxes=None, points=None, labels=None, **kwargs):\n        \"\"\"\n        Performs segmentation prediction on the given image or video source.\n\n        Args:\n            source (str): Path to the image or video file, or a PIL.Image object, or a numpy.ndarray object.\n            stream (bool, optional): If True, enables real-time streaming. Defaults to False.\n            bboxes (list, optional): List of bounding box coordinates for prompted segmentation. Defaults to None.\n            points (list, optional): List of points for prompted segmentation. Defaults to None.\n            labels (list, optional): List of labels for prompted segmentation. Defaults to None.\n\n        Returns:\n            (list): The model predictions.\n        \"\"\"\n        overrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", imgsz=1024)\n        kwargs.update(overrides)\n        prompts = dict(bboxes=bboxes, points=points, labels=labels)\n        return super().predict(source, stream, prompts=prompts, **kwargs)\n\n    def __call__(self, source=None, stream=False, bboxes=None, points=None, labels=None, **kwargs):\n        \"\"\"\n        Alias for the 'predict' method.\n\n        Args:\n            source (str): Path to the image or video file, or a PIL.Image object, or a numpy.ndarray object.\n            stream (bool, optional): If True, enables real-time streaming. Defaults to False.\n            bboxes (list, optional): List of bounding box coordinates for prompted segmentation. Defaults to None.\n            points (list, optional): List of points for prompted segmentation. Defaults to None.\n            labels (list, optional): List of labels for prompted segmentation. Defaults to None.\n\n        Returns:\n            (list): The model predictions.\n        \"\"\"\n        return self.predict(source, stream, bboxes, points, labels, **kwargs)\n\n    def info(self, detailed=False, verbose=True):\n        \"\"\"\n        Logs information about the SAM model.\n\n        Args:\n            detailed (bool, optional): If True, displays detailed information about the model. Defaults to False.\n            verbose (bool, optional): If True, displays information on the console. Defaults to True.\n\n        Returns:\n            (tuple): A tuple containing the model's information.\n        \"\"\"\n        return model_info(self.model, detailed=detailed, verbose=verbose)\n\n    @property\n    def task_map(self):\n        \"\"\"\n        Provides a mapping from the 'segment' task to its corresponding 'Predictor'.\n\n        Returns:\n            (dict): A dictionary mapping the 'segment' task to its corresponding 'Predictor'.\n        \"\"\"\n        return {\"segment\": {\"predictor\": Predictor}}\n",
            "Examples": [
                "from ultralytics import SAM\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\ndef test_sam_image_generation():\n    # 初始化 SAM 模型\n    sam = SAM(model=\"sam_b.pt\")\n\n    # 定义测试图片源，你可以根据实际情况修改图片路径\n    source = \"testimg/test.jpg\"\n\n    try:\n        # 读取图像\n        img = Image.open(source)\n        img_np = np.array(img)\n\n        # 进行预测\n        results = sam.predict(source=img_np)\n        \n        print(results)\n        \n\n    except Exception as e:\n        print(f\"测试过程中出现错误: {e}\")\n\nif __name__ == \"__main__\":\n    test_sam_image_generation()\n"
            ]
        }
    ]
}