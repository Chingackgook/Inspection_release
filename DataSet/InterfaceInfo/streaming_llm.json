{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/streaming-llm",
    "API_Calls": [
        {
            "Name": "call_greedy_generate",
            "Description": "call_greedy_generate",
            "Code": "import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport argparse\nimport json\nimport os\nimport time\nimport re\nimport sys\n\nfrom tqdm import tqdm\nfrom streaming_llm.utils import load, download_url, load_jsonl\nfrom streaming_llm.enable_streaming_llm import enable_streaming_llm\n\n@torch.no_grad()\ndef streaming_inference(model, tokenizer, prompts, kv_cache=None, max_gen_len=1000):\n    past_key_values = None\n    for idx, prompt in enumerate(prompts):\n        prompt = \"USER: \" + prompt + \"\\n\\nASSISTANT: \"\n        print(\"\\n\" + prompt, end=\"\")\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n        input_ids = input_ids.to(model.device)\n        seq_len = input_ids.shape[1]\n        if kv_cache is not None:\n            space_needed = seq_len + max_gen_len\n            past_key_values = kv_cache.evict_for_space(past_key_values, space_needed)\n\n        past_key_values = greedy_generate(\n            model, tokenizer, input_ids, past_key_values, max_gen_len=max_gen_len\n        )\n\n\ndef main(args):\n    model_name_or_path = args.model_name_or_path\n    model, tokenizer = load(model_name_or_path)\n    test_filepath = os.path.join(args.data_root, \"mt_bench.jsonl\")\n    print(f\"Loading data from {test_filepath} ...\")\n\n    if not os.path.exists(test_filepath):\n        download_url(\n            \"https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/llm_judge/data/mt_bench/question.jsonl\",\n            args.data_root,\n        )\n        os.rename(os.path.join(args.data_root, \"question.jsonl\"), test_filepath)\n\n    list_data = load_jsonl(test_filepath)\n    prompts = []\n    for sample in list_data:\n        prompts += sample[\"turns\"]\n\n    if args.enable_streaming:\n        kv_cache = enable_streaming_llm(\n            model, start_size=args.start_size, recent_size=args.recent_size\n        )\n    else:\n        kv_cache = None\n\n    streaming_inference(\n        model,\n        tokenizer,\n        prompts,\n        kv_cache,\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_name_or_path\", type=str, default=\"lmsys/vicuna-7b-v1.5\"\n    )\n    parser.add_argument(\"--data_root\", type=str, default=\"data/\")\n    parser.add_argument(\"--enable_streaming\", action=\"store_true\")\n    parser.add_argument(\"--start_size\", type=int, default=4)\n    parser.add_argument(\"--recent_size\", type=int, default=2000)\n    args = parser.parse_args()\n\n    main(args)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/streaming-llm/examples/run_streaming_llama.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "greedy_generate",
            "Description": "greedy_generate",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/streaming-llm/examples/run_streaming_llama.py",
            "Implementation": "import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport argparse\nimport json\nimport os\nimport time\nimport re\nimport sys\n\nfrom tqdm import tqdm\nfrom streaming_llm.utils import load, download_url, load_jsonl\nfrom streaming_llm.enable_streaming_llm import enable_streaming_llm\n\n\n@torch.no_grad()\ndef greedy_generate(model, tokenizer, input_ids, past_key_values, max_gen_len):\n    outputs = model(\n        input_ids=input_ids,\n        past_key_values=past_key_values,\n        use_cache=True,\n    )\n    past_key_values = outputs.past_key_values\n    pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n    generated_ids = [pred_token_idx.item()]\n    pos = 0\n    for _ in range(max_gen_len - 1):\n        outputs = model(\n            input_ids=pred_token_idx,\n            past_key_values=past_key_values,\n            use_cache=True,\n        )\n        past_key_values = outputs.past_key_values\n        pred_token_idx = outputs.logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n        generated_ids.append(pred_token_idx.item())\n        generated_text = (\n            tokenizer.decode(\n                generated_ids,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=True,\n                spaces_between_special_tokens=False,\n            )\n            .strip()\n            .split(\" \")\n        )\n\n        now = len(generated_text) - 1\n        if now > pos:\n            print(\" \".join(generated_text[pos:now]), end=\" \", flush=True)\n            pos = now\n\n        if pred_token_idx == tokenizer.eos_token_id:\n            break\n    print(\" \".join(generated_text[pos:]), flush=True)\n    return past_key_values\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}