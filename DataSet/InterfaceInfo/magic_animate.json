{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/magic-animate",
    "API_Calls": [
        {
            "Name": "call_MagicAnimate",
            "Description": "gradio call MagicAnimate",
            "Code": "# Copyright 2023 ByteDance and/or its affiliates.\n#\n# Copyright (2023) MagicAnimate Authors\n#\n# ByteDance, its affiliates and licensors retain all intellectual\n# property and proprietary rights in and to this material, related\n# documentation and any modifications thereto. Any use, reproduction,\n# disclosure or distribution of this material and related documentation\n# without an express license agreement from ByteDance or\n# its affiliates is strictly prohibited.\nimport argparse\nimport imageio\nimport numpy as np\nimport gradio as gr\nfrom PIL import Image\n\nfrom demo.animate import MagicAnimate\n\nanimator = MagicAnimate()\n\ndef animate(reference_image, motion_sequence_state, seed, steps, guidance_scale):\n    return animator(reference_image, motion_sequence_state, seed, steps, guidance_scale)\n\nwith gr.Blocks() as demo:\n\n    gr.HTML(\n        \"\"\"\n        <div style=\"display: flex; justify-content: center; align-items: center; text-align: center;\">\n        <a href=\"https://github.com/magic-research/magic-animate\" style=\"margin-right: 20px; text-decoration: none; display: flex; align-items: center;\">\n        </a>\n        <div>\n            <h1 >MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</h1>\n            <h5 style=\"margin: 0;\">If you like our project, please give us a star âœ¨ on Github for the latest update.</h5>\n            <div style=\"display: flex; justify-content: center; align-items: center; text-align: center;>\n                <a href=\"https://arxiv.org/abs/2311.16498\"><img src=\"https://img.shields.io/badge/Arxiv-2311.16498-red\"></a>\n                <a href='https://showlab.github.io/magicanimate'><img src='https://img.shields.io/badge/Project_Page-MagicAnimate-green' alt='Project Page'></a>\n                <a href='https://github.com/magic-research/magic-animate'><img src='https://img.shields.io/badge/Github-Code-blue'></a>\n            </div>\n        </div>\n        </div>\n        \"\"\")\n    animation = gr.Video(format=\"mp4\", label=\"Animation Results\", autoplay=True)\n    \n    with gr.Row():\n        reference_image  = gr.Image(label=\"Reference Image\")\n        motion_sequence  = gr.Video(format=\"mp4\", label=\"Motion Sequence\")\n        \n        with gr.Column():\n            random_seed         = gr.Textbox(label=\"Random seed\", value=1, info=\"default: -1\")\n            sampling_steps      = gr.Textbox(label=\"Sampling steps\", value=25, info=\"default: 25\")\n            guidance_scale      = gr.Textbox(label=\"Guidance scale\", value=7.5, info=\"default: 7.5\")\n            submit              = gr.Button(\"Animate\")\n\n    def read_video(video):\n        reader = imageio.get_reader(video)\n        fps = reader.get_meta_data()['fps']\n        return video\n    \n    def read_image(image, size=512):\n        return np.array(Image.fromarray(image).resize((size, size)))\n    \n    # when user uploads a new video\n    motion_sequence.upload(\n        read_video,\n        motion_sequence,\n        motion_sequence\n    )\n    # when `first_frame` is updated\n    reference_image.upload(\n        read_image,\n        reference_image,\n        reference_image\n    )\n    # when the `submit` button is clicked\n    submit.click(\n        animate,\n        [reference_image, motion_sequence, random_seed, sampling_steps, guidance_scale], \n        animation\n    )\n\n    # Examples\n    gr.Markdown(\"## Examples\")\n    gr.Examples(\n        examples=[\n            [\"inputs/applications/source_image/monalisa.png\", \"inputs/applications/driving/densepose/running.mp4\"], \n            [\"inputs/applications/source_image/demo4.png\", \"inputs/applications/driving/densepose/demo4.mp4\"],\n            [\"inputs/applications/source_image/dalle2.jpeg\", \"inputs/applications/driving/densepose/running2.mp4\"],\n            [\"inputs/applications/source_image/dalle8.jpeg\", \"inputs/applications/driving/densepose/dancing2.mp4\"],\n            [\"inputs/applications/source_image/multi1_source.png\", \"inputs/applications/driving/densepose/multi_dancing.mp4\"],\n        ],\n        inputs=[reference_image, motion_sequence],\n        outputs=animation,\n    )\n\n\ndemo.launch(share=True)",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/magic-animate/demo/gradio_animate.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "MagicAnimate",
            "Description": "MagicAnimate implements",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/magic-animate/demo/animate.py",
            "Implementation": "# Copyright 2023 ByteDance and/or its affiliates.\n#\n# Copyright (2023) MagicAnimate Authors\n#\n# ByteDance, its affiliates and licensors retain all intellectual\n# property and proprietary rights in and to this material, related\n# documentation and any modifications thereto. Any use, reproduction,\n# disclosure or distribution of this material and related documentation\n# without an express license agreement from ByteDance or\n# its affiliates is strictly prohibited.\nimport argparse\nimport argparse\nimport datetime\nimport inspect\nimport os\nimport numpy as np\nfrom PIL import Image\nfrom omegaconf import OmegaConf\nfrom collections import OrderedDict\n\nimport torch\n\nfrom diffusers import AutoencoderKL, DDIMScheduler, UniPCMultistepScheduler\n\nfrom tqdm import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nfrom magicanimate.models.unet_controlnet import UNet3DConditionModel\nfrom magicanimate.models.controlnet import ControlNetModel\nfrom magicanimate.models.appearance_encoder import AppearanceEncoderModel\nfrom magicanimate.models.mutual_self_attention import ReferenceAttentionControl\nfrom magicanimate.pipelines.pipeline_animation import AnimationPipeline\nfrom magicanimate.utils.util import save_videos_grid\nfrom accelerate.utils import set_seed\n\nfrom magicanimate.utils.videoreader import VideoReader\n\nfrom einops import rearrange, repeat\n\nimport csv, pdb, glob\nfrom safetensors import safe_open\nimport math\nfrom pathlib import Path\n\nclass MagicAnimate():\n    def __init__(self, config=\"configs/prompts/animation.yaml\") -> None:\n        print(\"Initializing MagicAnimate Pipeline...\")\n        *_, func_args = inspect.getargvalues(inspect.currentframe())\n        func_args = dict(func_args)\n        \n        config  = OmegaConf.load(config)\n        \n        inference_config = OmegaConf.load(config.inference_config)\n            \n        motion_module = config.motion_module\n       \n        ### >>> create animation pipeline >>> ###\n        tokenizer = CLIPTokenizer.from_pretrained(config.pretrained_model_path, subfolder=\"tokenizer\")\n        text_encoder = CLIPTextModel.from_pretrained(config.pretrained_model_path, subfolder=\"text_encoder\")\n        if config.pretrained_unet_path:\n            unet = UNet3DConditionModel.from_pretrained_2d(config.pretrained_unet_path, unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs))\n        else:\n            unet = UNet3DConditionModel.from_pretrained_2d(config.pretrained_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs))\n        self.appearance_encoder = AppearanceEncoderModel.from_pretrained(config.pretrained_appearance_encoder_path, subfolder=\"appearance_encoder\").cuda()\n        self.reference_control_writer = ReferenceAttentionControl(self.appearance_encoder, do_classifier_free_guidance=True, mode='write', fusion_blocks=config.fusion_blocks)\n        self.reference_control_reader = ReferenceAttentionControl(unet, do_classifier_free_guidance=True, mode='read', fusion_blocks=config.fusion_blocks)\n        if config.pretrained_vae_path is not None:\n            vae = AutoencoderKL.from_pretrained(config.pretrained_vae_path)\n        else:\n            vae = AutoencoderKL.from_pretrained(config.pretrained_model_path, subfolder=\"vae\")\n\n        ### Load controlnet\n        controlnet   = ControlNetModel.from_pretrained(config.pretrained_controlnet_path)\n\n        vae.to(torch.float16)\n        unet.to(torch.float16)\n        text_encoder.to(torch.float16)\n        controlnet.to(torch.float16)\n        self.appearance_encoder.to(torch.float16)\n        \n        unet.enable_xformers_memory_efficient_attention()\n        self.appearance_encoder.enable_xformers_memory_efficient_attention()\n        controlnet.enable_xformers_memory_efficient_attention()\n\n        self.pipeline = AnimationPipeline(\n            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet, controlnet=controlnet,\n            scheduler=DDIMScheduler(**OmegaConf.to_container(inference_config.noise_scheduler_kwargs)),\n            # NOTE: UniPCMultistepScheduler\n        ).to(\"cuda\")\n\n        # 1. unet ckpt\n        # 1.1 motion module\n        motion_module_state_dict = torch.load(motion_module, map_location=\"cpu\")\n        if \"global_step\" in motion_module_state_dict: func_args.update({\"global_step\": motion_module_state_dict[\"global_step\"]})\n        motion_module_state_dict = motion_module_state_dict['state_dict'] if 'state_dict' in motion_module_state_dict else motion_module_state_dict\n        try:\n            # extra steps for self-trained models\n            state_dict = OrderedDict()\n            for key in motion_module_state_dict.keys():\n                if key.startswith(\"module.\"):\n                    _key = key.split(\"module.\")[-1]\n                    state_dict[_key] = motion_module_state_dict[key]\n                else:\n                    state_dict[key] = motion_module_state_dict[key]\n            motion_module_state_dict = state_dict\n            del state_dict\n            missing, unexpected = self.pipeline.unet.load_state_dict(motion_module_state_dict, strict=False)\n            assert len(unexpected) == 0\n        except:\n            _tmp_ = OrderedDict()\n            for key in motion_module_state_dict.keys():\n                if \"motion_modules\" in key:\n                    if key.startswith(\"unet.\"):\n                        _key = key.split('unet.')[-1]\n                        _tmp_[_key] = motion_module_state_dict[key]\n                    else:\n                        _tmp_[key] = motion_module_state_dict[key]\n            missing, unexpected = unet.load_state_dict(_tmp_, strict=False)\n            assert len(unexpected) == 0\n            del _tmp_\n        del motion_module_state_dict\n\n        self.pipeline.to(\"cuda\")\n        self.L = config.L\n        \n        print(\"Initialization Done!\")\n        \n    def __call__(self, source_image, motion_sequence, random_seed, step, guidance_scale, size=512):\n            prompt = n_prompt = \"\"\n            random_seed = int(random_seed)\n            step = int(step)\n            guidance_scale = float(guidance_scale)\n            samples_per_video = []\n            # manually set random seed for reproduction\n            if random_seed != -1: \n                torch.manual_seed(random_seed)\n                set_seed(random_seed)\n            else:\n                torch.seed()\n\n            if motion_sequence.endswith('.mp4'):\n                control = VideoReader(motion_sequence).read()\n                if control[0].shape[0] != size:\n                    control = [np.array(Image.fromarray(c).resize((size, size))) for c in control]\n                control = np.array(control)\n            \n            if source_image.shape[0] != size:\n                source_image = np.array(Image.fromarray(source_image).resize((size, size)))\n            H, W, C = source_image.shape\n            \n            init_latents = None\n            original_length = control.shape[0]\n            if control.shape[0] % self.L > 0:\n                control = np.pad(control, ((0, self.L-control.shape[0] % self.L), (0, 0), (0, 0), (0, 0)), mode='edge')\n            generator = torch.Generator(device=torch.device(\"cuda:0\"))\n            generator.manual_seed(torch.initial_seed())\n            sample = self.pipeline(\n                prompt,\n                negative_prompt         = n_prompt,\n                num_inference_steps     = step,\n                guidance_scale          = guidance_scale,\n                width                   = W,\n                height                  = H,\n                video_length            = len(control),\n                controlnet_condition    = control,\n                init_latents            = init_latents,\n                generator               = generator,\n                appearance_encoder       = self.appearance_encoder, \n                reference_control_writer = self.reference_control_writer,\n                reference_control_reader = self.reference_control_reader,\n                source_image             = source_image,\n            ).videos\n\n            source_images = np.array([source_image] * original_length)\n            source_images = rearrange(torch.from_numpy(source_images), \"t h w c -> 1 c t h w\") / 255.0\n            samples_per_video.append(source_images)\n            \n            control = control / 255.0\n            control = rearrange(control, \"t h w c -> 1 c t h w\")\n            control = torch.from_numpy(control)\n            samples_per_video.append(control[:, :, :original_length])\n\n            samples_per_video.append(sample[:, :, :original_length])\n\n            samples_per_video = torch.cat(samples_per_video)\n\n            time_str = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n            savedir = f\"demo/outputs\"\n            animation_path = f\"{savedir}/{time_str}.mp4\"\n\n            os.makedirs(savedir, exist_ok=True)\n            save_videos_grid(samples_per_video, animation_path)\n            \n            return animation_path\n            ",
            "Examples": [
                "\n"
            ]
        }
    ]
}