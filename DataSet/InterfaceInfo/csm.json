{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/csm",
    "API_Calls": [
        {
            "Name": "generate_call",
            "Description": "This file contains source code for inspection-related functionality, likely involving data processing, analysis, or validation tasks.",
            "Code": "import os\nimport torch\nimport torchaudio\nfrom huggingface_hub import hf_hub_download\nfrom generator import load_csm_1b, Segment\nfrom dataclasses import dataclass\n\n# Disable Triton compilation\nos.environ[\"NO_TORCH_COMPILE\"] = \"1\"\n\n# Default prompts are available at https://hf.co/sesame/csm-1b\nprompt_filepath_conversational_a = hf_hub_download(\n    repo_id=\"sesame/csm-1b\",\n    filename=\"prompts/conversational_a.wav\"\n)\nprompt_filepath_conversational_b = hf_hub_download(\n    repo_id=\"sesame/csm-1b\",\n    filename=\"prompts/conversational_b.wav\"\n)\n\nSPEAKER_PROMPTS = {\n    \"conversational_a\": {\n        \"text\": (\n            \"like revising for an exam I'd have to try and like keep up the momentum because I'd \"\n            \"start really early I'd be like okay I'm gonna start revising now and then like \"\n            \"you're revising for ages and then I just like start losing steam I didn't do that \"\n            \"for the exam we had recently to be fair that was a more of a last minute scenario \"\n            \"but like yeah I'm trying to like yeah I noticed this yesterday that like Mondays I \"\n            \"sort of start the day with this not like a panic but like a\"\n        ),\n        \"audio\": prompt_filepath_conversational_a\n    },\n    \"conversational_b\": {\n        \"text\": (\n            \"like a super Mario level. Like it's very like high detail. And like, once you get \"\n            \"into the park, it just like, everything looks like a computer game and they have all \"\n            \"these, like, you know, if, if there's like a, you know, like in a Mario game, they \"\n            \"will have like a question block. And if you like, you know, punch it, a coin will \"\n            \"come out. So like everyone, when they come into the park, they get like this little \"\n            \"bracelet and then you can go punching question blocks around.\"\n        ),\n        \"audio\": prompt_filepath_conversational_b\n    }\n}\n\ndef load_prompt_audio(audio_path: str, target_sample_rate: int) -> torch.Tensor:\n    audio_tensor, sample_rate = torchaudio.load(audio_path)\n    audio_tensor = audio_tensor.squeeze(0)\n    # Resample is lazy so we can always call it\n    audio_tensor = torchaudio.functional.resample(\n        audio_tensor, orig_freq=sample_rate, new_freq=target_sample_rate\n    )\n    return audio_tensor\n\ndef prepare_prompt(text: str, speaker: int, audio_path: str, sample_rate: int) -> Segment:\n    audio_tensor = load_prompt_audio(audio_path, sample_rate)\n    return Segment(text=text, speaker=speaker, audio=audio_tensor)\n\ndef main():\n    # Select the best available device, skipping MPS due to float64 limitations\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using device: {device}\")\n\n    # Load model\n    generator = load_csm_1b(device)\n\n    # Prepare prompts\n    prompt_a = prepare_prompt(\n        SPEAKER_PROMPTS[\"conversational_a\"][\"text\"],\n        0,\n        SPEAKER_PROMPTS[\"conversational_a\"][\"audio\"],\n        generator.sample_rate\n    )\n\n    prompt_b = prepare_prompt(\n        SPEAKER_PROMPTS[\"conversational_b\"][\"text\"],\n        1,\n        SPEAKER_PROMPTS[\"conversational_b\"][\"audio\"],\n        generator.sample_rate\n    )\n\n    # Generate conversation\n    conversation = [\n        {\"text\": \"Hey how are you doing?\", \"speaker_id\": 0},\n        {\"text\": \"Pretty good, pretty good. How about you?\", \"speaker_id\": 1},\n        {\"text\": \"I'm great! So happy to be speaking with you today.\", \"speaker_id\": 0},\n        {\"text\": \"Me too! This is some cool stuff, isn't it?\", \"speaker_id\": 1}\n    ]\n\n    # Generate each utterance\n    generated_segments = []\n    prompt_segments = [prompt_a, prompt_b]\n\n    for utterance in conversation:\n        print(f\"Generating: {utterance['text']}\")\n        audio_tensor = generator.generate(\n            text=utterance['text'],\n            speaker=utterance['speaker_id'],\n            context=prompt_segments + generated_segments,\n            max_audio_length_ms=10_000,\n        )\n        generated_segments.append(Segment(text=utterance['text'], speaker=utterance['speaker_id'], audio=audio_tensor))\n\n    # Concatenate all generations\n    all_audio = torch.cat([seg.audio for seg in generated_segments], dim=0)\n    torchaudio.save(\n        \"full_conversation.wav\",\n        all_audio.unsqueeze(0).cpu(),\n        generator.sample_rate\n    )\n    print(\"Successfully generated full_conversation.wav\")\n\nif __name__ == \"__main__\":\n    main() ",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/csm/run_csm.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "Generator",
            "Description": "a generator class defined in the `csm` module.",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/csm/generator.py",
            "Implementation": "from dataclasses import dataclass\nfrom typing import List, Tuple\n\nimport torch\nimport torchaudio\nfrom huggingface_hub import hf_hub_download\nfrom models import Model\nfrom moshi.models import loaders\nfrom tokenizers.processors import TemplateProcessing\nfrom transformers import AutoTokenizer\nfrom watermarking import CSM_1B_GH_WATERMARK, load_watermarker, watermark\n\n\ndef load_llama3_tokenizer():\n    \"\"\"\n    https://github.com/huggingface/transformers/issues/22794#issuecomment-2092623992\n    \"\"\"\n    tokenizer_name = \"meta-llama/Llama-3.2-1B\"\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    bos = tokenizer.bos_token\n    eos = tokenizer.eos_token\n    tokenizer._tokenizer.post_processor = TemplateProcessing(\n        single=f\"{bos}:0 $A:0 {eos}:0\",\n        pair=f\"{bos}:0 $A:0 {eos}:0 {bos}:1 $B:1 {eos}:1\",\n        special_tokens=[(f\"{bos}\", tokenizer.bos_token_id), (f\"{eos}\", tokenizer.eos_token_id)],\n    )\n\n    return tokenizer\n\n\nclass Generator:\n    def __init__(\n        self,\n        model: Model,\n    ):\n        self._model = model\n        self._model.setup_caches(1)\n\n        self._text_tokenizer = load_llama3_tokenizer()\n\n        device = next(model.parameters()).device\n        mimi_weight = hf_hub_download(loaders.DEFAULT_REPO, loaders.MIMI_NAME)\n        mimi = loaders.get_mimi(mimi_weight, device=device)\n        mimi.set_num_codebooks(32)\n        self._audio_tokenizer = mimi\n\n        self._watermarker = load_watermarker(device=device)\n\n        self.sample_rate = mimi.sample_rate\n        self.device = device\n\n    def _tokenize_text_segment(self, text: str, speaker: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        frame_tokens = []\n        frame_masks = []\n\n        text_tokens = self._text_tokenizer.encode(f\"[{speaker}]{text}\")\n        text_frame = torch.zeros(len(text_tokens), 33).long()\n        text_frame_mask = torch.zeros(len(text_tokens), 33).bool()\n        text_frame[:, -1] = torch.tensor(text_tokens)\n        text_frame_mask[:, -1] = True\n\n        frame_tokens.append(text_frame.to(self.device))\n        frame_masks.append(text_frame_mask.to(self.device))\n\n        return torch.cat(frame_tokens, dim=0), torch.cat(frame_masks, dim=0)\n\n    def _tokenize_audio(self, audio: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        assert audio.ndim == 1, \"Audio must be single channel\"\n\n        frame_tokens = []\n        frame_masks = []\n\n        # (K, T)\n        audio = audio.to(self.device)\n        audio_tokens = self._audio_tokenizer.encode(audio.unsqueeze(0).unsqueeze(0))[0]\n        # add EOS frame\n        eos_frame = torch.zeros(audio_tokens.size(0), 1).to(self.device)\n        audio_tokens = torch.cat([audio_tokens, eos_frame], dim=1)\n\n        audio_frame = torch.zeros(audio_tokens.size(1), 33).long().to(self.device)\n        audio_frame_mask = torch.zeros(audio_tokens.size(1), 33).bool().to(self.device)\n        audio_frame[:, :-1] = audio_tokens.transpose(0, 1)\n        audio_frame_mask[:, :-1] = True\n\n        frame_tokens.append(audio_frame)\n        frame_masks.append(audio_frame_mask)\n\n        return torch.cat(frame_tokens, dim=0), torch.cat(frame_masks, dim=0)\n\n    def _tokenize_segment(self, segment: Segment) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Returns:\n            (seq_len, 33), (seq_len, 33)\n        \"\"\"\n        text_tokens, text_masks = self._tokenize_text_segment(segment.text, segment.speaker)\n        audio_tokens, audio_masks = self._tokenize_audio(segment.audio)\n\n        return torch.cat([text_tokens, audio_tokens], dim=0), torch.cat([text_masks, audio_masks], dim=0)\n\n    @torch.inference_mode()\n    def generate(\n        self,\n        text: str,\n        speaker: int,\n        context: List[Segment],\n        max_audio_length_ms: float = 90_000,\n        temperature: float = 0.9,\n        topk: int = 50,\n    ) -> torch.Tensor:\n        self._model.reset_caches()\n\n        max_generation_len = int(max_audio_length_ms / 80)\n        tokens, tokens_mask = [], []\n        for segment in context:\n            segment_tokens, segment_tokens_mask = self._tokenize_segment(segment)\n            tokens.append(segment_tokens)\n            tokens_mask.append(segment_tokens_mask)\n\n        gen_segment_tokens, gen_segment_tokens_mask = self._tokenize_text_segment(text, speaker)\n        tokens.append(gen_segment_tokens)\n        tokens_mask.append(gen_segment_tokens_mask)\n\n        prompt_tokens = torch.cat(tokens, dim=0).long().to(self.device)\n        prompt_tokens_mask = torch.cat(tokens_mask, dim=0).bool().to(self.device)\n\n        samples = []\n        curr_tokens = prompt_tokens.unsqueeze(0)\n        curr_tokens_mask = prompt_tokens_mask.unsqueeze(0)\n        curr_pos = torch.arange(0, prompt_tokens.size(0)).unsqueeze(0).long().to(self.device)\n\n        max_seq_len = 2048\n        max_context_len = max_seq_len - max_generation_len\n        if curr_tokens.size(1) >= max_context_len:\n            raise ValueError(\n                f\"Inputs too long, must be below max_seq_len - max_generation_len: {max_context_len}\"\n            )\n\n        for _ in range(max_generation_len):\n            sample = self._model.generate_frame(curr_tokens, curr_tokens_mask, curr_pos, temperature, topk)\n            if torch.all(sample == 0):\n                break  # eos\n\n            samples.append(sample)\n\n            curr_tokens = torch.cat([sample, torch.zeros(1, 1).long().to(self.device)], dim=1).unsqueeze(1)\n            curr_tokens_mask = torch.cat(\n                [torch.ones_like(sample).bool(), torch.zeros(1, 1).bool().to(self.device)], dim=1\n            ).unsqueeze(1)\n            curr_pos = curr_pos[:, -1:] + 1\n\n        audio = self._audio_tokenizer.decode(torch.stack(samples).permute(1, 2, 0)).squeeze(0).squeeze(0)\n\n        # This applies an imperceptible watermark to identify audio as AI-generated.\n        # Watermarking ensures transparency, dissuades misuse, and enables traceability.\n        # Please be a responsible AI citizen and keep the watermarking in place.\n        # If using CSM 1B in another application, use your own private key and keep it secret.\n        audio, wm_sample_rate = watermark(self._watermarker, audio, self.sample_rate, CSM_1B_GH_WATERMARK)\n        audio = torchaudio.functional.resample(audio, orig_freq=wm_sample_rate, new_freq=self.sample_rate)\n\n        return audio\n\n\ndef load_csm_1b(device: str = \"cuda\") -> Generator:\n    model = Model.from_pretrained(\"sesame/csm-1b\")\n    model.to(device=device, dtype=torch.bfloat16)\n\n    generator = Generator(model)\n    return generator",
            "Examples": [
                "\n"
            ]
        }
    ]
}