{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/F5-TTS/src",
    "API_Calls": [
        {
            "Name": "infer_process_call",
            "Description": "This code is a command-line interface for F5-TTS (Text-to-Speech) model inference with advanced batch processing capabilities. \nusage: \nf5-tts_infer-cli --model F5TTS_v1_Base \\\n--ref_audio \"provide_prompt_wav_path_here.wav\" \\\n--ref_text \"The content, subtitle or transcription of reference audio.\" \\\n--gen_text \"Some text you want TTS model generate for you.\"",
            "Code": "import argparse\nimport codecs\nimport os\nimport re\nfrom datetime import datetime\nfrom importlib.resources import files\nfrom pathlib import Path\n\nimport numpy as np\nimport soundfile as sf\nimport tomli\nfrom cached_path import cached_path\nfrom hydra.utils import get_class\nfrom omegaconf import OmegaConf\n\nfrom f5_tts.infer.utils_infer import (\n    cfg_strength,\n    cross_fade_duration,\n    device,\n    fix_duration,\n    infer_process,\n    load_model,\n    load_vocoder,\n    mel_spec_type,\n    nfe_step,\n    preprocess_ref_audio_text,\n    remove_silence_for_generated_wav,\n    speed,\n    sway_sampling_coef,\n    target_rms,\n)\n\n\nparser = argparse.ArgumentParser(\n    prog=\"python3 infer-cli.py\",\n    description=\"Commandline interface for E2/F5 TTS with Advanced Batch Processing.\",\n    epilog=\"Specify options above to override one or more settings from config.\",\n)\nparser.add_argument(\n    \"-c\",\n    \"--config\",\n    type=str,\n    default=os.path.join(files(\"f5_tts\").joinpath(\"infer/examples/basic\"), \"basic.toml\"),\n    help=\"The configuration file, default see infer/examples/basic/basic.toml\",\n)\n\n\n# Note. Not to provide default value here in order to read default from config file\n\nparser.add_argument(\n    \"-m\",\n    \"--model\",\n    type=str,\n    help=\"The model name: F5TTS_v1_Base | F5TTS_Base | E2TTS_Base | etc.\",\n)\nparser.add_argument(\n    \"-mc\",\n    \"--model_cfg\",\n    type=str,\n    help=\"The path to F5-TTS model config file .yaml\",\n)\nparser.add_argument(\n    \"-p\",\n    \"--ckpt_file\",\n    type=str,\n    help=\"The path to model checkpoint .pt, leave blank to use default\",\n)\nparser.add_argument(\n    \"-v\",\n    \"--vocab_file\",\n    type=str,\n    help=\"The path to vocab file .txt, leave blank to use default\",\n)\nparser.add_argument(\n    \"-r\",\n    \"--ref_audio\",\n    type=str,\n    help=\"The reference audio file.\",\n)\nparser.add_argument(\n    \"-s\",\n    \"--ref_text\",\n    type=str,\n    help=\"The transcript/subtitle for the reference audio\",\n)\nparser.add_argument(\n    \"-t\",\n    \"--gen_text\",\n    type=str,\n    help=\"The text to make model synthesize a speech\",\n)\nparser.add_argument(\n    \"-f\",\n    \"--gen_file\",\n    type=str,\n    help=\"The file with text to generate, will ignore --gen_text\",\n)\nparser.add_argument(\n    \"-o\",\n    \"--output_dir\",\n    type=str,\n    help=\"The path to output folder\",\n)\nparser.add_argument(\n    \"-w\",\n    \"--output_file\",\n    type=str,\n    help=\"The name of output file\",\n)\nparser.add_argument(\n    \"--save_chunk\",\n    action=\"store_true\",\n    help=\"To save each audio chunks during inference\",\n)\nparser.add_argument(\n    \"--remove_silence\",\n    action=\"store_true\",\n    help=\"To remove long silence found in ouput\",\n)\nparser.add_argument(\n    \"--load_vocoder_from_local\",\n    action=\"store_true\",\n    help=\"To load vocoder from local dir, default to ../checkpoints/vocos-mel-24khz\",\n)\nparser.add_argument(\n    \"--vocoder_name\",\n    type=str,\n    choices=[\"vocos\", \"bigvgan\"],\n    help=f\"Used vocoder name: vocos | bigvgan, default {mel_spec_type}\",\n)\nparser.add_argument(\n    \"--target_rms\",\n    type=float,\n    help=f\"Target output speech loudness normalization value, default {target_rms}\",\n)\nparser.add_argument(\n    \"--cross_fade_duration\",\n    type=float,\n    help=f\"Duration of cross-fade between audio segments in seconds, default {cross_fade_duration}\",\n)\nparser.add_argument(\n    \"--nfe_step\",\n    type=int,\n    help=f\"The number of function evaluation (denoising steps), default {nfe_step}\",\n)\nparser.add_argument(\n    \"--cfg_strength\",\n    type=float,\n    help=f\"Classifier-free guidance strength, default {cfg_strength}\",\n)\nparser.add_argument(\n    \"--sway_sampling_coef\",\n    type=float,\n    help=f\"Sway Sampling coefficient, default {sway_sampling_coef}\",\n)\nparser.add_argument(\n    \"--speed\",\n    type=float,\n    help=f\"The speed of the generated audio, default {speed}\",\n)\nparser.add_argument(\n    \"--fix_duration\",\n    type=float,\n    help=f\"Fix the total duration (ref and gen audios) in seconds, default {fix_duration}\",\n)\nparser.add_argument(\n    \"--device\",\n    type=str,\n    help=\"Specify the device to run on\",\n)\nargs = parser.parse_args()\n\n\n# config file\n\nconfig = tomli.load(open(args.config, \"rb\"))\n\n\n# command-line interface parameters\n\nmodel = args.model or config.get(\"model\", \"F5TTS_v1_Base\")\nckpt_file = args.ckpt_file or config.get(\"ckpt_file\", \"\")\nvocab_file = args.vocab_file or config.get(\"vocab_file\", \"\")\n\nref_audio = args.ref_audio or config.get(\"ref_audio\", \"infer/examples/basic/basic_ref_en.wav\")\nref_text = (\n    args.ref_text\n    if args.ref_text is not None\n    else config.get(\"ref_text\", \"Some call me nature, others call me mother nature.\")\n)\ngen_text = args.gen_text or config.get(\"gen_text\", \"Here we generate something just for test.\")\ngen_file = args.gen_file or config.get(\"gen_file\", \"\")\n\noutput_dir = args.output_dir or config.get(\"output_dir\", \"tests\")\noutput_file = args.output_file or config.get(\n    \"output_file\", f\"infer_cli_{datetime.now().strftime(r'%Y%m%d_%H%M%S')}.wav\"\n)\n\nsave_chunk = args.save_chunk or config.get(\"save_chunk\", False)\nremove_silence = args.remove_silence or config.get(\"remove_silence\", False)\nload_vocoder_from_local = args.load_vocoder_from_local or config.get(\"load_vocoder_from_local\", False)\n\nvocoder_name = args.vocoder_name or config.get(\"vocoder_name\", mel_spec_type)\ntarget_rms = args.target_rms or config.get(\"target_rms\", target_rms)\ncross_fade_duration = args.cross_fade_duration or config.get(\"cross_fade_duration\", cross_fade_duration)\nnfe_step = args.nfe_step or config.get(\"nfe_step\", nfe_step)\ncfg_strength = args.cfg_strength or config.get(\"cfg_strength\", cfg_strength)\nsway_sampling_coef = args.sway_sampling_coef or config.get(\"sway_sampling_coef\", sway_sampling_coef)\nspeed = args.speed or config.get(\"speed\", speed)\nfix_duration = args.fix_duration or config.get(\"fix_duration\", fix_duration)\ndevice = args.device or config.get(\"device\", device)\n\n\n# patches for pip pkg user\nif \"infer/examples/\" in ref_audio:\n    ref_audio = str(files(\"f5_tts\").joinpath(f\"{ref_audio}\"))\nif \"infer/examples/\" in gen_file:\n    gen_file = str(files(\"f5_tts\").joinpath(f\"{gen_file}\"))\nif \"voices\" in config:\n    for voice in config[\"voices\"]:\n        voice_ref_audio = config[\"voices\"][voice][\"ref_audio\"]\n        if \"infer/examples/\" in voice_ref_audio:\n            config[\"voices\"][voice][\"ref_audio\"] = str(files(\"f5_tts\").joinpath(f\"{voice_ref_audio}\"))\n\n\n# ignore gen_text if gen_file provided\n\nif gen_file:\n    gen_text = codecs.open(gen_file, \"r\", \"utf-8\").read()\n\n\n# output path\n\nwave_path = Path(output_dir) / output_file\n# spectrogram_path = Path(output_dir) / \"infer_cli_out.png\"\nif save_chunk:\n    output_chunk_dir = os.path.join(output_dir, f\"{Path(output_file).stem}_chunks\")\n    if not os.path.exists(output_chunk_dir):\n        os.makedirs(output_chunk_dir)\n\n\n# load vocoder\n\nif vocoder_name == \"vocos\":\n    vocoder_local_path = \"../checkpoints/vocos-mel-24khz\"\nelif vocoder_name == \"bigvgan\":\n    vocoder_local_path = \"../checkpoints/bigvgan_v2_24khz_100band_256x\"\n\nvocoder = load_vocoder(\n    vocoder_name=vocoder_name, is_local=load_vocoder_from_local, local_path=vocoder_local_path, device=device\n)\n\n\n# load TTS model\n\nmodel_cfg = OmegaConf.load(\n    args.model_cfg or config.get(\"model_cfg\", str(files(\"f5_tts\").joinpath(f\"configs/{model}.yaml\")))\n)\nmodel_cls = get_class(f\"f5_tts.model.{model_cfg.model.backbone}\")\nmodel_arc = model_cfg.model.arch\n\nrepo_name, ckpt_step, ckpt_type = \"F5-TTS\", 1250000, \"safetensors\"\n\nif model != \"F5TTS_Base\":\n    assert vocoder_name == model_cfg.model.mel_spec.mel_spec_type\n\n# override for previous models\nif model == \"F5TTS_Base\":\n    if vocoder_name == \"vocos\":\n        ckpt_step = 1200000\n    elif vocoder_name == \"bigvgan\":\n        model = \"F5TTS_Base_bigvgan\"\n        ckpt_type = \"pt\"\nelif model == \"E2TTS_Base\":\n    repo_name = \"E2-TTS\"\n    ckpt_step = 1200000\n\nif not ckpt_file:\n    ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{model}/model_{ckpt_step}.{ckpt_type}\"))\n\nprint(f\"Using {model}...\")\nema_model = load_model(\n    model_cls, model_arc, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file, device=device\n)\n\n\n# inference process\n\n\ndef main():\n    main_voice = {\"ref_audio\": ref_audio, \"ref_text\": ref_text}\n    if \"voices\" not in config:\n        voices = {\"main\": main_voice}\n    else:\n        voices = config[\"voices\"]\n        voices[\"main\"] = main_voice\n    for voice in voices:\n        print(\"Voice:\", voice)\n        print(\"ref_audio \", voices[voice][\"ref_audio\"])\n        voices[voice][\"ref_audio\"], voices[voice][\"ref_text\"] = preprocess_ref_audio_text(\n            voices[voice][\"ref_audio\"], voices[voice][\"ref_text\"]\n        )\n        print(\"ref_audio_\", voices[voice][\"ref_audio\"], \"\\n\\n\")\n\n    generated_audio_segments = []\n    reg1 = r\"(?=\\[\\w+\\])\"\n    chunks = re.split(reg1, gen_text)\n    reg2 = r\"\\[(\\w+)\\]\"\n    for text in chunks:\n        if not text.strip():\n            continue\n        match = re.match(reg2, text)\n        if match:\n            voice = match[1]\n        else:\n            print(\"No voice tag found, using main.\")\n            voice = \"main\"\n        if voice not in voices:\n            print(f\"Voice {voice} not found, using main.\")\n            voice = \"main\"\n        text = re.sub(reg2, \"\", text)\n        ref_audio_ = voices[voice][\"ref_audio\"]\n        ref_text_ = voices[voice][\"ref_text\"]\n        gen_text_ = text.strip()\n        print(f\"Voice: {voice}\")\n        audio_segment, final_sample_rate, spectrogram = infer_process(\n            ref_audio_,\n            ref_text_,\n            gen_text_,\n            ema_model,\n            vocoder,\n            mel_spec_type=vocoder_name,\n            target_rms=target_rms,\n            cross_fade_duration=cross_fade_duration,\n            nfe_step=nfe_step,\n            cfg_strength=cfg_strength,\n            sway_sampling_coef=sway_sampling_coef,\n            speed=speed,\n            fix_duration=fix_duration,\n            device=device,\n        )\n        generated_audio_segments.append(audio_segment)\n\n        if save_chunk:\n            if len(gen_text_) > 200:\n                gen_text_ = gen_text_[:200] + \" ... \"\n            sf.write(\n                os.path.join(output_chunk_dir, f\"{len(generated_audio_segments) - 1}_{gen_text_}.wav\"),\n                audio_segment,\n                final_sample_rate,\n            )\n\n    if generated_audio_segments:\n        final_wave = np.concatenate(generated_audio_segments)\n\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        with open(wave_path, \"wb\") as f:\n            sf.write(f.name, final_wave, final_sample_rate)\n            # Remove silence\n            if remove_silence:\n                remove_silence_for_generated_wav(f.name)\n            print(f.name)\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/F5-TTS/src/f5_tts/infer/infer_cli.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "infer_process",
            "Description": "inpl of inference process for F5-TTS model, and other related functionalities.",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/F5-TTS/src/f5_tts/infer/utils_infer.py",
            "Implementation": "# A unified script for inference process\n# Make adjustments inside functions, and consider both gradio and cli scripts if need to change func output format\nimport os\nimport sys\nfrom concurrent.futures import ThreadPoolExecutor\n\n\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # for MPS device compatibility\nsys.path.append(f\"{os.path.dirname(os.path.abspath(__file__))}/../../third_party/BigVGAN/\")\n\nimport hashlib\nimport re\nimport tempfile\nfrom importlib.resources import files\n\nimport matplotlib\n\n\nmatplotlib.use(\"Agg\")\n\nimport matplotlib.pylab as plt\nimport numpy as np\nimport torch\nimport torchaudio\nimport tqdm\nfrom huggingface_hub import hf_hub_download\nfrom pydub import AudioSegment, silence\nfrom transformers import pipeline\nfrom vocos import Vocos\n\nfrom f5_tts.model import CFM\nfrom f5_tts.model.utils import convert_char_to_pinyin, get_tokenizer\n\n\n_ref_audio_cache = {}\n_ref_text_cache = {}\n\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"xpu\"\n    if torch.xpu.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\n\ntempfile_kwargs = {\"delete_on_close\": False} if sys.version_info >= (3, 12) else {\"delete\": False}\n\n# -----------------------------------------\n\ntarget_sample_rate = 24000\nn_mel_channels = 100\nhop_length = 256\nwin_length = 1024\nn_fft = 1024\nmel_spec_type = \"vocos\"\ntarget_rms = 0.1\ncross_fade_duration = 0.15\node_method = \"euler\"\nnfe_step = 32  # 16, 32\ncfg_strength = 2.0\nsway_sampling_coef = -1.0\nspeed = 1.0\nfix_duration = None\n\n# -----------------------------------------\n\n\n# chunk text into smaller pieces\n\n\ndef chunk_text(text, max_chars=135):\n    \"\"\"\n    Splits the input text into chunks, each with a maximum number of characters.\n\n    Args:\n        text (str): The text to be split.\n        max_chars (int): The maximum number of characters per chunk.\n\n    Returns:\n        List[str]: A list of text chunks.\n    \"\"\"\n    chunks = []\n    current_chunk = \"\"\n    # Split the text into sentences based on punctuation followed by whitespace\n    sentences = re.split(r\"(?<=[;:,.!?])\\s+|(?<=[；：，。！？])\", text)\n\n    for sentence in sentences:\n        if len(current_chunk.encode(\"utf-8\")) + len(sentence.encode(\"utf-8\")) <= max_chars:\n            current_chunk += sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n        else:\n            if current_chunk:\n                chunks.append(current_chunk.strip())\n            current_chunk = sentence + \" \" if sentence and len(sentence[-1].encode(\"utf-8\")) == 1 else sentence\n\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n\n    return chunks\n\n\n# load vocoder\ndef load_vocoder(vocoder_name=\"vocos\", is_local=False, local_path=\"\", device=device, hf_cache_dir=None):\n    if vocoder_name == \"vocos\":\n        # vocoder = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\").to(device)\n        if is_local:\n            print(f\"Load vocos from local path {local_path}\")\n            config_path = f\"{local_path}/config.yaml\"\n            model_path = f\"{local_path}/pytorch_model.bin\"\n        else:\n            print(\"Download Vocos from huggingface charactr/vocos-mel-24khz\")\n            repo_id = \"charactr/vocos-mel-24khz\"\n            config_path = hf_hub_download(repo_id=repo_id, cache_dir=hf_cache_dir, filename=\"config.yaml\")\n            model_path = hf_hub_download(repo_id=repo_id, cache_dir=hf_cache_dir, filename=\"pytorch_model.bin\")\n        vocoder = Vocos.from_hparams(config_path)\n        state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n        from vocos.feature_extractors import EncodecFeatures\n\n        if isinstance(vocoder.feature_extractor, EncodecFeatures):\n            encodec_parameters = {\n                \"feature_extractor.encodec.\" + key: value\n                for key, value in vocoder.feature_extractor.encodec.state_dict().items()\n            }\n            state_dict.update(encodec_parameters)\n        vocoder.load_state_dict(state_dict)\n        vocoder = vocoder.eval().to(device)\n    elif vocoder_name == \"bigvgan\":\n        try:\n            from third_party.BigVGAN import bigvgan\n        except ImportError:\n            print(\"You need to follow the README to init submodule and change the BigVGAN source code.\")\n        if is_local:\n            # download generator from https://huggingface.co/nvidia/bigvgan_v2_24khz_100band_256x/tree/main\n            vocoder = bigvgan.BigVGAN.from_pretrained(local_path, use_cuda_kernel=False)\n        else:\n            vocoder = bigvgan.BigVGAN.from_pretrained(\n                \"nvidia/bigvgan_v2_24khz_100band_256x\", use_cuda_kernel=False, cache_dir=hf_cache_dir\n            )\n\n        vocoder.remove_weight_norm()\n        vocoder = vocoder.eval().to(device)\n    return vocoder\n\n\n# load asr pipeline\n\nasr_pipe = None\n\n\ndef initialize_asr_pipeline(device: str = device, dtype=None):\n    if dtype is None:\n        dtype = (\n            torch.float16\n            if \"cuda\" in device\n            and torch.cuda.get_device_properties(device).major >= 7\n            and not torch.cuda.get_device_name().endswith(\"[ZLUDA]\")\n            else torch.float32\n        )\n    global asr_pipe\n    asr_pipe = pipeline(\n        \"automatic-speech-recognition\",\n        model=\"openai/whisper-large-v3-turbo\",\n        torch_dtype=dtype,\n        device=device,\n    )\n\n\n# transcribe\n\n\ndef transcribe(ref_audio, language=None):\n    global asr_pipe\n    if asr_pipe is None:\n        initialize_asr_pipeline(device=device)\n    return asr_pipe(\n        ref_audio,\n        chunk_length_s=30,\n        batch_size=128,\n        generate_kwargs={\"task\": \"transcribe\", \"language\": language} if language else {\"task\": \"transcribe\"},\n        return_timestamps=False,\n    )[\"text\"].strip()\n\n\n# load model checkpoint for inference\n\n\ndef load_checkpoint(model, ckpt_path, device: str, dtype=None, use_ema=True):\n    if dtype is None:\n        dtype = (\n            torch.float16\n            if \"cuda\" in device\n            and torch.cuda.get_device_properties(device).major >= 7\n            and not torch.cuda.get_device_name().endswith(\"[ZLUDA]\")\n            else torch.float32\n        )\n    model = model.to(dtype)\n\n    ckpt_type = ckpt_path.split(\".\")[-1]\n    if ckpt_type == \"safetensors\":\n        from safetensors.torch import load_file\n\n        checkpoint = load_file(ckpt_path, device=device)\n    else:\n        checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)\n\n    if use_ema:\n        if ckpt_type == \"safetensors\":\n            checkpoint = {\"ema_model_state_dict\": checkpoint}\n        checkpoint[\"model_state_dict\"] = {\n            k.replace(\"ema_model.\", \"\"): v\n            for k, v in checkpoint[\"ema_model_state_dict\"].items()\n            if k not in [\"initted\", \"step\"]\n        }\n\n        # patch for backward compatibility, 305e3ea\n        for key in [\"mel_spec.mel_stft.mel_scale.fb\", \"mel_spec.mel_stft.spectrogram.window\"]:\n            if key in checkpoint[\"model_state_dict\"]:\n                del checkpoint[\"model_state_dict\"][key]\n\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n    else:\n        if ckpt_type == \"safetensors\":\n            checkpoint = {\"model_state_dict\": checkpoint}\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    del checkpoint\n    torch.cuda.empty_cache()\n\n    return model.to(device)\n\n\n# load model for inference\n\n\ndef load_model(\n    model_cls,\n    model_cfg,\n    ckpt_path,\n    mel_spec_type=mel_spec_type,\n    vocab_file=\"\",\n    ode_method=ode_method,\n    use_ema=True,\n    device=device,\n):\n    if vocab_file == \"\":\n        vocab_file = str(files(\"f5_tts\").joinpath(\"infer/examples/vocab.txt\"))\n    tokenizer = \"custom\"\n\n    print(\"\\nvocab : \", vocab_file)\n    print(\"token : \", tokenizer)\n    print(\"model : \", ckpt_path, \"\\n\")\n\n    vocab_char_map, vocab_size = get_tokenizer(vocab_file, tokenizer)\n    model = CFM(\n        transformer=model_cls(**model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels),\n        mel_spec_kwargs=dict(\n            n_fft=n_fft,\n            hop_length=hop_length,\n            win_length=win_length,\n            n_mel_channels=n_mel_channels,\n            target_sample_rate=target_sample_rate,\n            mel_spec_type=mel_spec_type,\n        ),\n        odeint_kwargs=dict(\n            method=ode_method,\n        ),\n        vocab_char_map=vocab_char_map,\n    ).to(device)\n\n    dtype = torch.float32 if mel_spec_type == \"bigvgan\" else None\n    model = load_checkpoint(model, ckpt_path, device, dtype=dtype, use_ema=use_ema)\n\n    return model\n\n\ndef remove_silence_edges(audio, silence_threshold=-42):\n    # Remove silence from the start\n    non_silent_start_idx = silence.detect_leading_silence(audio, silence_threshold=silence_threshold)\n    audio = audio[non_silent_start_idx:]\n\n    # Remove silence from the end\n    non_silent_end_duration = audio.duration_seconds\n    for ms in reversed(audio):\n        if ms.dBFS > silence_threshold:\n            break\n        non_silent_end_duration -= 0.001\n    trimmed_audio = audio[: int(non_silent_end_duration * 1000)]\n\n    return trimmed_audio\n\n\n# preprocess reference audio and text\n\n\ndef preprocess_ref_audio_text(ref_audio_orig, ref_text, show_info=print):\n    show_info(\"Converting audio...\")\n\n    # Compute a hash of the reference audio file\n    with open(ref_audio_orig, \"rb\") as audio_file:\n        audio_data = audio_file.read()\n        audio_hash = hashlib.md5(audio_data).hexdigest()\n\n    global _ref_audio_cache\n\n    if audio_hash in _ref_audio_cache:\n        show_info(\"Using cached preprocessed reference audio...\")\n        ref_audio = _ref_audio_cache[audio_hash]\n\n    else:  # first pass, do preprocess\n        with tempfile.NamedTemporaryFile(suffix=\".wav\", **tempfile_kwargs) as f:\n            temp_path = f.name\n\n        aseg = AudioSegment.from_file(ref_audio_orig)\n\n        # 1. try to find long silence for clipping\n        non_silent_segs = silence.split_on_silence(\n            aseg, min_silence_len=1000, silence_thresh=-50, keep_silence=1000, seek_step=10\n        )\n        non_silent_wave = AudioSegment.silent(duration=0)\n        for non_silent_seg in non_silent_segs:\n            if len(non_silent_wave) > 6000 and len(non_silent_wave + non_silent_seg) > 12000:\n                show_info(\"Audio is over 12s, clipping short. (1)\")\n                break\n            non_silent_wave += non_silent_seg\n\n        # 2. try to find short silence for clipping if 1. failed\n        if len(non_silent_wave) > 12000:\n            non_silent_segs = silence.split_on_silence(\n                aseg, min_silence_len=100, silence_thresh=-40, keep_silence=1000, seek_step=10\n            )\n            non_silent_wave = AudioSegment.silent(duration=0)\n            for non_silent_seg in non_silent_segs:\n                if len(non_silent_wave) > 6000 and len(non_silent_wave + non_silent_seg) > 12000:\n                    show_info(\"Audio is over 12s, clipping short. (2)\")\n                    break\n                non_silent_wave += non_silent_seg\n\n        aseg = non_silent_wave\n\n        # 3. if no proper silence found for clipping\n        if len(aseg) > 12000:\n            aseg = aseg[:12000]\n            show_info(\"Audio is over 12s, clipping short. (3)\")\n\n        aseg = remove_silence_edges(aseg) + AudioSegment.silent(duration=50)\n        aseg.export(temp_path, format=\"wav\")\n        ref_audio = temp_path\n\n        # Cache the processed reference audio\n        _ref_audio_cache[audio_hash] = ref_audio\n\n    if not ref_text.strip():\n        global _ref_text_cache\n        if audio_hash in _ref_text_cache:\n            # Use cached asr transcription\n            show_info(\"Using cached reference text...\")\n            ref_text = _ref_text_cache[audio_hash]\n        else:\n            show_info(\"No reference text provided, transcribing reference audio...\")\n            ref_text = transcribe(ref_audio)\n            # Cache the transcribed text (not caching custom ref_text, enabling users to do manual tweak)\n            _ref_text_cache[audio_hash] = ref_text\n    else:\n        show_info(\"Using custom reference text...\")\n\n    # Ensure ref_text ends with a proper sentence-ending punctuation\n    if not ref_text.endswith(\". \") and not ref_text.endswith(\"。\"):\n        if ref_text.endswith(\".\"):\n            ref_text += \" \"\n        else:\n            ref_text += \". \"\n\n    print(\"\\nref_text  \", ref_text)\n\n    return ref_audio, ref_text\n\n\n# infer process: chunk text -> infer batches [i.e. infer_batch_process()]\n\n\ndef infer_process(\n    ref_audio,\n    ref_text,\n    gen_text,\n    model_obj,\n    vocoder,\n    mel_spec_type=mel_spec_type,\n    show_info=print,\n    progress=tqdm,\n    target_rms=target_rms,\n    cross_fade_duration=cross_fade_duration,\n    nfe_step=nfe_step,\n    cfg_strength=cfg_strength,\n    sway_sampling_coef=sway_sampling_coef,\n    speed=speed,\n    fix_duration=fix_duration,\n    device=device,\n):\n    # Split the input text into batches\n    audio, sr = torchaudio.load(ref_audio)\n    max_chars = int(len(ref_text.encode(\"utf-8\")) / (audio.shape[-1] / sr) * (22 - audio.shape[-1] / sr) * speed)\n    gen_text_batches = chunk_text(gen_text, max_chars=max_chars)\n    for i, gen_text in enumerate(gen_text_batches):\n        print(f\"gen_text {i}\", gen_text)\n    print(\"\\n\")\n\n    show_info(f\"Generating audio in {len(gen_text_batches)} batches...\")\n    return next(\n        infer_batch_process(\n            (audio, sr),\n            ref_text,\n            gen_text_batches,\n            model_obj,\n            vocoder,\n            mel_spec_type=mel_spec_type,\n            progress=progress,\n            target_rms=target_rms,\n            cross_fade_duration=cross_fade_duration,\n            nfe_step=nfe_step,\n            cfg_strength=cfg_strength,\n            sway_sampling_coef=sway_sampling_coef,\n            speed=speed,\n            fix_duration=fix_duration,\n            device=device,\n        )\n    )\n\n\n# infer batches\n\n\ndef infer_batch_process(\n    ref_audio,\n    ref_text,\n    gen_text_batches,\n    model_obj,\n    vocoder,\n    mel_spec_type=\"vocos\",\n    progress=tqdm,\n    target_rms=0.1,\n    cross_fade_duration=0.15,\n    nfe_step=32,\n    cfg_strength=2.0,\n    sway_sampling_coef=-1,\n    speed=1,\n    fix_duration=None,\n    device=None,\n    streaming=False,\n    chunk_size=2048,\n):\n    audio, sr = ref_audio\n    if audio.shape[0] > 1:\n        audio = torch.mean(audio, dim=0, keepdim=True)\n\n    rms = torch.sqrt(torch.mean(torch.square(audio)))\n    if rms < target_rms:\n        audio = audio * target_rms / rms\n    if sr != target_sample_rate:\n        resampler = torchaudio.transforms.Resample(sr, target_sample_rate)\n        audio = resampler(audio)\n    audio = audio.to(device)\n\n    generated_waves = []\n    spectrograms = []\n\n    if len(ref_text[-1].encode(\"utf-8\")) == 1:\n        ref_text = ref_text + \" \"\n\n    def process_batch(gen_text):\n        local_speed = speed\n        if len(gen_text.encode(\"utf-8\")) < 10:\n            local_speed = 0.3\n\n        # Prepare the text\n        text_list = [ref_text + gen_text]\n        final_text_list = convert_char_to_pinyin(text_list)\n\n        ref_audio_len = audio.shape[-1] // hop_length\n        if fix_duration is not None:\n            duration = int(fix_duration * target_sample_rate / hop_length)\n        else:\n            # Calculate duration\n            ref_text_len = len(ref_text.encode(\"utf-8\"))\n            gen_text_len = len(gen_text.encode(\"utf-8\"))\n            duration = ref_audio_len + int(ref_audio_len / ref_text_len * gen_text_len / local_speed)\n\n        # inference\n        with torch.inference_mode():\n            generated, _ = model_obj.sample(\n                cond=audio,\n                text=final_text_list,\n                duration=duration,\n                steps=nfe_step,\n                cfg_strength=cfg_strength,\n                sway_sampling_coef=sway_sampling_coef,\n            )\n            del _\n\n            generated = generated.to(torch.float32)  # generated mel spectrogram\n            generated = generated[:, ref_audio_len:, :]\n            generated = generated.permute(0, 2, 1)\n            if mel_spec_type == \"vocos\":\n                generated_wave = vocoder.decode(generated)\n            elif mel_spec_type == \"bigvgan\":\n                generated_wave = vocoder(generated)\n            if rms < target_rms:\n                generated_wave = generated_wave * rms / target_rms\n\n            # wav -> numpy\n            generated_wave = generated_wave.squeeze().cpu().numpy()\n\n            if streaming:\n                for j in range(0, len(generated_wave), chunk_size):\n                    yield generated_wave[j : j + chunk_size], target_sample_rate\n            else:\n                generated_cpu = generated[0].cpu().numpy()\n                del generated\n                yield generated_wave, generated_cpu\n\n    if streaming:\n        for gen_text in progress.tqdm(gen_text_batches) if progress is not None else gen_text_batches:\n            for chunk in process_batch(gen_text):\n                yield chunk\n    else:\n        with ThreadPoolExecutor() as executor:\n            futures = [executor.submit(process_batch, gen_text) for gen_text in gen_text_batches]\n            for future in progress.tqdm(futures) if progress is not None else futures:\n                result = future.result()\n                if result:\n                    generated_wave, generated_mel_spec = next(result)\n                    generated_waves.append(generated_wave)\n                    spectrograms.append(generated_mel_spec)\n\n        if generated_waves:\n            if cross_fade_duration <= 0:\n                # Simply concatenate\n                final_wave = np.concatenate(generated_waves)\n            else:\n                # Combine all generated waves with cross-fading\n                final_wave = generated_waves[0]\n                for i in range(1, len(generated_waves)):\n                    prev_wave = final_wave\n                    next_wave = generated_waves[i]\n\n                    # Calculate cross-fade samples, ensuring it does not exceed wave lengths\n                    cross_fade_samples = int(cross_fade_duration * target_sample_rate)\n                    cross_fade_samples = min(cross_fade_samples, len(prev_wave), len(next_wave))\n\n                    if cross_fade_samples <= 0:\n                        # No overlap possible, concatenate\n                        final_wave = np.concatenate([prev_wave, next_wave])\n                        continue\n\n                    # Overlapping parts\n                    prev_overlap = prev_wave[-cross_fade_samples:]\n                    next_overlap = next_wave[:cross_fade_samples]\n\n                    # Fade out and fade in\n                    fade_out = np.linspace(1, 0, cross_fade_samples)\n                    fade_in = np.linspace(0, 1, cross_fade_samples)\n\n                    # Cross-faded overlap\n                    cross_faded_overlap = prev_overlap * fade_out + next_overlap * fade_in\n\n                    # Combine\n                    new_wave = np.concatenate(\n                        [prev_wave[:-cross_fade_samples], cross_faded_overlap, next_wave[cross_fade_samples:]]\n                    )\n\n                    final_wave = new_wave\n\n            # Create a combined spectrogram\n            combined_spectrogram = np.concatenate(spectrograms, axis=1)\n\n            yield final_wave, target_sample_rate, combined_spectrogram\n\n        else:\n            yield None, target_sample_rate, None\n\n\n# remove silence from generated wav\n\n\ndef remove_silence_for_generated_wav(filename):\n    aseg = AudioSegment.from_file(filename)\n    non_silent_segs = silence.split_on_silence(\n        aseg, min_silence_len=1000, silence_thresh=-50, keep_silence=500, seek_step=10\n    )\n    non_silent_wave = AudioSegment.silent(duration=0)\n    for non_silent_seg in non_silent_segs:\n        non_silent_wave += non_silent_seg\n    aseg = non_silent_wave\n    aseg.export(filename, format=\"wav\")\n\n\n# save spectrogram\n\n\ndef save_spectrogram(spectrogram, path):\n    plt.figure(figsize=(12, 4))\n    plt.imshow(spectrogram, origin=\"lower\", aspect=\"auto\")\n    plt.colorbar()\n    plt.savefig(path)\n    plt.close()\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}