{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/AnimateDiff",
    "API_Calls": [
        {
            "Name": "call_AnimationPipeline",
            "Description": "call AnimationPipeline to generates animate",
            "Code": "import argparse\nimport datetime\nimport inspect\nimport os\nfrom omegaconf import OmegaConf\n\nimport torch\nimport torchvision.transforms as transforms\n\nimport diffusers\nfrom diffusers import AutoencoderKL, DDIMScheduler\n\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nfrom animatediff.models.unet import UNet3DConditionModel\nfrom animatediff.models.sparse_controlnet import SparseControlNetModel\nfrom animatediff.pipelines.pipeline_animation import AnimationPipeline\nfrom animatediff.utils.util import save_videos_grid\nfrom animatediff.utils.util import load_weights, auto_download\nfrom diffusers.utils.import_utils import is_xformers_available\n\nfrom einops import rearrange, repeat\n\nimport csv, pdb, glob, math\nfrom pathlib import Path\nfrom PIL import Image\nimport numpy as np\n\n\n@torch.no_grad()\ndef main(args):\n    *_, func_args = inspect.getargvalues(inspect.currentframe())\n    func_args = dict(func_args)\n    \n    time_str = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n    savedir = f\"samples/{Path(args.config).stem}-{time_str}\"\n    os.makedirs(savedir)\n\n    config  = OmegaConf.load(args.config)\n    samples = []\n\n    # create validation pipeline\n    tokenizer    = CLIPTokenizer.from_pretrained(args.pretrained_model_path, subfolder=\"tokenizer\")\n    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_path, subfolder=\"text_encoder\").cuda()\n    vae          = AutoencoderKL.from_pretrained(args.pretrained_model_path, subfolder=\"vae\").cuda()\n\n    sample_idx = 0\n    for model_idx, model_config in enumerate(config):\n        model_config.W = model_config.get(\"W\", args.W)\n        model_config.H = model_config.get(\"H\", args.H)\n        model_config.L = model_config.get(\"L\", args.L)\n\n        inference_config = OmegaConf.load(model_config.get(\"inference_config\", args.inference_config))\n        unet = UNet3DConditionModel.from_pretrained_2d(args.pretrained_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs)).cuda()\n\n        # load controlnet model\n        controlnet = controlnet_images = None\n        if model_config.get(\"controlnet_path\", \"\") != \"\":\n            assert model_config.get(\"controlnet_images\", \"\") != \"\"\n            assert model_config.get(\"controlnet_config\", \"\") != \"\"\n            \n            unet.config.num_attention_heads = 8\n            unet.config.projection_class_embeddings_input_dim = None\n\n            controlnet_config = OmegaConf.load(model_config.controlnet_config)\n            controlnet = SparseControlNetModel.from_unet(unet, controlnet_additional_kwargs=controlnet_config.get(\"controlnet_additional_kwargs\", {}))\n\n            auto_download(model_config.controlnet_path, is_dreambooth_lora=False)\n            print(f\"loading controlnet checkpoint from {model_config.controlnet_path} ...\")\n            controlnet_state_dict = torch.load(model_config.controlnet_path, map_location=\"cpu\")\n            controlnet_state_dict = controlnet_state_dict[\"controlnet\"] if \"controlnet\" in controlnet_state_dict else controlnet_state_dict\n            controlnet_state_dict = {name: param for name, param in controlnet_state_dict.items() if \"pos_encoder.pe\" not in name}\n            controlnet_state_dict.pop(\"animatediff_config\", \"\")\n            controlnet.load_state_dict(controlnet_state_dict)\n            controlnet.cuda()\n\n            image_paths = model_config.controlnet_images\n            if isinstance(image_paths, str): image_paths = [image_paths]\n\n            print(f\"controlnet image paths:\")\n            for path in image_paths: print(path)\n            assert len(image_paths) <= model_config.L\n\n            image_transforms = transforms.Compose([\n                transforms.RandomResizedCrop(\n                    (model_config.H, model_config.W), (1.0, 1.0), \n                    ratio=(model_config.W/model_config.H, model_config.W/model_config.H)\n                ),\n                transforms.ToTensor(),\n            ])\n\n            if model_config.get(\"normalize_condition_images\", False):\n                def image_norm(image):\n                    image = image.mean(dim=0, keepdim=True).repeat(3,1,1)\n                    image -= image.min()\n                    image /= image.max()\n                    return image\n            else: image_norm = lambda x: x\n                \n            controlnet_images = [image_norm(image_transforms(Image.open(path).convert(\"RGB\"))) for path in image_paths]\n\n            os.makedirs(os.path.join(savedir, \"control_images\"), exist_ok=True)\n            for i, image in enumerate(controlnet_images):\n                Image.fromarray((255. * (image.numpy().transpose(1,2,0))).astype(np.uint8)).save(f\"{savedir}/control_images/{i}.png\")\n\n            controlnet_images = torch.stack(controlnet_images).unsqueeze(0).cuda()\n            controlnet_images = rearrange(controlnet_images, \"b f c h w -> b c f h w\")\n\n            if controlnet.use_simplified_condition_embedding:\n                num_controlnet_images = controlnet_images.shape[2]\n                controlnet_images = rearrange(controlnet_images, \"b c f h w -> (b f) c h w\")\n                controlnet_images = vae.encode(controlnet_images * 2. - 1.).latent_dist.sample() * 0.18215\n                controlnet_images = rearrange(controlnet_images, \"(b f) c h w -> b c f h w\", f=num_controlnet_images)\n\n        # set xformers\n        if is_xformers_available() and (not args.without_xformers):\n            unet.enable_xformers_memory_efficient_attention()\n            if controlnet is not None: controlnet.enable_xformers_memory_efficient_attention()\n\n        pipeline = AnimationPipeline(\n            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n            controlnet=controlnet,\n            scheduler=DDIMScheduler(**OmegaConf.to_container(inference_config.noise_scheduler_kwargs)),\n        ).to(\"cuda\")\n\n        pipeline = load_weights(\n            pipeline,\n            # motion module\n            motion_module_path         = model_config.get(\"motion_module\", \"\"),\n            motion_module_lora_configs = model_config.get(\"motion_module_lora_configs\", []),\n            # domain adapter\n            adapter_lora_path          = model_config.get(\"adapter_lora_path\", \"\"),\n            adapter_lora_scale         = model_config.get(\"adapter_lora_scale\", 1.0),\n            # image layers\n            dreambooth_model_path      = model_config.get(\"dreambooth_path\", \"\"),\n            lora_model_path            = model_config.get(\"lora_model_path\", \"\"),\n            lora_alpha                 = model_config.get(\"lora_alpha\", 0.8),\n        ).to(\"cuda\")\n\n        prompts      = model_config.prompt\n        n_prompts    = list(model_config.n_prompt) * len(prompts) if len(model_config.n_prompt) == 1 else model_config.n_prompt\n        \n        random_seeds = model_config.get(\"seed\", [-1])\n        random_seeds = [random_seeds] if isinstance(random_seeds, int) else list(random_seeds)\n        random_seeds = random_seeds * len(prompts) if len(random_seeds) == 1 else random_seeds\n        \n        config[model_idx].random_seed = []\n        for prompt_idx, (prompt, n_prompt, random_seed) in enumerate(zip(prompts, n_prompts, random_seeds)):\n            \n            # manually set random seed for reproduction\n            if random_seed != -1: torch.manual_seed(random_seed)\n            else: torch.seed()\n            config[model_idx].random_seed.append(torch.initial_seed())\n            \n            print(f\"current seed: {torch.initial_seed()}\")\n            print(f\"sampling {prompt} ...\")\n            sample = pipeline(\n                prompt,\n                negative_prompt     = n_prompt,\n                num_inference_steps = model_config.steps,\n                guidance_scale      = model_config.guidance_scale,\n                width               = model_config.W,\n                height              = model_config.H,\n                video_length        = model_config.L,\n\n                controlnet_images = controlnet_images,\n                controlnet_image_index = model_config.get(\"controlnet_image_indexs\", [0]),\n            ).videos\n            samples.append(sample)\n\n            prompt = \"-\".join((prompt.replace(\"/\", \"\").split(\" \")[:10]))\n            save_videos_grid(sample, f\"{savedir}/sample/{sample_idx}-{prompt}.gif\")\n            print(f\"save to {savedir}/sample/{prompt}.gif\")\n            \n            sample_idx += 1\n\n    samples = torch.concat(samples)\n    save_videos_grid(samples, f\"{savedir}/sample.gif\", n_rows=4)\n\n    OmegaConf.save(config, f\"{savedir}/config.yaml\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--pretrained-model-path\", type=str, default=\"runwayml/stable-diffusion-v1-5\")\n    parser.add_argument(\"--inference-config\",      type=str, default=\"configs/inference/inference-v1.yaml\")    \n    parser.add_argument(\"--config\",                type=str, required=True)\n    \n    parser.add_argument(\"--L\", type=int, default=16 )\n    parser.add_argument(\"--W\", type=int, default=512)\n    parser.add_argument(\"--H\", type=int, default=512)\n\n    parser.add_argument(\"--without-xformers\", action=\"store_true\")\n\n    args = parser.parse_args()\n    main(args)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/AnimateDiff/scripts/animate.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "AnimationPipeline",
            "Description": "AnimationPipeline implements",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/AnimateDiff/animatediff/pipelines/pipeline_animation.py",
            "Implementation": "# Adapted from https://github.com/showlab/Tune-A-Video/blob/main/tuneavideo/pipelines/pipeline_tuneavideo.py\n\nimport inspect\nfrom typing import Callable, List, Optional, Union\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom diffusers.utils import is_accelerate_available\nfrom packaging import version\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nfrom diffusers.configuration_utils import FrozenDict\nfrom diffusers.models import AutoencoderKL\nfrom diffusers.pipeline_utils import DiffusionPipeline\nfrom diffusers.schedulers import (\n    DDIMScheduler,\n    DPMSolverMultistepScheduler,\n    EulerAncestralDiscreteScheduler,\n    EulerDiscreteScheduler,\n    LMSDiscreteScheduler,\n    PNDMScheduler,\n)\nfrom diffusers.utils import deprecate, logging, BaseOutput\n\nfrom einops import rearrange\n\nfrom ..models.unet import UNet3DConditionModel\nfrom ..models.sparse_controlnet import SparseControlNetModel\nimport pdb\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\n@dataclass\nclass AnimationPipelineOutput(BaseOutput):\n    videos: Union[torch.Tensor, np.ndarray]\n\n\nclass AnimationPipeline(DiffusionPipeline):\n    _optional_components = []\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet3DConditionModel,\n        scheduler: Union[\n            DDIMScheduler,\n            PNDMScheduler,\n            LMSDiscreteScheduler,\n            EulerDiscreteScheduler,\n            EulerAncestralDiscreteScheduler,\n            DPMSolverMultistepScheduler,\n        ],\n        controlnet: Union[SparseControlNetModel, None] = None,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n                \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n                \" config accordingly as not setting `clip_sample` in the config might lead to incorrect results in\"\n                \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n                \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n            )\n            deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"clip_sample\"] = False\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        is_unet_version_less_0_9_0 = hasattr(unet.config, \"_diffusers_version\") and version.parse(\n            version.parse(unet.config._diffusers_version).base_version\n        ) < version.parse(\"0.9.0.dev0\")\n        is_unet_sample_size_less_64 = hasattr(unet.config, \"sample_size\") and unet.config.sample_size < 64\n        if is_unet_version_less_0_9_0 and is_unet_sample_size_less_64:\n            deprecation_message = (\n                \"The configuration file of the unet has set the default `sample_size` to smaller than\"\n                \" 64 which seems highly unlikely. If your checkpoint is a fine-tuned version of any of the\"\n                \" following: \\n- CompVis/stable-diffusion-v1-4 \\n- CompVis/stable-diffusion-v1-3 \\n-\"\n                \" CompVis/stable-diffusion-v1-2 \\n- CompVis/stable-diffusion-v1-1 \\n- runwayml/stable-diffusion-v1-5\"\n                \" \\n- runwayml/stable-diffusion-inpainting \\n you should change 'sample_size' to 64 in the\"\n                \" configuration file. Please make sure to update the config accordingly as leaving `sample_size=32`\"\n                \" in the config might lead to incorrect results in future versions. If you have downloaded this\"\n                \" checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for\"\n                \" the `unet/config.json` file\"\n            )\n            deprecate(\"sample_size<64\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(unet.config)\n            new_config[\"sample_size\"] = 64\n            unet._internal_dict = FrozenDict(new_config)\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            controlnet=controlnet,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n\n    def enable_vae_slicing(self):\n        self.vae.enable_slicing()\n\n    def disable_vae_slicing(self):\n        self.vae.disable_slicing()\n\n    def enable_sequential_cpu_offload(self, gpu_id=0):\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)\n\n\n    @property\n    def _execution_device(self):\n        if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):\n            return self.device\n        for module in self.unet.modules():\n            if (\n                hasattr(module, \"_hf_hook\")\n                and hasattr(module._hf_hook, \"execution_device\")\n                and module._hf_hook.execution_device is not None\n            ):\n                return torch.device(module._hf_hook.execution_device)\n        return self.device\n\n    def _encode_prompt(self, prompt, device, num_videos_per_prompt, do_classifier_free_guidance, negative_prompt):\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n            )\n\n        if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n            attention_mask = text_inputs.attention_mask.to(device)\n        else:\n            attention_mask = None\n\n        text_embeddings = self.text_encoder(\n            text_input_ids.to(device),\n            attention_mask=attention_mask,\n        )\n        text_embeddings = text_embeddings[0]\n\n        # duplicate text embeddings for each generation per prompt, using mps friendly method\n        bs_embed, seq_len, _ = text_embeddings.shape\n        text_embeddings = text_embeddings.repeat(1, num_videos_per_prompt, 1)\n        text_embeddings = text_embeddings.view(bs_embed * num_videos_per_prompt, seq_len, -1)\n\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance:\n            uncond_tokens: List[str]\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            elif type(prompt) is not type(negative_prompt):\n                raise TypeError(\n                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n                    f\" {type(prompt)}.\"\n                )\n            elif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\n            elif batch_size != len(negative_prompt):\n                raise ValueError(\n                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                    \" the batch size of `prompt`.\"\n                )\n            else:\n                uncond_tokens = negative_prompt\n\n            max_length = text_input_ids.shape[-1]\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n\n            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n                attention_mask = uncond_input.attention_mask.to(device)\n            else:\n                attention_mask = None\n\n            uncond_embeddings = self.text_encoder(\n                uncond_input.input_ids.to(device),\n                attention_mask=attention_mask,\n            )\n            uncond_embeddings = uncond_embeddings[0]\n\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            seq_len = uncond_embeddings.shape[1]\n            uncond_embeddings = uncond_embeddings.repeat(1, num_videos_per_prompt, 1)\n            uncond_embeddings = uncond_embeddings.view(batch_size * num_videos_per_prompt, seq_len, -1)\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n        return text_embeddings\n\n    def decode_latents(self, latents):\n        video_length = latents.shape[2]\n        latents = 1 / 0.18215 * latents\n        latents = rearrange(latents, \"b c f h w -> (b f) c h w\")\n        # video = self.vae.decode(latents).sample\n        video = []\n        for frame_idx in tqdm(range(latents.shape[0])):\n            video.append(self.vae.decode(latents[frame_idx:frame_idx+1]).sample)\n        video = torch.cat(video)\n        video = rearrange(video, \"(b f) c h w -> b c f h w\", f=video_length)\n        video = (video / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        video = video.cpu().float().numpy()\n        return video\n\n    def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs\n\n    def check_inputs(self, prompt, height, width, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n    def prepare_latents(self, batch_size, num_channels_latents, video_length, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, video_length, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n        if latents is None:\n            rand_device = \"cpu\" if device.type == \"mps\" else device\n\n            if isinstance(generator, list):\n                shape = shape\n                # shape = (1,) + shape[1:]\n                latents = [\n                    torch.randn(shape, generator=generator[i], device=rand_device, dtype=dtype)\n                    for i in range(batch_size)\n                ]\n                latents = torch.cat(latents, dim=0).to(device)\n            else:\n                latents = torch.randn(shape, generator=generator, device=rand_device, dtype=dtype).to(device)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        video_length: Optional[int],\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_videos_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"tensor\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n\n        # support controlnet\n        controlnet_images: torch.FloatTensor = None,\n        controlnet_image_index: list = [0],\n        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n\n        **kwargs,\n    ):\n        # Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        # Check inputs. Raise error if not correct\n        self.check_inputs(prompt, height, width, callback_steps)\n\n        # Define call parameters\n        # batch_size = 1 if isinstance(prompt, str) else len(prompt)\n        batch_size = 1\n        if latents is not None:\n            batch_size = latents.shape[0]\n        if isinstance(prompt, list):\n            batch_size = len(prompt)\n\n        device = self._execution_device\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        # Encode input prompt\n        prompt = prompt if isinstance(prompt, list) else [prompt] * batch_size\n        if negative_prompt is not None:\n            negative_prompt = negative_prompt if isinstance(negative_prompt, list) else [negative_prompt] * batch_size \n        text_embeddings = self._encode_prompt(\n            prompt, device, num_videos_per_prompt, do_classifier_free_guidance, negative_prompt\n        )\n\n        # Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n\n        # Prepare latent variables\n        num_channels_latents = self.unet.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_videos_per_prompt,\n            num_channels_latents,\n            video_length,\n            height,\n            width,\n            text_embeddings.dtype,\n            device,\n            generator,\n            latents,\n        )\n        latents_dtype = latents.dtype\n\n        # Prepare extra step kwargs.\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n                down_block_additional_residuals = mid_block_additional_residual = None\n                if (getattr(self, \"controlnet\", None) != None) and (controlnet_images != None):\n                    assert controlnet_images.dim() == 5\n\n                    controlnet_noisy_latents = latent_model_input\n                    controlnet_prompt_embeds = text_embeddings\n\n                    controlnet_images = controlnet_images.to(latents.device)\n\n                    controlnet_cond_shape    = list(controlnet_images.shape)\n                    controlnet_cond_shape[2] = video_length\n                    controlnet_cond = torch.zeros(controlnet_cond_shape).to(latents.device)\n\n                    controlnet_conditioning_mask_shape    = list(controlnet_cond.shape)\n                    controlnet_conditioning_mask_shape[1] = 1\n                    controlnet_conditioning_mask          = torch.zeros(controlnet_conditioning_mask_shape).to(latents.device)\n\n                    assert controlnet_images.shape[2] >= len(controlnet_image_index)\n                    controlnet_cond[:,:,controlnet_image_index] = controlnet_images[:,:,:len(controlnet_image_index)]\n                    controlnet_conditioning_mask[:,:,controlnet_image_index] = 1\n\n                    down_block_additional_residuals, mid_block_additional_residual = self.controlnet(\n                        controlnet_noisy_latents, t,\n                        encoder_hidden_states=controlnet_prompt_embeds,\n                        controlnet_cond=controlnet_cond,\n                        conditioning_mask=controlnet_conditioning_mask,\n                        conditioning_scale=controlnet_conditioning_scale,\n                        guess_mode=False, return_dict=False,\n                    )\n\n                # predict the noise residual\n                noise_pred = self.unet(\n                    latent_model_input, t, \n                    encoder_hidden_states=text_embeddings,\n                    down_block_additional_residuals = down_block_additional_residuals,\n                    mid_block_additional_residual   = mid_block_additional_residual,\n                ).sample.to(dtype=latents_dtype)\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n\n        # Post-processing\n        video = self.decode_latents(latents)\n\n        # Convert to tensor\n        if output_type == \"tensor\":\n            video = torch.from_numpy(video)\n\n        if not return_dict:\n            return video\n\n        return AnimationPipelineOutput(videos=video)\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}