{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/SadTalker",
    "API_Calls": [
        {
            "Name": "interface_generate_video",
            "Code": "from glob import glob\nimport shutil\nimport torch\nfrom time import  strftime\nimport os, sys, time\nfrom argparse import ArgumentParser\n\nfrom src.utils.preprocess import CropAndExtract\nfrom src.test_audio2coeff import Audio2Coeff  \nfrom src.facerender.animate import AnimateFromCoeff\nfrom src.generate_batch import get_data\nfrom src.generate_facerender_batch import get_facerender_data\nfrom src.utils.init_path import init_path\n\ndef main(args):\n    #torch.backends.cudnn.enabled = False\n\n    pic_path = args.source_image\n    audio_path = args.driven_audio\n    save_dir = os.path.join(args.result_dir, strftime(\"%Y_%m_%d_%H.%M.%S\"))\n    os.makedirs(save_dir, exist_ok=True)\n    pose_style = args.pose_style\n    device = args.device\n    batch_size = args.batch_size\n    input_yaw_list = args.input_yaw\n    input_pitch_list = args.input_pitch\n    input_roll_list = args.input_roll\n    ref_eyeblink = args.ref_eyeblink\n    ref_pose = args.ref_pose\n\n    current_root_path = os.path.split(sys.argv[0])[0]\n\n    sadtalker_paths = init_path(args.checkpoint_dir, os.path.join(current_root_path, 'src/config'), args.size, args.old_version, args.preprocess)\n\n    #init model\n    preprocess_model = CropAndExtract(sadtalker_paths, device)\n\n    audio_to_coeff = Audio2Coeff(sadtalker_paths,  device)\n    \n    animate_from_coeff = AnimateFromCoeff(sadtalker_paths, device)\n\n    #crop image and extract 3dmm from image\n    first_frame_dir = os.path.join(save_dir, 'first_frame_dir')\n    os.makedirs(first_frame_dir, exist_ok=True)\n    print('3DMM Extraction for source image')\n    first_coeff_path, crop_pic_path, crop_info =  preprocess_model.generate(pic_path, first_frame_dir, args.preprocess,                                                                             source_image_flag=True, pic_size=args.size)\n    if first_coeff_path is None:\n        print(\"Can't get the coeffs of the input\")\n        return\n\n    if ref_eyeblink is not None:\n        ref_eyeblink_videoname = os.path.splitext(os.path.split(ref_eyeblink)[-1])[0]\n        ref_eyeblink_frame_dir = os.path.join(save_dir, ref_eyeblink_videoname)\n        os.makedirs(ref_eyeblink_frame_dir, exist_ok=True)\n        print('3DMM Extraction for the reference video providing eye blinking')\n        ref_eyeblink_coeff_path, _, _ =  preprocess_model.generate(ref_eyeblink, ref_eyeblink_frame_dir, args.preprocess, source_image_flag=False)\n    else:\n        ref_eyeblink_coeff_path=None\n\n    if ref_pose is not None:\n        if ref_pose == ref_eyeblink: \n            ref_pose_coeff_path = ref_eyeblink_coeff_path\n        else:\n            ref_pose_videoname = os.path.splitext(os.path.split(ref_pose)[-1])[0]\n            ref_pose_frame_dir = os.path.join(save_dir, ref_pose_videoname)\n            os.makedirs(ref_pose_frame_dir, exist_ok=True)\n            print('3DMM Extraction for the reference video providing pose')\n            ref_pose_coeff_path, _, _ =  preprocess_model.generate(ref_pose, ref_pose_frame_dir, args.preprocess, source_image_flag=False)\n    else:\n        ref_pose_coeff_path=None\n\n    #audio2ceoff\n    batch = get_data(first_coeff_path, audio_path, device, ref_eyeblink_coeff_path, still=args.still)\n    coeff_path = audio_to_coeff.generate(batch, save_dir, pose_style, ref_pose_coeff_path)\n\n    # 3dface render\n    if args.face3dvis:\n        from src.face3d.visualize import gen_composed_video\n        gen_composed_video(args, device, first_coeff_path, coeff_path, audio_path, os.path.join(save_dir, '3dface.mp4'))\n    \n    #coeff2video\n    data = get_facerender_data(coeff_path, crop_pic_path, first_coeff_path, audio_path, \n                                batch_size, input_yaw_list, input_pitch_list, input_roll_list,\n                                expression_scale=args.expression_scale, still_mode=args.still, preprocess=args.preprocess, size=args.size)\n    \n    result = animate_from_coeff.generate(data, save_dir, pic_path, crop_info,                                 enhancer=args.enhancer, background_enhancer=args.background_enhancer, preprocess=args.preprocess, img_size=args.size)\n    \n    shutil.move(result, save_dir+'.mp4')\n    print('The generated video is named:', save_dir+'.mp4')\n\n    if not args.verbose:\n        shutil.rmtree(save_dir)\n\n    \nif __name__ == '__main__':\n\n    parser = ArgumentParser()  \n    parser.add_argument(\"--driven_audio\", default='./examples/driven_audio/bus_chinese.wav', help=\"path to driven audio\")\n    parser.add_argument(\"--source_image\", default='./examples/source_image/full_body_1.png', help=\"path to source image\")\n    parser.add_argument(\"--ref_eyeblink\", default=None, help=\"path to reference video providing eye blinking\")\n    parser.add_argument(\"--ref_pose\", default=None, help=\"path to reference video providing pose\")\n    parser.add_argument(\"--checkpoint_dir\", default='./checkpoints', help=\"path to output\")\n    parser.add_argument(\"--result_dir\", default='./results', help=\"path to output\")\n    parser.add_argument(\"--pose_style\", type=int, default=0,  help=\"input pose style from [0, 46)\")\n    parser.add_argument(\"--batch_size\", type=int, default=2,  help=\"the batch size of facerender\")\n    parser.add_argument(\"--size\", type=int, default=256,  help=\"the image size of the facerender\")\n    parser.add_argument(\"--expression_scale\", type=float, default=1.,  help=\"the batch size of facerender\")\n    parser.add_argument('--input_yaw', nargs='+', type=int, default=None, help=\"the input yaw degree of the user \")\n    parser.add_argument('--input_pitch', nargs='+', type=int, default=None, help=\"the input pitch degree of the user\")\n    parser.add_argument('--input_roll', nargs='+', type=int, default=None, help=\"the input roll degree of the user\")\n    parser.add_argument('--enhancer',  type=str, default=None, help=\"Face enhancer, [gfpgan, RestoreFormer]\")\n    parser.add_argument('--background_enhancer',  type=str, default=None, help=\"background enhancer, [realesrgan]\")\n    parser.add_argument(\"--cpu\", dest=\"cpu\", action=\"store_true\") \n    parser.add_argument(\"--face3dvis\", action=\"store_true\", help=\"generate 3d face and 3d landmarks\") \n    parser.add_argument(\"--still\", action=\"store_true\", help=\"can crop back to the original videos for the full body aniamtion\") \n    parser.add_argument(\"--preprocess\", default='crop', choices=['crop', 'extcrop', 'resize', 'full', 'extfull'], help=\"how to preprocess the images\" ) \n    parser.add_argument(\"--verbose\",action=\"store_true\", help=\"saving the intermedia output or not\" ) \n    parser.add_argument(\"--old_version\",action=\"store_true\", help=\"use the pth other than safetensor version\" ) \n\n\n    # net structure and parameters\n    parser.add_argument('--net_recon', type=str, default='resnet50', choices=['resnet18', 'resnet34', 'resnet50'], help='useless')\n    parser.add_argument('--init_path', type=str, default=None, help='Useless')\n    parser.add_argument('--use_last_fc',default=False, help='zero initialize the last fc')\n    parser.add_argument('--bfm_folder', type=str, default='./checkpoints/BFM_Fitting/')\n    parser.add_argument('--bfm_model', type=str, default='BFM_model_front.mat', help='bfm model')\n\n    # default renderer parameters\n    parser.add_argument('--focal', type=float, default=1015.)\n    parser.add_argument('--center', type=float, default=112.)\n    parser.add_argument('--camera_d', type=float, default=10.)\n    parser.add_argument('--z_near', type=float, default=5.)\n    parser.add_argument('--z_far', type=float, default=15.)\n\n    args = parser.parse_args()\n\n    if torch.cuda.is_available() and not args.cpu:\n        args.device = \"cuda\"\n    else:\n        args.device = \"cpu\"\n\n    main(args)\n\n\n",
            "Description": "这是根据图像和视频生成新视频的全流程，例如我是通过执行命令行命令：python inference.py --driven_audio examples/driven_audio/bus_chinese.wav                     --source_image examples/source_image/art_0.png 生成一个视频",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/SadTalker/interface.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "Audio2Coeff",
            "Description": "这是将音频特征映射为人脸表情系数，实现音画同步的智能驱动，跨模态理解的部分",
            "Implementation": "import os \nimport torch\nimport numpy as np\nfrom scipy.io import savemat, loadmat\nfrom yacs.config import CfgNode as CN\nfrom scipy.signal import savgol_filter\n\nimport safetensors\nimport safetensors.torch \n\nfrom src.audio2pose_models.audio2pose import Audio2Pose\nfrom src.audio2exp_models.networks import SimpleWrapperV2 \nfrom src.audio2exp_models.audio2exp import Audio2Exp\nfrom src.utils.safetensor_helper import load_x_from_safetensor  \n\ndef load_cpk(checkpoint_path, model=None, optimizer=None, device=\"cpu\"):\n    checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n    if model is not None:\n        model.load_state_dict(checkpoint['model'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n\n    return checkpoint['epoch']\n\nclass Audio2Coeff():\n\n    def __init__(self, sadtalker_path, device):\n        #load config\n        fcfg_pose = open(sadtalker_path['audio2pose_yaml_path'])\n        cfg_pose = CN.load_cfg(fcfg_pose)\n        cfg_pose.freeze()\n        fcfg_exp = open(sadtalker_path['audio2exp_yaml_path'])\n        cfg_exp = CN.load_cfg(fcfg_exp)\n        cfg_exp.freeze()\n\n        # load audio2pose_model\n        self.audio2pose_model = Audio2Pose(cfg_pose, None, device=device)\n        self.audio2pose_model = self.audio2pose_model.to(device)\n        self.audio2pose_model.eval()\n        for param in self.audio2pose_model.parameters():\n            param.requires_grad = False \n        \n        try:\n            if sadtalker_path['use_safetensor']:\n                checkpoints = safetensors.torch.load_file(sadtalker_path['checkpoint'])\n                self.audio2pose_model.load_state_dict(load_x_from_safetensor(checkpoints, 'audio2pose'))\n            else:\n                load_cpk(sadtalker_path['audio2pose_checkpoint'], model=self.audio2pose_model, device=device)\n        except:\n            raise Exception(\"Failed in loading audio2pose_checkpoint\")\n\n        # load audio2exp_model\n        netG = SimpleWrapperV2()\n        netG = netG.to(device)\n        for param in netG.parameters():\n            netG.requires_grad = False\n        netG.eval()\n        try:\n            if sadtalker_path['use_safetensor']:\n                checkpoints = safetensors.torch.load_file(sadtalker_path['checkpoint'])\n                netG.load_state_dict(load_x_from_safetensor(checkpoints, 'audio2exp'))\n            else:\n                load_cpk(sadtalker_path['audio2exp_checkpoint'], model=netG, device=device)\n        except:\n            raise Exception(\"Failed in loading audio2exp_checkpoint\")\n        self.audio2exp_model = Audio2Exp(netG, cfg_exp, device=device, prepare_training_loss=False)\n        self.audio2exp_model = self.audio2exp_model.to(device)\n        for param in self.audio2exp_model.parameters():\n            param.requires_grad = False\n        self.audio2exp_model.eval()\n \n        self.device = device\n\n    def generate(self, batch, coeff_save_dir, pose_style, ref_pose_coeff_path=None):\n\n        with torch.no_grad():\n            #test\n            results_dict_exp= self.audio2exp_model.test(batch)\n            exp_pred = results_dict_exp['exp_coeff_pred']                         #bs T 64\n\n            #for class_id in  range(1):\n            #class_id = 0#(i+10)%45\n            #class_id = random.randint(0,46)                                   #46 styles can be selected \n            batch['class'] = torch.LongTensor([pose_style]).to(self.device)\n            results_dict_pose = self.audio2pose_model.test(batch) \n            pose_pred = results_dict_pose['pose_pred']                        #bs T 6\n\n            pose_len = pose_pred.shape[1]\n            if pose_len<13: \n                pose_len = int((pose_len-1)/2)*2+1\n                pose_pred = torch.Tensor(savgol_filter(np.array(pose_pred.cpu()), pose_len, 2, axis=1)).to(self.device)\n            else:\n                pose_pred = torch.Tensor(savgol_filter(np.array(pose_pred.cpu()), 13, 2, axis=1)).to(self.device) \n            \n            coeffs_pred = torch.cat((exp_pred, pose_pred), dim=-1)            #bs T 70\n\n            coeffs_pred_numpy = coeffs_pred[0].clone().detach().cpu().numpy() \n\n            if ref_pose_coeff_path is not None: \n                 coeffs_pred_numpy = self.using_refpose(coeffs_pred_numpy, ref_pose_coeff_path)\n        \n            savemat(os.path.join(coeff_save_dir, '%s##%s.mat'%(batch['pic_name'], batch['audio_name'])),  \n                    {'coeff_3dmm': coeffs_pred_numpy})\n\n            return os.path.join(coeff_save_dir, '%s##%s.mat'%(batch['pic_name'], batch['audio_name']))\n    \n    def using_refpose(self, coeffs_pred_numpy, ref_pose_coeff_path):\n        num_frames = coeffs_pred_numpy.shape[0]\n        refpose_coeff_dict = loadmat(ref_pose_coeff_path)\n        refpose_coeff = refpose_coeff_dict['coeff_3dmm'][:,64:70]\n        refpose_num_frames = refpose_coeff.shape[0]\n        if refpose_num_frames<num_frames:\n            div = num_frames//refpose_num_frames\n            re = num_frames%refpose_num_frames\n            refpose_coeff_list = [refpose_coeff for i in range(div)]\n            refpose_coeff_list.append(refpose_coeff[:re, :])\n            refpose_coeff = np.concatenate(refpose_coeff_list, axis=0)\n\n        #### relative head pose\n        coeffs_pred_numpy[:, 64:70] = coeffs_pred_numpy[:, 64:70] + ( refpose_coeff[:num_frames, :] - refpose_coeff[0:1, :] )\n        return coeffs_pred_numpy\n\n\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/SadTalker/src/test_audio2coeff.py",
            "Examples": []
        }
    ]
}