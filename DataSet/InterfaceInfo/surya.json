{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/surya",
    "API_Calls": [
        {
            "Name": "call_RecognitionPredictor",
            "Description": "call RecognitionPredictor",
            "Code": "import os\nimport click\nimport json\nimport time\nfrom collections import defaultdict\n\nfrom surya.common.surya.schema import TaskNames\nfrom surya.detection import DetectionPredictor\nfrom surya.debug.text import draw_text_on_image\nfrom surya.logging import configure_logging, get_logger\nfrom surya.recognition import RecognitionPredictor\nfrom surya.scripts.config import CLILoader\n\nconfigure_logging()\nlogger = get_logger()\n\n\n@click.command(help=\"OCR text.\")\n@click.option(\"--task_name\", type=str, default=TaskNames.ocr_with_boxes)\n@click.option(\n    \"--disable_math\", is_flag=True, default=False, help=\"Do not recognize math in OCR.\"\n)\n@CLILoader.common_options\ndef ocr_text_cli(input_path: str, task_name: str, disable_math: bool, **kwargs):\n    loader = CLILoader(input_path, kwargs, highres=True)\n    task_names = [task_name] * len(loader.images)\n\n    det_predictor = DetectionPredictor()\n    rec_predictor = RecognitionPredictor()\n\n    start = time.time()\n    predictions_by_image = rec_predictor(\n        loader.images,\n        task_names=task_names,\n        det_predictor=det_predictor,\n        highres_images=loader.highres_images,\n        math_mode=not disable_math,\n    )\n\n    if loader.debug:\n        logger.debug(f\"OCR took {time.time() - start:.2f} seconds\")\n        max_chars = max(\n            [len(line.text) for p in predictions_by_image for line in p.text_lines]\n        )\n        logger.debug(f\"Max chars: {max_chars}\")\n\n    if loader.save_images:\n        for idx, (name, image, pred) in enumerate(\n            zip(loader.names, loader.images, predictions_by_image)\n        ):\n            bboxes = [line.bbox for line in pred.text_lines]\n            pred_text = [line.text for line in pred.text_lines]\n            page_image = draw_text_on_image(bboxes, pred_text, image.size)\n            page_image.save(os.path.join(loader.result_path, f\"{name}_{idx}_text.png\"))\n\n    out_preds = defaultdict(list)\n    for name, pred, image in zip(loader.names, predictions_by_image, loader.images):\n        out_pred = pred.model_dump()\n        out_pred[\"page\"] = len(out_preds[name]) + 1\n        out_preds[name].append(out_pred)\n\n    with open(\n        os.path.join(loader.result_path, \"results.json\"), \"w+\", encoding=\"utf-8\"\n    ) as f:\n        json.dump(out_preds, f, ensure_ascii=False)\n\n    logger.info(f\"Wrote results to {loader.result_path}\")\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/surya/surya/scripts/ocr_text.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "RecognitionPredictor",
            "Description": "RecognitionPredictor implements an OCR engine that processes images to extract text, including mathematical expressions if enabled. It uses a detection predictor for locating text regions and a recognition predictor for interpreting the text within those regions. The results can be saved as images with drawn text annotations.",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/surya/surya/recognition/__init__.py",
            "Implementation": "from __future__ import annotations\n\nimport re\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nfrom collections import deque\n\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom transformers import QuantizedCacheConfig\n\nfrom surya.common.polygon import PolygonBox\nfrom surya.common.surya import SuryaModelOutput\nfrom surya.common.surya.processor import NOMATH_TOKEN\nfrom surya.common.util import mark_step\nfrom surya.common.predictor import BasePredictor\nfrom surya.detection import DetectionPredictor\nfrom surya.input.processing import (\n    convert_if_not_rgb,\n    slice_polys_from_image,\n    slice_bboxes_from_image,\n)\n\nfrom surya.recognition.loader import RecognitionModelLoader\nfrom surya.recognition.postprocessing import fix_unbalanced_tags\nfrom surya.recognition.util import (\n    sort_text_lines,\n    clean_close_polygons,\n    words_from_chars,\n    detect_repeat_token,\n    prediction_to_polygon_batch,\n    unwrap_math,\n    clean_math_tags,\n)\nfrom surya.recognition.schema import TextLine, OCRResult, TextChar\nfrom surya.common.surya.schema import TaskNames\nfrom surya.recognition.cache import (\n    ContinuousBatchingCache,\n    ContinuousBatchingQuantizedCache,\n)\nfrom surya.settings import settings\nfrom surya.logging import get_logger, configure_logging\n\nconfigure_logging()\nlogger = get_logger()\n\n\n@dataclass\nclass ContinuousBatchInput:\n    input_ids: torch.Tensor\n    attention_mask: torch.Tensor\n    position_ids: torch.Tensor\n\n\n@dataclass\nclass ContinuousBatchOutput:\n    input_ids: torch.Tensor\n    preds: torch.Tensor\n    bbox_preds: torch.Tensor\n    done: torch.Tensor\n    scores: torch.Tensor\n\n\n@dataclass\nclass RecognitionPrompt:\n    id: int\n    task_name: TaskNames\n    image: np.ndarray\n    text: str\n    math_mode: bool\n\n\nclass RecognitionPredictor(BasePredictor):\n    model_loader_cls = RecognitionModelLoader\n    batch_size = settings.RECOGNITION_BATCH_SIZE\n    torch_dtype = None  # No default, loader picks the dtype based on device properties - bf16/fp16\n    default_batch_sizes = {\"cpu\": 32, \"mps\": 64, \"cuda\": 256, \"xla\": 128}\n    encoder_chunk_size: int = 4096  # Default chunk size\n    encoder_chunk_sizes = {\"cpu\": 4096, \"mps\": 4096, \"cuda\": 32768, \"xla\": 32768}\n    min_prefill_ratio: int = 0.2\n    min_trim_length: int = 50\n    tasks = {\n        TaskNames.ocr_with_boxes: {\n            \"needs_bboxes\": True,\n            \"img_size\": (1024, 256),  # 370 max tokens\n            \"max_tokens\": 224,\n        },\n        TaskNames.ocr_without_boxes: {\n            \"needs_bboxes\": False,\n            \"img_size\": (1024, 256),  # 370 max tokens\n            \"max_tokens\": 224,\n        },\n        TaskNames.block_without_boxes: {\n            \"needs_bboxes\": False,\n            \"img_size\": (1024, 512),  # 703 max tokens\n            \"max_tokens\": 768,\n        },\n    }\n\n    def __init__(self, checkpoint=None, device=settings.TORCH_DEVICE_MODEL, dtype=None):\n        super().__init__(checkpoint, device, dtype)\n        self.kv_cache = None\n        self.prompt_queue = deque()\n        self.batch_prompt_mapping = None\n\n        # Setup various tokens on-device\n        self.device_pad_token = torch.tensor(\n            self.processor.pad_token_id, device=self.model.device, dtype=torch.long\n        )\n\n    def get_encoder_chunk_size(self) -> int:\n        if settings.RECOGNITION_CHUNK_SIZE is not None:\n            return settings.RECOGNITION_CHUNK_SIZE\n\n        chunk_size = self.encoder_chunk_size\n        if settings.TORCH_DEVICE_MODEL in self.encoder_chunk_sizes:\n            if settings.TORCH_DEVICE_MODEL in self.encoder_chunk_sizes:\n                chunk_size = self.encoder_chunk_sizes[settings.TORCH_DEVICE_MODEL]\n        return chunk_size\n\n    def setup_cache(self, batch_size: int):\n        self.kv_cache = None\n        self.prompt_queue.clear()\n        self.batch_prompt_mapping = {i: None for i in range(batch_size)}\n\n    @property\n    def num_empty_slots(self):\n        return sum(v is None for v in self.batch_prompt_mapping.values())\n\n    @property\n    def num_active_slots(self):\n        return len(self.batch_prompt_mapping) - self.num_empty_slots\n\n    def detect_and_slice_bboxes(\n        self,\n        images: List[Image.Image],\n        task_names: List[str],\n        det_predictor: DetectionPredictor,\n        detection_batch_size: int | None = None,\n        highres_images: List[Image.Image] | None = None,\n    ):\n        det_predictions = det_predictor(images, batch_size=detection_batch_size)\n\n        all_slices = []\n        slice_map = []\n        all_polygons = []\n        all_task_names = []\n        all_res_scales = []\n\n        for idx, (det_pred, image, highres_image, task_name) in enumerate(\n            zip(det_predictions, images, highres_images, task_names)\n        ):\n            polygons = [p.polygon for p in det_pred.bboxes]\n            if highres_image:\n                width_scaler = highres_image.size[0] / image.size[0]\n                height_scaler = highres_image.size[1] / image.size[1]\n                scaled_polygons = [\n                    [\n                        [int(p[0] * width_scaler), int(p[1] * height_scaler)]\n                        for p in polygon\n                    ]\n                    for polygon in polygons\n                ]\n                highres_image = self.processor.image_processor(highres_image)\n                slices = slice_polys_from_image(highres_image, scaled_polygons)\n                res_scales = [(width_scaler, height_scaler) for _ in range(len(slices))]\n            else:\n                image = self.processor.image_processor(image)\n                slices = slice_polys_from_image(image, polygons)\n                res_scales = [(1, 1) for _ in range(len(slices))]\n\n            slice_map.append(len(slices))\n            all_slices.extend(slices)\n            all_polygons.extend(polygons)\n            all_task_names.extend([task_name] * len(slices))\n            all_res_scales.extend(res_scales)\n\n        assert (\n            len(all_slices)\n            == sum(slice_map)\n            == len(all_polygons)\n            == len(all_task_names)\n            == len(all_res_scales)\n        )\n\n        return {\n            \"slices\": all_slices,\n            \"slice_map\": slice_map,\n            \"polygons\": all_polygons,\n            \"task_names\": all_task_names,\n            \"input_text\": [None] * len(all_slices),\n            \"res_scales\": all_res_scales,\n        }\n\n    def slice_bboxes(\n        self,\n        images: List[Image.Image],\n        task_names: List[str],\n        bboxes: List[List[List[int]]] | None = None,\n        polygons: List[List[List[List[int]]]] | None = None,\n        input_text: List[List[str | None]] | None = None,\n    ) -> dict:\n        assert bboxes is not None or polygons is not None\n        slice_map = []\n        all_slices = []\n        all_polygons = []\n        all_text = []\n        all_task_names = []\n\n        for idx, image in enumerate(images):\n            image = self.processor.image_processor(image)\n            if polygons is not None:\n                polys = polygons[idx]\n                slices = slice_polys_from_image(image, polys)\n            else:\n                slices = slice_bboxes_from_image(image, bboxes[idx])\n                polys = [\n                    [\n                        [bbox[0], bbox[1]],\n                        [bbox[2], bbox[1]],\n                        [bbox[2], bbox[3]],\n                        [bbox[0], bbox[3]],\n                    ]\n                    for bbox in bboxes[idx]\n                ]\n            slice_map.append(len(slices))\n            all_slices.extend(slices)\n            all_polygons.extend(polys)\n            all_task_names.extend([task_names[idx]] * len(slices))\n\n            if input_text is None:\n                all_text.extend([None] * len(slices))\n            else:\n                all_text.extend(input_text[idx])\n\n        assert (\n            len(all_slices)\n            == sum(slice_map)\n            == len(all_polygons)\n            == len(all_text)\n            == len(all_task_names)\n        ), (\n            f\"Mismatch in lengths: {len(all_slices)}, {sum(slice_map)}, {len(all_polygons)}, {len(all_text)}, {len(all_task_names)}\"\n        )\n\n        return {\n            \"slices\": all_slices,\n            \"slice_map\": slice_map,\n            \"polygons\": all_polygons,\n            \"input_text\": all_text,\n            \"task_names\": all_task_names,\n            \"res_scales\": [(1, 1) for _ in range(len(all_slices))],\n        }\n\n    def prepare_input(\n        self,\n        task_names: List[str],\n        images: List[Image.Image],\n        input_text: List[str | None],\n        math_modes: List[bool],\n    ):\n        batch = []\n        for image, text, task_name, math_mode in zip(\n            images, input_text, task_names, math_modes\n        ):\n            image_size = self.tasks[task_name][\"img_size\"]\n\n            try:\n                image = self.processor.scale_to_fit(\n                    image, image_size\n                )  # Only resizes if out of bounds (max/min)\n            except cv2.error:\n                # The image is empty if it can't be resized, so just make a blank image\n                image = np.zeros((image_size[1], image_size[0], 3), dtype=np.float32)\n\n            # Task input is the same for all tasks for now\n            text = text or \"\"\n\n            # Remove input text that exceeds max generation tokens (likely invalid)\n            if len(text) > self.tasks[task_name][\"max_tokens\"]:\n                text = \"\"\n            inputs = [\n                {\"type\": \"image\", \"image\": image, \"rotated\": False},\n                {\"type\": \"text\", \"text\": text.strip(), \"math\": math_mode},\n            ]\n            batch.append({\"task\": task_name, \"inputs\": inputs})\n\n        return batch\n\n    def process_outputs(self, outputs: SuryaModelOutput) -> ContinuousBatchOutput:\n        # Get logits and initial preds\n        next_token_logits = outputs[\"lm_logits\"][:, -1:, :].clone().float()\n        next_bbox_logits = outputs[\"bbox_logits\"][:, -1:, :].clone().float()\n        preds = torch.argmax(next_token_logits, dim=-1)\n\n        # Handle inference completion\n        done = (preds == self.processor.eos_token_id) | (\n            preds == self.processor.pad_token_id\n        )\n        done = done.squeeze(-1)\n        # If this batch item is done, input a pad token\n        input_ids = torch.where(done.unsqueeze(1), self.device_pad_token, preds).to(\n            torch.long\n        )\n\n        # Confidence score for the current token\n        scores = torch.max(F.softmax(next_token_logits[:, -1], dim=-1), dim=-1).values\n        scores = scores.masked_fill(done, 0).unsqueeze(1)\n\n        # Update input boxes\n        box_preds = next_bbox_logits * self.model.config.bbox_size\n        input_boxes = box_preds.to(torch.long)\n\n        return ContinuousBatchOutput(\n            input_ids=input_ids,\n            preds=preds,\n            bbox_preds=input_boxes,\n            done=done,\n            scores=scores,\n        )\n\n    def decode(self, current_inputs: Optional[ContinuousBatchInput] = None):\n        input_ids = current_inputs.input_ids\n        attention_mask = current_inputs.attention_mask\n        position_ids = current_inputs.position_ids\n\n        with settings.INFERENCE_MODE():\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                use_cache=True,\n                past_key_values=self.kv_cache,\n                logits_to_keep=1,\n            )\n\n        processed_output: ContinuousBatchOutput = self.process_outputs(outputs)\n\n        attention_mask = F.pad(attention_mask, (0, 1), mode=\"constant\", value=1)\n\n        position_ids = position_ids[:, -1:] + 1\n        new_input = ContinuousBatchInput(\n            input_ids=processed_output.input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n        )\n\n        return new_input, processed_output\n\n    def prefill(self, current_inputs: Optional[ContinuousBatchInput] = None):\n        logger.debug(f\"Prefilling {self.num_empty_slots} slots\")\n        prompts: List[RecognitionPrompt] = [\n            self.prompt_queue.popleft()\n            for _ in range(min(self.num_empty_slots, len(self.prompt_queue)))\n        ]\n\n        batch_input = self.prepare_input(\n            task_names=[p.task_name for p in prompts],\n            images=[p.image for p in prompts],\n            input_text=[p.text for p in prompts],\n            math_modes=[\n                p.math_mode for p in prompts\n            ],  # Pass math mode to the processor\n        )\n        processed_inputs = self.processor(\n            batch_input, padding_side=\"left\", device=self.model.device\n        ).to(device=self.model.device)\n\n        input_ids = processed_inputs[\"input_ids\"].to(dtype=torch.long)\n        image_tiles = processed_inputs[\"image_tiles\"].to(dtype=self.model.dtype)\n        grid_thw = processed_inputs[\"grid_thw\"].to(dtype=torch.long)\n        attention_mask = processed_inputs[\"attention_mask\"].to(dtype=torch.long)\n        position_ids = processed_inputs[\"position_ids\"].to(dtype=torch.long)\n\n        if settings.RECOGNITION_MODEL_QUANTIZE:\n            try:\n                import hqq  # noqa: F401\n            except Exception:\n                raise ImportError(\n                    \"Please install hqq to use quantized recognition model\"\n                )\n\n        # Use quantized cache if setting activated\n        cache_config = QuantizedCacheConfig(\n            \"HQQ\", 8, 1, 1, device=self.model.device, compute_dtype=self.model.dtype\n        )\n        prefill_cache = (\n            ContinuousBatchingCache()\n            if not settings.RECOGNITION_MODEL_QUANTIZE\n            else ContinuousBatchingQuantizedCache(cache_config)\n        )\n\n        with settings.INFERENCE_MODE():\n            outputs = self.model(\n                input_ids=input_ids,\n                image_tiles=image_tiles,\n                grid_thw=grid_thw,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                inputs_embeds=None,\n                past_key_values=prefill_cache,\n                use_cache=True,\n                logits_to_keep=1,\n                encoder_chunk_size=self.get_encoder_chunk_size(),\n            )\n\n        # Process outputs\n        processed_outputs = self.process_outputs(outputs)\n\n        # Merge new kv cache with existing, update batch mapping\n        non_active_idxs = [k for k, v in self.batch_prompt_mapping.items() if v is None]\n        idxs_to_merge = non_active_idxs[: len(prompts)]\n\n        assert len(idxs_to_merge) == len(prompts), (\n            \"Number of prompts should match number of empty slots\"\n        )\n        for i, prompt in zip(idxs_to_merge, prompts):\n            self.batch_prompt_mapping[i] = prompt.id\n\n        if self.kv_cache:\n            offset = self.kv_cache.merge(\n                prefill_cache, idxs_to_merge, self.model.device\n            )\n        else:\n            self.kv_cache = prefill_cache\n            offset = 0\n\n        # Adjust attention mask and position ids to account for the newly generated tokens\n        attention_mask = F.pad(attention_mask, (0, 1), mode=\"constant\", value=1)\n        position_ids = position_ids[:, -1:] + 1\n\n        if current_inputs is None:\n            new_input = ContinuousBatchInput(\n                input_ids=processed_outputs.input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n            )\n\n            return (\n                new_input,\n                processed_outputs,\n                range(processed_outputs.input_ids.shape[0]),\n            )\n\n        # Merging input_ids, attention masks and position ids\n        current_input_ids = current_inputs.input_ids\n        current_input_ids[idxs_to_merge] = processed_outputs.input_ids\n\n        current_attention_mask = current_inputs.attention_mask\n        if offset > 0:\n            attention_mask = F.pad(attention_mask, (offset, 0), value=0)\n        elif offset < 0:\n            current_attention_mask = F.pad(\n                current_attention_mask, (abs(offset), 0), value=0\n            )\n        current_attention_mask[idxs_to_merge] = attention_mask\n\n        current_position_ids = current_inputs.position_ids\n        current_position_ids[idxs_to_merge] = position_ids\n\n        new_input = ContinuousBatchInput(\n            input_ids=current_input_ids,\n            attention_mask=current_attention_mask,\n            position_ids=current_position_ids,\n        )\n\n        return new_input, processed_outputs, idxs_to_merge\n\n    # Due to continuous batching, we left pad the attention mask and cache to match new sequences\n    # This function trims the attention mask and the kv cache from the left whenever possible to remove excess padding\n    def maybe_trim_cache_padding(self, current_inputs: ContinuousBatchInput):\n        attention_mask = current_inputs.attention_mask\n        active_idxs = [k for k, v in self.batch_prompt_mapping.items() if v is not None]\n\n        # No more samples running\n        if not active_idxs:\n            return current_inputs\n\n        active_attention_mask = attention_mask[active_idxs]\n        first_non_padding_idx = (active_attention_mask == 1).to(torch.int).argmax(dim=1)\n        trim_start = first_non_padding_idx.min()\n\n        # Trimming too much slows things down\n        if trim_start < self.min_trim_length:\n            return current_inputs\n\n        logger.debug(f\"Trimming cache from left by {trim_start} tokens.\")\n        trimmed_attention_mask = attention_mask[:, trim_start:]\n        current_inputs.attention_mask = trimmed_attention_mask\n\n        # Trim the cache accordingly\n        if self.kv_cache:\n            self.kv_cache.trim_left(trim_start)\n\n        return current_inputs\n\n    def prediction_loop(\n        self,\n        flat: dict,\n        recognition_batch_size: int | None = None,\n        math_mode: bool = True,\n    ) -> tuple:\n        predicted_tokens = [[] for _ in range(len(flat[\"slices\"]))]\n        scores = [[] for _ in range(len(flat[\"slices\"]))]\n\n        if recognition_batch_size is None:\n            recognition_batch_size = self.get_batch_size()\n        current_inputs = None\n        self.setup_cache(recognition_batch_size)\n\n        batch_max_tokens = {}\n        for idx, (img, txt, task) in enumerate(\n            zip(flat[\"slices\"], flat[\"input_text\"], flat[\"task_names\"])\n        ):\n            self.prompt_queue.append(\n                RecognitionPrompt(\n                    id=idx, task_name=task, text=txt, image=img, math_mode=math_mode\n                )\n            )\n            batch_max_tokens[idx] = (\n                settings.RECOGNITION_MAX_TOKENS or self.tasks[task][\"max_tokens\"]\n            )\n\n        overall_max_tokens = max(batch_max_tokens.values())\n\n        pbar = tqdm(\n            total=len(self.prompt_queue),\n            desc=\"Recognizing Text\",\n            disable=self.disable_tqdm,\n        )\n\n        batch_bboxes = torch.zeros(len(flat[\"slices\"]), overall_max_tokens, 6)\n        batch_pos = [0] * len(flat[\"slices\"])\n\n        while self.prompt_queue or self.num_active_slots > 0:\n            if (\n                self.num_empty_slots / recognition_batch_size\n            ) > self.min_prefill_ratio and self.prompt_queue:\n                updated_inputs, outputs, merge_idxs = self.prefill(current_inputs)\n\n                predicted_tokens_cpu = outputs.preds.cpu()\n                scores_cpu = outputs.scores.cpu()\n                for temp_idx, b_idx in enumerate(merge_idxs):\n                    if self.batch_prompt_mapping[b_idx] is not None:\n                        p_idx = self.batch_prompt_mapping[b_idx]\n                        predicted_tokens[p_idx].append(\n                            predicted_tokens_cpu[temp_idx].item()\n                        )\n                        batch_bboxes[p_idx, batch_pos[p_idx]] = outputs.bbox_preds[\n                            temp_idx\n                        ][0]\n                        batch_pos[p_idx] += 1\n                        scores[p_idx].append(scores_cpu[temp_idx].item())\n\n                        if predicted_tokens[p_idx][-1] in [\n                            self.processor.eos_token_id,\n                            self.processor.no_output_token,\n                        ]:\n                            self.batch_prompt_mapping[b_idx] = None\n                            pbar.update(1)\n            else:\n                updated_inputs, outputs = self.decode(current_inputs)\n                # TODO Find a cleaner way of popping from the dict\n                predicted_tokens_cpu = outputs.preds.cpu()\n                scores_cpu = outputs.scores.cpu()\n\n                for b_idx, p_idx in self.batch_prompt_mapping.items():\n                    if p_idx is not None:\n                        predicted_tokens[p_idx].append(\n                            predicted_tokens_cpu[b_idx].item()\n                        )\n                        batch_bboxes[p_idx, batch_pos[p_idx]] = outputs.bbox_preds[\n                            b_idx\n                        ][0]\n                        batch_pos[p_idx] += 1\n\n                        scores[p_idx].append(scores_cpu[b_idx].item())\n\n                        repeats = len(predicted_tokens[p_idx]) >= batch_max_tokens[\n                            p_idx\n                        ] or detect_repeat_token(predicted_tokens[p_idx])\n                        if (\n                            predicted_tokens[p_idx][-1]\n                            in [\n                                self.processor.eos_token_id,\n                                self.processor.pad_token_id,\n                            ]\n                            or repeats\n                        ):\n                            self.batch_prompt_mapping[b_idx] = None\n                            pbar.update(1)\n\n            # Update inputs and mark XLA step\n            current_inputs = updated_inputs\n            current_inputs = self.maybe_trim_cache_padding(current_inputs)\n            mark_step()\n        pbar.close()\n\n        del self.kv_cache\n        self.kv_cache = None\n        torch.cuda.empty_cache()\n\n        return predicted_tokens, batch_bboxes, scores\n\n    def get_bboxes_text(\n        self,\n        flat: dict,\n        predicted_tokens: list,\n        scores: list,\n        predicted_polygons: list,\n        drop_repeated_text: bool = False,\n    ) -> list:\n        char_predictions = []\n        needs_boxes = [\n            self.tasks[task_name][\"needs_bboxes\"] for task_name in flat[\"task_names\"]\n        ]\n\n        for slice_idx, (\n            slice_image,\n            image_tokens,\n            image_polygons,\n            image_scores,\n            needs_box,\n        ) in enumerate(\n            zip(\n                flat[\"slices\"],\n                predicted_tokens,\n                predicted_polygons,\n                scores,\n                needs_boxes,\n            )\n        ):\n            blank_bbox = [[0, 0], [0, 1], [1, 1], [1, 0]]\n            if self.processor.no_output_token in image_tokens:\n                char_predictions.append(None)\n                continue\n\n            # If the image is very out of distribution, we can get nonsense repeats, and we may need to drop the text entirely\n            if drop_repeated_text and detect_repeat_token(image_tokens):\n                char_predictions.append(\n                    [\n                        TextChar(\n                            text=\"\",\n                            polygon=blank_bbox,\n                            confidence=0,\n                            bbox_valid=False,\n                        )\n                    ]\n                )\n                continue\n\n            image_polygons = image_polygons[: len(image_tokens)].cpu().numpy().tolist()\n\n            detokenize_sequences = []\n            detokenize_sequence = []\n            past_char_qwen_token = False\n\n            def _add_detokenize_sequence(\n                qwen_token: bool,\n                past_char_qwen_token: bool,\n                special_token: bool,\n                past_special_token: bool,\n                force: bool = False,\n            ):\n                nonlocal detokenize_sequence, detokenize_sequences\n\n                if (\n                    qwen_token != past_char_qwen_token\n                    or force\n                    or special_token\n                    or past_special_token\n                ) and detokenize_sequence:\n                    chars = [dt[0] for dt in detokenize_sequence]\n                    scores = [dt[1] for dt in detokenize_sequence]\n                    bboxes = [dt[2] for dt in detokenize_sequence]\n\n                    if past_char_qwen_token:\n                        detokenize_sequences.append((chars, scores, None, \"qwen\"))\n                    elif past_special_token:\n                        detokenize_sequences.append((chars, scores, None, \"special\"))\n                    else:\n                        detokenize_sequences.append((chars, scores, bboxes, \"ocr\"))\n\n                    detokenize_sequence = []\n\n            # Split up into sequences to detokenize separately\n            past_special_token = False\n            for bbox, char_id, score in zip(image_polygons, image_tokens, image_scores):\n                if char_id in [\n                    self.processor.eos_token_id,\n                    self.processor.pad_token_id,\n                ]:\n                    break\n\n                qwen_token = char_id < self.processor.ocr_tokenizer.qwen_offset\n                special_token = (\n                    self.processor.ocr_tokenizer.qwen_offset\n                    <= char_id\n                    < self.processor.ocr_tokenizer.special_token_offset\n                )\n                _add_detokenize_sequence(\n                    qwen_token, past_char_qwen_token, special_token, past_special_token\n                )\n                detokenize_sequence.append((char_id, score, bbox))\n                past_char_qwen_token = qwen_token\n                past_special_token = special_token\n\n            _add_detokenize_sequence(\n                False, past_char_qwen_token, False, past_special_token, force=True\n            )\n\n            img_chars = []\n            for sequence in detokenize_sequences:\n                token_ids, seq_score, bboxes, token_type = sequence\n                if token_type == \"ocr\":\n                    text = self.processor.ocr_tokenizer.decode(\n                        token_ids, task=TaskNames.ocr_with_boxes\n                    )\n                    bboxes = clean_close_polygons(\n                        bboxes\n                    )  # clean out bboxes that are close, like what happens with multiple utf-16 tokens per char\n                    bbox_idx = 0\n                    for text_idx, text_line in enumerate(text):\n                        img_chars.append(\n                            TextChar(\n                                text=text_line,\n                                polygon=bboxes[bbox_idx],\n                                confidence=seq_score[bbox_idx],\n                                bbox_valid=True,\n                            )\n                        )\n\n                        # Ensure we don't exceed the bbox count\n                        # Use the last bbox for the rest of the text\n                        if bbox_idx < len(bboxes) - 1:\n                            bbox_idx += 1\n                elif token_type == \"special\":\n                    text = self.processor.ocr_tokenizer.decode(\n                        token_ids, task=\"ocr_without_boxes\"\n                    )\n                    if text in [NOMATH_TOKEN] or re.match(r\"<SCRIPT-\\w+>\", text):\n                        continue\n\n                    img_chars.append(\n                        TextChar(\n                            text=text,\n                            polygon=blank_bbox,\n                            confidence=seq_score[0],\n                            bbox_valid=False,\n                        )\n                    )\n                else:\n                    text = self.processor.ocr_tokenizer.decode(\n                        token_ids, task=TaskNames.block_without_boxes\n                    )\n                    img_chars.append(\n                        TextChar(\n                            text=text,\n                            polygon=blank_bbox,\n                            confidence=seq_score[0],\n                            bbox_valid=False,\n                        )\n                    )\n\n            char_predictions.append(img_chars)\n\n        return char_predictions\n\n    def __call__(\n        self,\n        images: List[Image.Image],\n        task_names: List[str] | None = None,\n        det_predictor: DetectionPredictor | None = None,\n        detection_batch_size: int | None = None,\n        recognition_batch_size: int | None = None,\n        highres_images: List[Image.Image] | None = None,\n        bboxes: List[List[List[int]]] | None = None,\n        polygons: List[List[List[List[int]]]] | None = None,\n        input_text: List[List[str | None]] | None = None,\n        sort_lines: bool = False,\n        math_mode: bool = True,\n        return_words: bool = False,\n        drop_repeated_text: bool = False,\n    ) -> List[OCRResult]:\n        allowed_tasks = self.tasks.keys()\n        if task_names is None:\n            task_names = [TaskNames.ocr_with_boxes] * len(images)\n\n        assert all([task_name in allowed_tasks for task_name in task_names]), (\n            f\"One or more tasks in {task_names} is not supported. Supported tasks are {allowed_tasks}\"\n        )\n        assert len(images) == len(task_names), (\n            \"You need to pass in one task name for each image\"\n        )\n\n        images = convert_if_not_rgb(images)\n        if highres_images is not None:\n            assert len(images) == len(highres_images), (\n                \"You need to pass in one highres image for each image\"\n            )\n\n        highres_images = (\n            convert_if_not_rgb(highres_images)\n            if highres_images is not None\n            else [None] * len(images)\n        )\n\n        if bboxes is None and polygons is None:\n            assert det_predictor is not None, (\n                \"You need to pass in a detection predictor if you don't provide bboxes or polygons\"\n            )\n\n            # Detect then slice\n            flat = self.detect_and_slice_bboxes(\n                images,\n                task_names,\n                det_predictor,\n                detection_batch_size=detection_batch_size,\n                highres_images=highres_images,\n            )\n        else:\n            if bboxes is not None:\n                assert len(images) == len(bboxes), (\n                    \"You need to pass in one list of bboxes for each image\"\n                )\n            if polygons is not None:\n                assert len(images) == len(polygons), (\n                    \"You need to pass in one list of polygons for each image\"\n                )\n\n            flat = self.slice_bboxes(\n                images,\n                bboxes=bboxes,\n                polygons=polygons,\n                input_text=input_text,\n                task_names=task_names,\n            )\n\n        # No images passed, or no boxes passed, or no text detected in the images\n        if len(flat[\"slices\"]) == 0:\n            return []\n\n        # Sort by line widths. Negative so that longer images come first, fits in with continuous batching better\n        sorted_pairs = sorted(enumerate(flat[\"slices\"]), key=lambda x: -x[1].shape[1])\n        indices, sorted_slices = zip(*sorted_pairs)\n\n        # Reorder input_text and task_names based on the new order\n        flat[\"slices\"] = list(sorted_slices)\n        flat[\"input_text\"] = [flat[\"input_text\"][i] for i in indices]\n        flat[\"task_names\"] = [flat[\"task_names\"][i] for i in indices]\n\n        # Make predictions\n        predicted_tokens, batch_bboxes, scores = self.prediction_loop(\n            flat, recognition_batch_size=recognition_batch_size, math_mode=math_mode\n        )\n\n        # Get text and bboxes in structured form\n        bbox_size = self.model.config.bbox_size\n        image_sizes = [img.shape for img in flat[\"slices\"]]\n        predicted_polygons = prediction_to_polygon_batch(\n            batch_bboxes, image_sizes, bbox_size, bbox_size // 2\n        )\n        char_predictions = self.get_bboxes_text(\n            flat,\n            predicted_tokens,\n            scores,\n            predicted_polygons,\n            drop_repeated_text=drop_repeated_text,\n        )\n\n        char_predictions = sorted(zip(indices, char_predictions), key=lambda x: x[0])\n        char_predictions = [pred for _, pred in char_predictions]\n\n        predictions_by_image = []\n        slice_start = 0\n        for idx, image in enumerate(images):\n            slice_end = slice_start + flat[\"slice_map\"][idx]\n            image_lines = char_predictions[slice_start:slice_end]\n            polygons = flat[\"polygons\"][slice_start:slice_end]\n            res_scales = flat[\"res_scales\"][slice_start:slice_end]\n            slice_start = slice_end\n\n            lines = []\n            for text_line, polygon, res_scale in zip(image_lines, polygons, res_scales):\n                # Special case when input text is good\n                if not text_line:\n                    lines.append(\n                        TextLine(\n                            text=\"\",\n                            polygon=polygon,\n                            chars=[],\n                            confidence=1,\n                            original_text_good=True,\n                        )\n                    )\n                else:\n                    confidence = (\n                        float(np.mean([char.confidence for char in text_line]))\n                        if len(text_line) > 0\n                        else 0\n                    )\n                    poly_box = PolygonBox(polygon=polygon)\n                    for char in text_line:\n                        char.rescale(\n                            res_scale, (1, 1)\n                        )  # Rescale from highres if needed\n                        char.shift(\n                            poly_box.bbox[0], poly_box.bbox[1]\n                        )  # Ensure character boxes match line boxes (relative to page)\n                        char.clamp(poly_box.bbox)\n\n                    text_line = fix_unbalanced_tags(\n                        text_line, self.processor.ocr_tokenizer.special_tokens\n                    )\n                    text = \"\".join([char.text for char in text_line])\n                    text = unwrap_math(text)\n                    text = clean_math_tags(text)\n                    lines.append(\n                        TextLine(\n                            text=text,\n                            polygon=polygon,\n                            chars=text_line,\n                            confidence=confidence,\n                            words=words_from_chars(text_line, poly_box)\n                            if return_words\n                            else [],\n                        )\n                    )\n\n            if sort_lines:\n                lines = sort_text_lines(lines)\n            predictions_by_image.append(\n                OCRResult(\n                    text_lines=lines, image_bbox=[0, 0, image.size[0], image.size[1]]\n                )\n            )\n\n        return predictions_by_image\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}