{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/first-order-model",
    "API_Calls": [
        {
            "Name": "OcclusionAwareGenerator_call",
            "Description": "call OcclusionAwareGenerator to generate animations based on source images and driving videos.",
            "Code": "import sys\nimport yaml\nfrom argparse import ArgumentParser\nfrom tqdm.auto import tqdm\n\nimport imageio\nimport numpy as np\nfrom skimage.transform import resize\nfrom skimage import img_as_ubyte\nimport torch\nfrom sync_batchnorm import DataParallelWithCallback\n\nfrom modules.generator import OcclusionAwareGenerator\nfrom modules.keypoint_detector import KPDetector\nfrom animate import normalize_kp\n\nimport ffmpeg\nfrom os.path import splitext\nfrom shutil import copyfileobj\nfrom tempfile import NamedTemporaryFile\n\nif sys.version_info[0] < 3:\n    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n\ndef load_checkpoints(config_path, checkpoint_path, cpu=False):\n\n    with open(config_path) as f:\n        config = yaml.full_load(f)\n\n    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n                                        **config['model_params']['common_params'])\n    if not cpu:\n        generator.cuda()\n\n    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n                             **config['model_params']['common_params'])\n    if not cpu:\n        kp_detector.cuda()\n\n    if cpu:\n        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    else:\n        checkpoint = torch.load(checkpoint_path)\n\n    generator.load_state_dict(checkpoint['generator'])\n    kp_detector.load_state_dict(checkpoint['kp_detector'])\n\n    if not cpu:\n        generator = DataParallelWithCallback(generator)\n        kp_detector = DataParallelWithCallback(kp_detector)\n\n    generator.eval()\n    kp_detector.eval()\n\n    return generator, kp_detector\n\n\ndef make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n    with torch.no_grad():\n        predictions = []\n        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n        if not cpu:\n            source = source.cuda()\n        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n        kp_source = kp_detector(source)\n        kp_driving_initial = kp_detector(driving[:, :, 0])\n\n        for frame_idx in tqdm(range(driving.shape[2])):\n            driving_frame = driving[:, :, frame_idx]\n            if not cpu:\n                driving_frame = driving_frame.cuda()\n            kp_driving = kp_detector(driving_frame)\n            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n\n            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n    return predictions\n\ndef find_best_frame(source, driving, cpu=False):\n    import face_alignment  # type: ignore (local file)\n    from scipy.spatial import ConvexHull\n\n    def normalize_kp(kp):\n        kp = kp - kp.mean(axis=0, keepdims=True)\n        area = ConvexHull(kp[:, :2]).volume\n        area = np.sqrt(area)\n        kp[:, :2] = kp[:, :2] / area\n        return kp\n\n    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True,\n                                      device='cpu' if cpu else 'cuda')\n    kp_source = fa.get_landmarks(255 * source)[0]\n    kp_source = normalize_kp(kp_source)\n    norm  = float('inf')\n    frame_num = 0\n    for i, image in tqdm(enumerate(driving)):\n        kp_driving = fa.get_landmarks(255 * image)[0]\n        kp_driving = normalize_kp(kp_driving)\n        new_norm = (np.abs(kp_source - kp_driving) ** 2).sum()\n        if new_norm < norm:\n            norm = new_norm\n            frame_num = i\n    return frame_num\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--config\", required=True, help=\"path to config\")\n    parser.add_argument(\"--checkpoint\", default='vox-cpk.pth.tar', help=\"path to checkpoint to restore\")\n\n    parser.add_argument(\"--source_image\", default='sup-mat/source.png', help=\"path to source image\")\n    parser.add_argument(\"--driving_video\", default='driving.mp4', help=\"path to driving video\")\n    parser.add_argument(\"--result_video\", default='result.mp4', help=\"path to output\")\n\n    parser.add_argument(\"--relative\", dest=\"relative\", action=\"store_true\", help=\"use relative or absolute keypoint coordinates\")\n    parser.add_argument(\"--adapt_scale\", dest=\"adapt_scale\", action=\"store_true\", help=\"adapt movement scale based on convex hull of keypoints\")\n\n    parser.add_argument(\"--find_best_frame\", dest=\"find_best_frame\", action=\"store_true\",\n                        help=\"Generate from the frame that is the most alligned with source. (Only for faces, requires face_aligment lib)\")\n\n    parser.add_argument(\"--best_frame\", dest=\"best_frame\", type=int, default=None, help=\"Set frame to start from.\")\n\n    parser.add_argument(\"--cpu\", dest=\"cpu\", action=\"store_true\", help=\"cpu mode.\")\n\n    parser.add_argument(\"--audio\", dest=\"audio\", action=\"store_true\", help=\"copy audio to output from the driving video\" )\n\n    parser.set_defaults(relative=False)\n    parser.set_defaults(adapt_scale=False)\n    parser.set_defaults(audio_on=False)\n\n    opt = parser.parse_args()\n\n    source_image = imageio.imread(opt.source_image)\n    reader = imageio.get_reader(opt.driving_video)\n    fps = reader.get_meta_data()['fps']\n    driving_video = []\n    try:\n        for im in reader:\n            driving_video.append(im)\n    except RuntimeError:\n        pass\n    reader.close()\n\n    source_image = resize(source_image, (256, 256))[..., :3]\n    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n    generator, kp_detector = load_checkpoints(config_path=opt.config, checkpoint_path=opt.checkpoint, cpu=opt.cpu)\n\n    if opt.find_best_frame or opt.best_frame is not None:\n        i = opt.best_frame if opt.best_frame is not None else find_best_frame(source_image, driving_video, cpu=opt.cpu)\n        print(\"Best frame: \" + str(i))\n        driving_forward = driving_video[i:]\n        driving_backward = driving_video[:(i+1)][::-1]\n        predictions_forward = make_animation(source_image, driving_forward, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n        predictions_backward = make_animation(source_image, driving_backward, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n        predictions = predictions_backward[::-1] + predictions_forward[1:]\n    else:\n        predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n    imageio.mimsave(opt.result_video, [img_as_ubyte(frame) for frame in predictions], fps=fps)\n\n    if opt.audio:\n        try:\n            with NamedTemporaryFile(suffix=splitext(opt.result_video)[1]) as output:\n                ffmpeg.output(ffmpeg.input(opt.result_video).video, ffmpeg.input(opt.driving_video).audio, output.name, c='copy').run()\n                with open(opt.result_video, 'wb') as result:\n                    copyfileobj(output, result)\n        except ffmpeg.Error:\n            print(\"Failed to copy audio: the driving video may have no audio track or the audio format is invalid.\")\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/first-order-model/demo.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "OcclusionAwareGenerator",
            "Description": "OcclusionAwareGenerator it is based on the first-order-model project, which is used to generate animations by applying movements from driving videos to source images. The generator uses keypoint detection to align the source and driving frames, allowing for realistic animation generation.",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/first-order-model/modules/generator.py",
            "Implementation": "import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom modules.util import ResBlock2d, SameBlock2d, UpBlock2d, DownBlock2d\nfrom modules.dense_motion import DenseMotionNetwork\n\n\nclass OcclusionAwareGenerator(nn.Module):\n    \"\"\"\n    Generator that given source image and and keypoints try to transform image according to movement trajectories\n    induced by keypoints. Generator follows Johnson architecture.\n    \"\"\"\n\n    def __init__(self, num_channels, num_kp, block_expansion, max_features, num_down_blocks,\n                 num_bottleneck_blocks, estimate_occlusion_map=False, dense_motion_params=None, estimate_jacobian=False):\n        super(OcclusionAwareGenerator, self).__init__()\n\n        if dense_motion_params is not None:\n            self.dense_motion_network = DenseMotionNetwork(num_kp=num_kp, num_channels=num_channels,\n                                                           estimate_occlusion_map=estimate_occlusion_map,\n                                                           **dense_motion_params)\n        else:\n            self.dense_motion_network = None\n\n        self.first = SameBlock2d(num_channels, block_expansion, kernel_size=(7, 7), padding=(3, 3))\n\n        down_blocks = []\n        for i in range(num_down_blocks):\n            in_features = min(max_features, block_expansion * (2 ** i))\n            out_features = min(max_features, block_expansion * (2 ** (i + 1)))\n            down_blocks.append(DownBlock2d(in_features, out_features, kernel_size=(3, 3), padding=(1, 1)))\n        self.down_blocks = nn.ModuleList(down_blocks)\n\n        up_blocks = []\n        for i in range(num_down_blocks):\n            in_features = min(max_features, block_expansion * (2 ** (num_down_blocks - i)))\n            out_features = min(max_features, block_expansion * (2 ** (num_down_blocks - i - 1)))\n            up_blocks.append(UpBlock2d(in_features, out_features, kernel_size=(3, 3), padding=(1, 1)))\n        self.up_blocks = nn.ModuleList(up_blocks)\n\n        self.bottleneck = torch.nn.Sequential()\n        in_features = min(max_features, block_expansion * (2 ** num_down_blocks))\n        for i in range(num_bottleneck_blocks):\n            self.bottleneck.add_module('r' + str(i), ResBlock2d(in_features, kernel_size=(3, 3), padding=(1, 1)))\n\n        self.final = nn.Conv2d(block_expansion, num_channels, kernel_size=(7, 7), padding=(3, 3))\n        self.estimate_occlusion_map = estimate_occlusion_map\n        self.num_channels = num_channels\n\n    def deform_input(self, inp, deformation):\n        _, h_old, w_old, _ = deformation.shape\n        _, _, h, w = inp.shape\n        if h_old != h or w_old != w:\n            deformation = deformation.permute(0, 3, 1, 2)\n            deformation = F.interpolate(deformation, size=(h, w), mode='bilinear')\n            deformation = deformation.permute(0, 2, 3, 1)\n        return F.grid_sample(inp, deformation)\n\n    def forward(self, source_image, kp_driving, kp_source):\n        # Encoding (downsampling) part\n        out = self.first(source_image)\n        for i in range(len(self.down_blocks)):\n            out = self.down_blocks[i](out)\n\n        # Transforming feature representation according to deformation and occlusion\n        output_dict = {}\n        if self.dense_motion_network is not None:\n            dense_motion = self.dense_motion_network(source_image=source_image, kp_driving=kp_driving,\n                                                     kp_source=kp_source)\n            output_dict['mask'] = dense_motion['mask']\n            output_dict['sparse_deformed'] = dense_motion['sparse_deformed']\n\n            if 'occlusion_map' in dense_motion:\n                occlusion_map = dense_motion['occlusion_map']\n                output_dict['occlusion_map'] = occlusion_map\n            else:\n                occlusion_map = None\n            deformation = dense_motion['deformation']\n            out = self.deform_input(out, deformation)\n\n            if occlusion_map is not None:\n                if out.shape[2] != occlusion_map.shape[2] or out.shape[3] != occlusion_map.shape[3]:\n                    occlusion_map = F.interpolate(occlusion_map, size=out.shape[2:], mode='bilinear')\n                out = out * occlusion_map\n\n            output_dict[\"deformed\"] = self.deform_input(source_image, deformation)\n\n        # Decoding part\n        out = self.bottleneck(out)\n        for i in range(len(self.up_blocks)):\n            out = self.up_blocks[i](out)\n        out = self.final(out)\n        out = F.sigmoid(out)\n\n        output_dict[\"prediction\"] = out\n\n        return output_dict\n",
            "Examples": [
                "from modules.generator import OcclusionAwareGenerator\nimport torch\ngenerator = OcclusionAwareGenerator(num_channels=3, num_kp=10, block_expansion=32,\n                                     max_features=512, num_down_blocks=4, num_bottleneck_blocks=2,\n                                     estimate_occlusion_map=True, dense_motion_params=None,\n                                     estimate_jacobian=False)\ngenerator.eval()\nsource_image = torch.randn(1, 3, 256, 256)\ndriving_video = torch.randn(1, 3, 256, 256)\nkp_source = torch.randn(1, 10, 2)\nkp_driving = torch.randn(1, 10, 2)\nout = generator(source_image, kp_source, kp_driving)\nprint(out['prediction'].shape)\n"
            ]
        }
    ]
}