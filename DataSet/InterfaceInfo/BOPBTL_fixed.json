{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/Bringing-Old-Photos-Back-to-Life/Global",
    "API_Calls": [
        {
            "Name": "repair_image",
            "Code": "import os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nfrom options.test_options import TestOptions\nfrom models.models import create_model\nfrom models.mapping_model import Pix2PixHDModel_Mapping\nimport util.util as util\nfrom PIL import Image\nimport torch\nimport torchvision.utils as vutils\nimport torchvision.transforms as transforms\nimport numpy as np\nimport cv2\n\ndef data_transforms(img, method=Image.BILINEAR, scale=False):\n    ow, oh = img.size; pw, ph = ow, oh\n    if scale == True:\n        if ow < oh:\n            ow = 256; oh = ph / pw * 256\n        else:\n            oh = 256; ow = pw / ph * 256\n    h = int(round(oh / 4) * 4); w = int(round(ow / 4) * 4)\n    return img if (h == ph) and (w == pw) else img.resize((w, h), method)\n\ndef data_transforms_rgb_old(img):\n    w, h = img.size; A = img\n    if w < 256 or h < 256: A = transforms.Scale(256, Image.BILINEAR)(img)\n    return transforms.CenterCrop(256)(A)\n\ndef irregular_hole_synthesize(img, mask):\n    img_np = np.array(img).astype(\"uint8\"); mask_np = np.array(mask).astype(\"uint8\")\n    mask_np = mask_np / 255; img_new = img_np * (1 - mask_np) + mask_np * 255\n    return Image.fromarray(img_new.astype(\"uint8\")).convert(\"RGB\")\n\ndef parameter_set(opt):\n    opt.serial_batches = True; opt.no_flip = True; opt.label_nc = 0\n    opt.n_downsample_global = 3; opt.mc = 64; opt.k_size = 4; opt.start_r = 1\n    opt.mapping_n_block = 6; opt.map_mc = 512; opt.no_instance = True; opt.checkpoints_dir = \"./checkpoints/restoration\"\n    if opt.Quality_restore:\n        opt.name = \"mapping_quality\"\n        opt.load_pretrainA = os.path.join(opt.checkpoints_dir, \"VAE_A_quality\")\n        opt.load_pretrainB = os.path.join(opt.checkpoints_dir, \"VAE_B_quality\")\n    if opt.Scratch_and_Quality_restore:\n        opt.NL_res = True; opt.use_SN = True; opt.correlation_renormalize = True; opt.NL_use_mask = True\n        opt.NL_fusion_method = \"combine\"; opt.non_local = \"Setting_42\"; opt.name = \"mapping_scratch\"\n        opt.load_pretrainA = os.path.join(opt.checkpoints_dir, \"VAE_A_quality\")\n        opt.load_pretrainB = os.path.join(opt.checkpoints_dir, \"VAE_B_scratch\")\n        if opt.HR:\n            opt.mapping_exp = 1; opt.inference_optimize = True; opt.mask_dilation = 3; opt.name = \"mapping_Patch_Attention\"\n\nif __name__ == \"__main__\":\n    opt = TestOptions().parse(save=False); parameter_set(opt)\n    model = Pix2PixHDModel_Mapping(); model.initialize(opt); model.eval()\n    for path in [\"input_image\", \"restored_image\", \"origin\"]:\n        os.makedirs(opt.outputs_dir + \"/\" + path, exist_ok=True)\n    input_loader = sorted(os.listdir(opt.test_input)); dataset_size = len(input_loader)\n    mask_loader = sorted(os.listdir(opt.test_mask)) if opt.test_mask != \"\" else []\n    img_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    mask_transform = transforms.ToTensor()\n    for i in range(dataset_size):\n        input_name = input_loader[i]; input_file = os.path.join(opt.test_input, input_name)\n        if not os.path.isfile(input_file): print(f\"Skipping non-file {input_name}\"); continue\n        input = Image.open(input_file).convert(\"RGB\"); print(f\"Now you are processing {input_name}\")\n        if opt.NL_use_mask:\n            mask_name = mask_loader[i]; mask = Image.open(os.path.join(opt.test_mask, mask_name)).convert(\"RGB\")\n            if opt.mask_dilation != 0:\n                kernel = np.ones((3,3),np.uint8); mask = np.array(mask)\n                mask = cv2.dilate(mask,kernel,iterations = opt.mask_dilation); mask = Image.fromarray(mask.astype('uint8'))\n            origin = input; input = irregular_hole_synthesize(input, mask)\n            mask = mask_transform(mask)[:1, :, :].unsqueeze(0); input = img_transform(input).unsqueeze(0)\n        else:\n            if opt.test_mode == \"Scale\": input = data_transforms(input, scale=True)\n            elif opt.test_mode == \"Full\": input = data_transforms(input, scale=False)\n            elif opt.test_mode == \"Crop\": input = data_transforms_rgb_old(input)\n            origin = input; input = img_transform(input).unsqueeze(0); mask = torch.zeros_like(input)\n        try:\n            with torch.no_grad(): generated = model.inference(input, mask)\n        except Exception as ex: print(f\"Skip {input_name} due to an error:\\n{str(ex)}\"); continue\n        output_name = input_name[:-4] + \".png\" if input_name.endswith(\".jpg\") else input_name\n        vutils.save_image((input + 1.0) / 2.0, f\"{opt.outputs_dir}/input_image/{output_name}\", nrow=1, padding=0, normalize=True)\n        vutils.save_image((generated.data.cpu() + 1.0) / 2.0, f\"{opt.outputs_dir}/restored_image/{output_name}\", nrow=1, padding=0, normalize=True)\n        origin.save(f\"{opt.outputs_dir}/origin/{output_name}\")",
            "Description": "调用模型进行推理，对照片进行全局修复",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Bringing-Old-Photos-Back-to-Life/Global/test.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class Pix2PixHDModel_Mapping(BaseModel)",
            "Description": "这是一个类接口，这里还给出了这个类的父类，可以通过调用类方法interface进行照片的全面修复",
            "Implementation": "class Mapping_Model(nn.Module):\n    def __init__(self, nc, mc=64, n_blocks=3, norm=\"instance\", padding_type=\"reflect\", opt=None):\n        super(Mapping_Model, self).__init__()\n        norm_layer = networks.get_norm_layer(norm_type=norm)\n        activation = nn.ReLU(True)\n        model = []\n        tmp_nc = 64\n        n_up = 4\n        print(\"Mapping: You are using the mapping model without global restoration.\")\n        for i in range(n_up):\n            ic = min(tmp_nc * (2 ​** i), mc)\n            oc = min(tmp_nc * (2 ​** (i + 1)), mc)\n            model += [nn.Conv2d(ic, oc, 3, 1, 1), norm_layer(oc), activation]\n        for i in range(n_blocks):\n            model += [\n                networks.ResnetBlock(\n                    mc,\n                    padding_type=padding_type,\n                    activation=activation,\n                    norm_layer=norm_layer,\n                    opt=opt,\n                    dilation=opt.mapping_net_dilation,\n                )\n            ]\n        for i in range(n_up - 1):\n            ic = min(64 * (2 ​** (4 - i)), mc)\n            oc = min(64 * (2 ​** (3 - i)), mc)\n            model += [nn.Conv2d(ic, oc, 3, 1, 1), norm_layer(oc), activation]\n        model += [nn.Conv2d(tmp_nc * 2, tmp_nc, 3, 1, 1)]\n        if opt.feat_dim > 0 and opt.feat_dim < 64:\n            model += [norm_layer(tmp_nc), activation, nn.Conv2d(tmp_nc, opt.feat_dim, 1, 1)]\n        self.model = nn.Sequential(*model)\n    def forward(self, input):\n        return self.model(input)\n\nclass Pix2PixHDModel_Mapping(BaseModel):\n    def name(self):\n        return \"Pix2PixHDModel_Mapping\"\n    def init_loss_filter(self, use_gan_feat_loss, use_vgg_loss, use_smooth_l1, stage_1_feat_l2):\n        flags = (True, True, use_gan_feat_loss, use_vgg_loss, True, True, use_smooth_l1, stage_1_feat_l2)\n        def loss_filter(g_feat_l2, g_gan, g_gan_feat, g_vgg, d_real, d_fake, smooth_l1, stage_1_feat_l2):\n            return [l for (l, f) in zip((g_feat_l2, g_gan, g_gan_feat, g_vgg, d_real, d_fake, smooth_l1, stage_1_feat_l2), flags) if f]\n        return loss_filter\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        if opt.resize_or_crop != \"none\" or not opt.isTrain:\n            torch.backends.cudnn.benchmark = True\n        self.isTrain = opt.isTrain\n        input_nc = opt.label_nc if opt.label_nc != 0 else opt.input_nc\n        self.netG_A = networks.GlobalGenerator_DCDCv2(netG_input_nc, opt.output_nc, opt.ngf, opt.k_size, opt.n_downsample_global, networks.get_norm_layer(norm_type=opt.norm), opt=opt)\n        self.netG_B = networks.GlobalGenerator_DCDCv2(netG_input_nc, opt.output_nc, opt.ngf, opt.k_size, opt.n_downsample_global, networks.get_norm_layer(norm_type=opt.norm), opt=opt)\n        if opt.non_local == \"Setting_42\" or opt.NL_use_mask:\n            self.mapping_net = Mapping_Model_with_mask_2(min(opt.ngf * 2 ​** opt.n_downsample_global, opt.mc), opt.map_mc, n_blocks=opt.mapping_n_block, opt=opt) if opt.mapping_exp==1 else Mapping_Model_with_mask(min(opt.ngf * 2 ​** opt.n_downsample_global, opt.mc), opt.map_mc, n_blocks=opt.mapping_n_block, opt=opt)\n        else:\n            self.mapping_net = Mapping_Model(min(opt.ngf * 2 ​** opt.n_downsample_global, opt.mc), opt.map_mc, n_blocks=opt.mapping_n_block, opt=opt)\n        self.mapping_net.apply(networks.weights_init)\n        if opt.load_pretrain != \"\":\n            self.load_network(self.mapping_net, \"mapping_net\", opt.which_epoch, opt.load_pretrain)\n        if not opt.no_load_VAE:\n            self.load_network(self.netG_A, \"G\", opt.use_vae_which_epoch, opt.load_pretrainA)\n            self.load_network(self.netG_B, \"G\", opt.use_vae_which_epoch, opt.load_pretrainB)\n            for param in self.netG_A.parameters():\n                param.requires_grad = False\n            for param in self.netG_B.parameters():\n                param.requires_grad = False\n            self.netG_A.eval()\n            self.netG_B.eval()\n        if opt.gpu_ids:\n            self.netG_A.cuda(opt.gpu_ids[0])\n            self.netG_B.cuda(opt.gpu_ids[0])\n            self.mapping_net.cuda(opt.gpu_ids[0])\n        if not self.isTrain:\n            self.load_network(self.mapping_net, \"mapping_net\", opt.which_epoch)\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            netD_input_nc = opt.ngf * 2 if opt.feat_gan else input_nc + opt.output_nc\n            if not opt.no_instance:\n                netD_input_nc += 1\n            self.netD = networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt, opt.norm, use_sigmoid, opt.num_D, not opt.no_ganFeat_loss, gpu_ids=self.gpu_ids)\n            self.fake_pool = ImagePool(opt.pool_size)\n            self.old_lr = opt.lr\n            self.loss_filter = self.init_loss_filter(not opt.no_ganFeat_loss, not opt.no_vgg_loss, opt.Smooth_L1, opt.use_two_stage_mapping)\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            self.criterionFeat = torch.nn.L1Loss()\n            self.criterionFeat_feat = torch.nn.L1Loss() if opt.use_l1_feat else torch.nn.MSELoss()\n            self.criterionImage = torch.nn.L1Loss() if self.opt.image_L1 else torch.nn.SmoothL1Loss()\n            if not opt.no_vgg_loss:\n                self.criterionVGG = networks.VGGLoss_torch(self.gpu_ids)\n            self.loss_names = self.loss_filter('G_Feat_L2', 'G_GAN', 'G_GAN_Feat', 'G_VGG','D_real', 'D_fake', 'Smooth_L1', 'G_Feat_L2_Stage_1')\n            if opt.no_TTUR:\n                beta1,beta2=opt.beta1,0.999\n                G_lr,D_lr=opt.lr,opt.lr\n            else:\n                beta1,beta2=0,0.9\n                G_lr,D_lr=opt.lr/2,opt.lr*2\n            if not opt.no_load_VAE:\n                params = list(self.mapping_net.parameters())\n                self.optimizer_mapping = torch.optim.Adam(params, lr=G_lr, betas=(beta1, beta2))\n            params = list(self.netD.parameters())\n            self.optimizer_D = torch.optim.Adam(params, lr=D_lr, betas=(beta1, beta2))\n    def encode_input(self, label_map, inst_map=None, real_image=None, feat_map=None, infer=False):\n        if self.opt.label_nc == 0:\n            input_label = label_map.data.cuda()\n        else:\n            size = label_map.size()\n            oneHot_size = (size[0], self.opt.label_nc, size[2], size[3])\n            input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n            input_label = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n            if self.opt.data_type == 16:\n                input_label = input_label.half()\n        if not self.opt.no_instance:\n            inst_map = inst_map.data.cuda()\n            edge_map = self.get_edges(inst_map)\n            input_label = torch.cat((input_label, edge_map), dim=1)\n        input_label = Variable(input_label, volatile=infer)\n        if real_image is not None:\n            real_image = Variable(real_image.data.cuda())\n        return input_label, inst_map, real_image, feat_map\n    def discriminate(self, input_label, test_image, use_pool=False):\n        input_concat = torch.cat((input_label, test_image.detach()), dim=1)\n        return self.netD.forward(self.fake_pool.query(input_concat) if use_pool else input_concat)\n    def forward(self, label, inst, image, feat, pair=True, infer=False, last_label=None, last_image=None):\n        input_label, inst_map, real_image, feat_map = self.encode_input(label, inst, image, feat)\n        input_concat = input_label\n        label_feat = self.netG_A.forward(input_concat, flow='enc')\n        if self.opt.NL_use_mask:\n            label_feat_map=self.mapping_net(label_feat.detach(),inst)\n        else:\n            label_feat_map = self.mapping_net(label_feat.detach())\n        fake_image = self.netG_B.forward(label_feat_map, flow='dec')\n        image_feat = self.netG_B.forward(real_image, flow='enc')\n        loss_feat_l2_stage_1=0\n        loss_feat_l2 = self.criterionFeat_feat(label_feat_map, image_feat.data) * self.opt.l2_feat\n        if self.opt.feat_gan:\n            pred_fake_pool = self.discriminate(label_feat.detach(), label_feat_map, use_pool=True)\n            loss_D_fake = self.criterionGAN(pred_fake_pool, False)\n            pred_real = self.discriminate(label_feat.detach(), image_feat)\n            loss_D_real = self.criterionGAN(pred_real, True)\n            pred_fake = self.netD.forward(torch.cat((label_feat.detach(), label_feat_map), dim=1))\n            loss_G_GAN = self.criterionGAN(pred_fake, True)\n        else:\n            pred_fake_pool = self.discriminate(input_label, fake_image, use_pool=True)\n            loss_D_fake = self.criterionGAN(pred_fake_pool, False)\n            pred_real = self.discriminate(input_label, real_image) if pair else self.discriminate(last_label, last_image)\n            loss_D_real = self.criterionGAN(pred_real, True)\n            pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1))\n            loss_G_GAN = self.criterionGAN(pred_fake, True)\n        loss_G_GAN_Feat = 0\n        if not self.opt.no_ganFeat_loss and pair:\n            feat_weights = 4.0 / (self.opt.n_layers_D + 1)\n            D_weights = 1.0 / self.opt.num_D\n            for i in range(self.opt.num_D):\n                for j in range(len(pred_fake[i])-1):\n                    tmp = self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat\n                    loss_G_GAN_Feat += D_weights * feat_weights * tmp\n        else:\n            loss_G_GAN_Feat = torch.zeros(1).to(label.device)\n        loss_G_VGG = self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat if pair and not self.opt.no_vgg_loss else torch.zeros(1).to(label.device)\n        smooth_l1_loss= self.criterionImage(fake_image,real_image)*self.opt.L1_weight if self.opt.Smooth_L1 else 0\n        return [ self.loss_filter(loss_feat_l2, loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake,smooth_l1_loss,loss_feat_l2_stage_1), None if not infer else fake_image ]\n    def inference(self, label, inst):\n        use_gpu = len(self.opt.gpu_ids) > 0\n        input_concat = label.data.cuda() if use_gpu else label.data\n        inst_data = inst.cuda() if use_gpu else inst\n        label_feat = self.netG_A.forward(input_concat, flow=\"enc\")\n        if self.opt.NL_use_mask and self.opt.inference_optimize:\n            label_feat_map=self.mapping_net.inference_forward(label_feat.detach(),inst_data)\n        elif self.opt.NL_use_mask:\n            label_feat_map = self.mapping_net(label_feat.detach(), inst_data)\n        else:\n            label_feat_map = self.mapping_net(label_feat.detach())\n        return self.netG_B.forward(label_feat_map, flow=\"dec\")",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Bringing-Old-Photos-Back-to-Life/Global/models/mapping_model.py",
            "Examples": [
                {
                    "Type": "Basic Usage",
                    "Code": "from options.test_options import TestOptions\nfrom models.models import create_model\nfrom models.mapping_model import Pix2PixHDModel_Mapping \nopt = TestOptions().parse(save=False)\nparameter_set(opt)\nmodel = Pix2PixHDModel_Mapping()\nmodel.initialize(opt)\nmodel.eval()\ngenerated = model.inference(input, mask)"
                }
            ]
        }
    ]
}