{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/gpt-engineer",
    "API_Calls": [
        {
            "Name": "AIcallstep",
            "Description": "主要用于通过使用AI类生成和改进代码。它提供了一系列函数，涵盖了从代码生成到执行、调试和逐步改进的完整流程\n",
            "Code": "\"\"\"\nAI Module\n\nThis module provides an AI class that interfaces with language models to perform various tasks such as\nstarting a conversation, advancing the conversation, and handling message serialization. It also includes\nbackoff strategies for handling rate limit errors from the OpenAI API.\n\nClasses:\n    AI: A class that interfaces with language models for conversation management and message serialization.\n\nFunctions:\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\n\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Union\n\nimport backoff\nimport openai\nimport pyperclip\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    messages_from_dict,\n    messages_to_dict,\n)\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_openai import AzureChatOpenAI, ChatOpenAI\n\nfrom gpt_engineer.core.token_usage import TokenUsageLog\n\n# Type hint for a chat message\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\n# Set up logging\nlogger = logging.getLogger(__name__)\n\n\nclass AI:\n    \"\"\"\n    A class that interfaces with language models for conversation management and message serialization.\n\n    This class provides methods to start and advance conversations, handle message serialization,\n    and implement backoff strategies for rate limit errors when interacting with the OpenAI API.\n\n    Attributes\n    ----------\n    temperature : float\n        The temperature setting for the language model.\n    azure_endpoint : str\n        The endpoint URL for the Azure-hosted language model.\n    model_name : str\n        The name of the language model to use.\n    streaming : bool\n        A flag indicating whether to use streaming for the language model.\n    llm : BaseChatModel\n        The language model instance for conversation management.\n    token_usage_log : TokenUsageLog\n        A log for tracking token usage during conversations.\n\n    Methods\n    -------\n    start(system: str, user: str, step_name: str) -> List[Message]\n        Start the conversation with a system message and a user message.\n    next(messages: List[Message], prompt: Optional[str], step_name: str) -> List[Message]\n        Advances the conversation by sending message history to LLM and updating with the response.\n    backoff_inference(messages: List[Message]) -> Any\n        Perform inference using the language model with an exponential backoff strategy.\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n    deserialize_messages(jsondictstr: str) -> List[Message]\n        Deserialize a JSON string to a list of messages.\n    _create_chat_model() -> BaseChatModel\n        Create a chat model with the specified model name and temperature.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name=\"gpt-4-turbo\",\n        temperature=0.1,\n        azure_endpoint=None,\n        streaming=True,\n        vision=False,\n    ):\n        \"\"\"\n        Initialize the AI class.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            The name of the model to use, by default \"gpt-4\".\n        temperature : float, optional\n            The temperature to use for the model, by default 0.1.\n        \"\"\"\n        self.temperature = temperature\n        self.azure_endpoint = azure_endpoint\n        self.model_name = model_name\n        self.streaming = streaming\n        self.vision = (\n            (\"vision-preview\" in model_name)\n            or (\"gpt-4-turbo\" in model_name and \"preview\" not in model_name)\n            or (\"claude\" in model_name)\n        )\n        self.llm = self._create_chat_model()\n        self.token_usage_log = TokenUsageLog(model_name)\n\n        logger.debug(f\"Using model {self.model_name}\")\n\n    def start(self, system: str, user: Any, *, step_name: str) -> List[Message]:\n        \"\"\"\n        Start the conversation with a system message and a user message.\n\n        Parameters\n        ----------\n        system : str\n            The content of the system message.\n        user : str\n            The content of the user message.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages in the conversation.\n        \"\"\"\n\n        messages: List[Message] = [\n            SystemMessage(content=system),\n            HumanMessage(content=user),\n        ]\n        return self.next(messages, step_name=step_name)\n\n    def _extract_content(self, content):\n        \"\"\"\n        Extracts text content from a message, supporting both string and list types.\n        Parameters\n        ----------\n        content : Union[str, List[dict]]\n            The content of a message, which could be a string or a list.\n        Returns\n        -------\n        str\n            The extracted text content.\n        \"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, list) and content and \"text\" in content[0]:\n            # Assuming the structure of list content is [{'type': 'text', 'text': 'Some text'}, ...]\n            return content[0][\"text\"]\n        else:\n            return \"\"\n\n    def _collapse_text_messages(self, messages: List[Message]):\n        \"\"\"\n        Combine consecutive messages of the same type into a single message, where if the message content\n        is a list type, the first text element's content is taken. This method keeps `combined_content` as a string.\n\n        This method iterates through the list of messages, combining consecutive messages of the same type\n        by joining their content with a newline character. If the content is a list, it extracts text from the first\n        text element's content. This reduces the number of messages and simplifies the conversation for processing.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to collapse.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages after collapsing consecutive messages of the same type.\n        \"\"\"\n        collapsed_messages = []\n        if not messages:\n            return collapsed_messages\n\n        previous_message = messages[0]\n        combined_content = self._extract_content(previous_message.content)\n\n        for current_message in messages[1:]:\n            if current_message.type == previous_message.type:\n                combined_content += \"\\n\\n\" + self._extract_content(\n                    current_message.content\n                )\n            else:\n                collapsed_messages.append(\n                    previous_message.__class__(content=combined_content)\n                )\n                previous_message = current_message\n                combined_content = self._extract_content(current_message.content)\n\n        collapsed_messages.append(previous_message.__class__(content=combined_content))\n        return collapsed_messages\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Advances the conversation by sending message history\n        to LLM and updating with the response.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages in the conversation.\n        prompt : Optional[str], optional\n            The prompt to use, by default None.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The updated list of messages in the conversation.\n        \"\"\"\n\n        if prompt:\n            messages.append(HumanMessage(content=prompt))\n\n        logger.debug(\n            \"Creating a new chat completion: %s\",\n            \"\\n\".join([m.pretty_repr() for m in messages]),\n        )\n\n        if not self.vision:\n            messages = self._collapse_text_messages(messages)\n\n        response = self.backoff_inference(messages)\n\n        self.token_usage_log.update_log(\n            messages=messages, answer=response.content, step_name=step_name\n        )\n        messages.append(response)\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        return messages\n\n    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=7, max_time=45)\n    def backoff_inference(self, messages):\n        \"\"\"\n        Perform inference using the language model while implementing an exponential backoff strategy.\n\n        This function will retry the inference in case of a rate limit error from the OpenAI API.\n        It uses an exponential backoff strategy, meaning the wait time between retries increases\n        exponentially. The function will attempt to retry up to 7 times within a span of 45 seconds.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of chat messages which will be passed to the language model for processing.\n\n        callbacks : List[Callable]\n            A list of callback functions that are triggered after each inference. These functions\n            can be used for logging, monitoring, or other auxiliary tasks.\n\n        Returns\n        -------\n        Any\n            The output from the language model after processing the provided messages.\n\n        Raises\n        ------\n        openai.error.RateLimitError\n            If the number of retries exceeds the maximum or if the rate limit persists beyond the\n            allotted time, the function will ultimately raise a RateLimitError.\n\n        Example\n        -------\n        >>> messages = [SystemMessage(content=\"Hello\"), HumanMessage(content=\"How's the weather?\")]\n        >>> response = backoff_inference(messages)\n        \"\"\"\n        return self.llm.invoke(messages)  # type: ignore\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        \"\"\"\n        Serialize a list of messages to a JSON string.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to serialize.\n\n        Returns\n        -------\n        str\n            The serialized messages as a JSON string.\n        \"\"\"\n        return json.dumps(messages_to_dict(messages))\n\n    @staticmethod\n    def deserialize_messages(jsondictstr: str) -> List[Message]:\n        \"\"\"\n        Deserialize a JSON string to a list of messages.\n\n        Parameters\n        ----------\n        jsondictstr : str\n            The JSON string to deserialize.\n\n        Returns\n        -------\n        List[Message]\n            The deserialized list of messages.\n        \"\"\"\n        data = json.loads(jsondictstr)\n        # Modify implicit is_chunk property to ALWAYS false\n        # since Langchain's Message schema is stricter\n        prevalidated_data = [\n            {**item, \"tools\": {**item.get(\"tools\", {}), \"is_chunk\": False}}\n            for item in data\n        ]\n        return list(messages_from_dict(prevalidated_data))  # type: ignore\n\n    def _create_chat_model(self) -> BaseChatModel:\n        \"\"\"\n        Create a chat model with the specified model name and temperature.\n\n        Parameters\n        ----------\n        model : str\n            The name of the model to create.\n        temperature : float\n            The temperature to use for the model.\n\n        Returns\n        -------\n        BaseChatModel\n            The created chat model.\n        \"\"\"\n        if self.azure_endpoint:\n            return AzureChatOpenAI(\n                azure_endpoint=self.azure_endpoint,\n                openai_api_version=os.getenv(\n                    \"OPENAI_API_VERSION\", \"2024-05-01-preview\"\n                ),\n                deployment_name=self.model_name,\n                openai_api_type=\"azure\",\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n        elif \"claude\" in self.model_name:\n            return ChatAnthropic(\n                model=self.model_name,\n                temperature=self.temperature,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                streaming=self.streaming,\n                max_tokens_to_sample=4096,\n            )\n        elif self.vision:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                max_tokens=4096,  # vision models default to low max token limits\n            )\n        else:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n\n\ndef serialize_messages(messages: List[Message]) -> str:\n    return AI.serialize_messages(messages)\n\n\nclass ClipboardAI(AI):\n    # Ignore not init superclass\n    def __init__(self, **_):  # type: ignore\n        self.vision = False\n        self.token_usage_log = TokenUsageLog(\"clipboard_llm\")\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        return \"\\n\\n\".join([f\"{m.type}:\\n{m.content}\" for m in messages])\n\n    @staticmethod\n    def multiline_input():\n        print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n        content = []\n        while True:\n            try:\n                line = input()\n            except EOFError:\n                break\n            content.append(line)\n        return \"\\n\".join(content)\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Not yet fully supported\n        \"\"\"\n        if prompt:\n            messages.append(HumanMessage(content=prompt))\n\n        logger.debug(f\"Creating a new chat completion: {messages}\")\n\n        msgs = self.serialize_messages(messages)\n        pyperclip.copy(msgs)\n        Path(\"clipboard.txt\").write_text(msgs)\n        print(\n            \"Messages copied to clipboard and written to clipboard.txt,\",\n            len(msgs),\n            \"characters in total\",\n        )\n\n        response = self.multiline_input()\n\n        messages.append(AIMessage(content=response))\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        return messages\n\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/gpt-engineer/gpt_engineer/core/steps.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class AI",
            "Description": "\nAI类的具体实现,该类与语言模型交互以执行各种任务，如启动会话、推进会话和处理消息序列化等",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/gpt-engineer/gpt_engineer/core/ai.py",
            "Implementation": "class AI:\n    \"\"\"\n    A class that interfaces with language models for conversation management and message serialization.\n\n    This class provides methods to start and advance conversations, handle message serialization,\n    and implement backoff strategies for rate limit errors when interacting with the OpenAI API.\n\n    Attributes\n    ----------\n    temperature : float\n        The temperature setting for the language model.\n    azure_endpoint : str\n        The endpoint URL for the Azure-hosted language model.\n    model_name : str\n        The name of the language model to use.\n    streaming : bool\n        A flag indicating whether to use streaming for the language model.\n    llm : BaseChatModel\n        The language model instance for conversation management.\n    token_usage_log : TokenUsageLog\n        A log for tracking token usage during conversations.\n\n    Methods\n    -------\n    start(system: str, user: str, step_name: str) -> List[Message]\n        Start the conversation with a system message and a user message.\n    next(messages: List[Message], prompt: Optional[str], step_name: str) -> List[Message]\n        Advances the conversation by sending message history to LLM and updating with the response.\n    backoff_inference(messages: List[Message]) -> Any\n        Perform inference using the language model with an exponential backoff strategy.\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n    deserialize_messages(jsondictstr: str) -> List[Message]\n        Deserialize a JSON string to a list of messages.\n    _create_chat_model() -> BaseChatModel\n        Create a chat model with the specified model name and temperature.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name=\"gpt-4-turbo\",\n        temperature=0.1,\n        azure_endpoint=None,\n        streaming=True,\n        vision=False,\n    ):\n        \"\"\"\n        Initialize the AI class.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            The name of the model to use, by default \"gpt-4\".\n        temperature : float, optional\n            The temperature to use for the model, by default 0.1.\n        \"\"\"\n        self.temperature = temperature\n        self.azure_endpoint = azure_endpoint\n        self.model_name = model_name\n        self.streaming = streaming\n        self.vision = (\n            (\"vision-preview\" in model_name)\n            or (\"gpt-4-turbo\" in model_name and \"preview\" not in model_name)\n            or (\"claude\" in model_name)\n        )\n        self.llm = self._create_chat_model()\n        self.token_usage_log = TokenUsageLog(model_name)\n\n        logger.debug(f\"Using model {self.model_name}\")\n\n    def start(self, system: str, user: Any, *, step_name: str) -> List[Message]:\n        \"\"\"\n        Start the conversation with a system message and a user message.\n\n        Parameters\n        ----------\n        system : str\n            The content of the system message.\n        user : str\n            The content of the user message.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages in the conversation.\n        \"\"\"\n\n        messages: List[Message] = [\n            SystemMessage(content=system),\n            HumanMessage(content=user),\n        ]\n        return self.next(messages, step_name=step_name)\n\n    def _extract_content(self, content):\n        \"\"\"\n        Extracts text content from a message, supporting both string and list types.\n        Parameters\n        ----------\n        content : Union[str, List[dict]]\n            The content of a message, which could be a string or a list.\n        Returns\n        -------\n        str\n            The extracted text content.\n        \"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, list) and content and \"text\" in content[0]:\n            # Assuming the structure of list content is [{'type': 'text', 'text': 'Some text'}, ...]\n            return content[0][\"text\"]\n        else:\n            return \"\"\n\n    def _collapse_text_messages(self, messages: List[Message]):\n        \"\"\"\n        Combine consecutive messages of the same type into a single message, where if the message content\n        is a list type, the first text element's content is taken. This method keeps `combined_content` as a string.\n\n        This method iterates through the list of messages, combining consecutive messages of the same type\n        by joining their content with a newline character. If the content is a list, it extracts text from the first\n        text element's content. This reduces the number of messages and simplifies the conversation for processing.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to collapse.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages after collapsing consecutive messages of the same type.\n        \"\"\"\n        collapsed_messages = []\n        if not messages:\n            return collapsed_messages\n\n        previous_message = messages[0]\n        combined_content = self._extract_content(previous_message.content)\n\n        for current_message in messages[1:]:\n            if current_message.type == previous_message.type:\n                combined_content += \"\\n\\n\" + self._extract_content(\n                    current_message.content\n                )\n            else:\n                collapsed_messages.append(\n                    previous_message.__class__(content=combined_content)\n                )\n                previous_message = current_message\n                combined_content = self._extract_content(current_message.content)\n\n        collapsed_messages.append(previous_message.__class__(content=combined_content))\n        return collapsed_messages\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Advances the conversation by sending message history\n        to LLM and updating with the response.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages in the conversation.\n        prompt : Optional[str], optional\n            The prompt to use, by default None.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The updated list of messages in the conversation.\n        \"\"\"\n\n        if prompt:\n            messages.append(HumanMessage(content=prompt))\n\n        logger.debug(\n            \"Creating a new chat completion: %s\",\n            \"\\n\".join([m.pretty_repr() for m in messages]),\n        )\n\n        if not self.vision:\n            messages = self._collapse_text_messages(messages)\n\n        response = self.backoff_inference(messages)\n\n        self.token_usage_log.update_log(\n            messages=messages, answer=response.content, step_name=step_name\n        )\n        messages.append(response)\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        return messages\n\n    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=7, max_time=45)\n    def backoff_inference(self, messages):\n        \"\"\"\n        Perform inference using the language model while implementing an exponential backoff strategy.\n\n        This function will retry the inference in case of a rate limit error from the OpenAI API.\n        It uses an exponential backoff strategy, meaning the wait time between retries increases\n        exponentially. The function will attempt to retry up to 7 times within a span of 45 seconds.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of chat messages which will be passed to the language model for processing.\n\n        callbacks : List[Callable]\n            A list of callback functions that are triggered after each inference. These functions\n            can be used for logging, monitoring, or other auxiliary tasks.\n\n        Returns\n        -------\n        Any\n            The output from the language model after processing the provided messages.\n\n        Raises\n        ------\n        openai.error.RateLimitError\n            If the number of retries exceeds the maximum or if the rate limit persists beyond the\n            allotted time, the function will ultimately raise a RateLimitError.\n\n        Example\n        -------\n        >>> messages = [SystemMessage(content=\"Hello\"), HumanMessage(content=\"How's the weather?\")]\n        >>> response = backoff_inference(messages)\n        \"\"\"\n        return self.llm.invoke(messages)  # type: ignore\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        \"\"\"\n        Serialize a list of messages to a JSON string.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to serialize.\n\n        Returns\n        -------\n        str\n            The serialized messages as a JSON string.\n        \"\"\"\n        return json.dumps(messages_to_dict(messages))\n\n    @staticmethod\n    def deserialize_messages(jsondictstr: str) -> List[Message]:\n        \"\"\"\n        Deserialize a JSON string to a list of messages.\n\n        Parameters\n        ----------\n        jsondictstr : str\n            The JSON string to deserialize.\n\n        Returns\n        -------\n        List[Message]\n            The deserialized list of messages.\n        \"\"\"\n        data = json.loads(jsondictstr)\n        # Modify implicit is_chunk property to ALWAYS false\n        # since Langchain's Message schema is stricter\n        prevalidated_data = [\n            {**item, \"tools\": {**item.get(\"tools\", {}), \"is_chunk\": False}}\n            for item in data\n        ]\n        return list(messages_from_dict(prevalidated_data))  # type: ignore\n\n    def _create_chat_model(self) -> BaseChatModel:\n        \"\"\"\n        Create a chat model with the specified model name and temperature.\n\n        Parameters\n        ----------\n        model : str\n            The name of the model to create.\n        temperature : float\n            The temperature to use for the model.\n\n        Returns\n        -------\n        BaseChatModel\n            The created chat model.\n        \"\"\"\n        if self.azure_endpoint:\n            return AzureChatOpenAI(\n                azure_endpoint=self.azure_endpoint,\n                openai_api_version=os.getenv(\n                    \"OPENAI_API_VERSION\", \"2024-05-01-preview\"\n                ),\n                deployment_name=self.model_name,\n                openai_api_type=\"azure\",\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n        elif \"claude\" in self.model_name:\n            return ChatAnthropic(\n                model=self.model_name,\n                temperature=self.temperature,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                streaming=self.streaming,\n                max_tokens_to_sample=4096,\n            )\n        elif self.vision:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                max_tokens=4096,  # vision models default to low max token limits\n            )\n        else:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n\n",
            "Example": [
                "dasdasdasdasd\n\n",
                ""
            ]
        }
    ]
}