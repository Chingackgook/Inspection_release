{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/OOTDiffusion",
    "API_Calls": [
        {
            "Name": "run_ootd",
            "Description": "run_ootd",
            "Code": "from pathlib import Path\nimport sys\nfrom PIL import Image\nfrom utils_ootd import get_mask_location\n\nPROJECT_ROOT = Path(__file__).absolute().parents[1].absolute()\nsys.path.insert(0, str(PROJECT_ROOT))\n\nfrom preprocess.openpose.run_openpose import OpenPose\nfrom preprocess.humanparsing.run_parsing import Parsing\nfrom ootd.inference_ootd_hd import OOTDiffusionHD\nfrom ootd.inference_ootd_dc import OOTDiffusionDC\n\n\nimport argparse\nparser = argparse.ArgumentParser(description='run ootd')\nparser.add_argument('--gpu_id', '-g', type=int, default=0, required=False)\nparser.add_argument('--model_path', type=str, default=\"\", required=True)\nparser.add_argument('--cloth_path', type=str, default=\"\", required=True)\nparser.add_argument('--model_type', type=str, default=\"hd\", required=False)\nparser.add_argument('--category', '-c', type=int, default=0, required=False)\nparser.add_argument('--scale', type=float, default=2.0, required=False)\nparser.add_argument('--step', type=int, default=20, required=False)\nparser.add_argument('--sample', type=int, default=4, required=False)\nparser.add_argument('--seed', type=int, default=-1, required=False)\nargs = parser.parse_args()\n\n\nopenpose_model = OpenPose(args.gpu_id)\nparsing_model = Parsing(args.gpu_id)\n\n\ncategory_dict = ['upperbody', 'lowerbody', 'dress']\ncategory_dict_utils = ['upper_body', 'lower_body', 'dresses']\n\nmodel_type = args.model_type # \"hd\" or \"dc\"\ncategory = args.category # 0:upperbody; 1:lowerbody; 2:dress\ncloth_path = args.cloth_path\nmodel_path = args.model_path\n\nimage_scale = args.scale\nn_steps = args.step\nn_samples = args.sample\nseed = args.seed\n\nif model_type == \"hd\":\n    model = OOTDiffusionHD(args.gpu_id)\nelif model_type == \"dc\":\n    model = OOTDiffusionDC(args.gpu_id)\nelse:\n    raise ValueError(\"model_type must be \\'hd\\' or \\'dc\\'!\")\n\n\nif __name__ == '__main__':\n\n    if model_type == 'hd' and category != 0:\n        raise ValueError(\"model_type \\'hd\\' requires category == 0 (upperbody)!\")\n\n    cloth_img = Image.open(cloth_path).resize((768, 1024))\n    model_img = Image.open(model_path).resize((768, 1024))\n    keypoints = openpose_model(model_img.resize((384, 512)))\n    model_parse, _ = parsing_model(model_img.resize((384, 512)))\n\n    mask, mask_gray = get_mask_location(model_type, category_dict_utils[category], model_parse, keypoints)\n    mask = mask.resize((768, 1024), Image.NEAREST)\n    mask_gray = mask_gray.resize((768, 1024), Image.NEAREST)\n    \n    masked_vton_img = Image.composite(mask_gray, model_img, mask)\n    masked_vton_img.save('./images_output/mask.jpg')\n\n    images = model(\n        model_type=model_type,\n        category=category_dict[category],\n        image_garm=cloth_img,\n        image_vton=masked_vton_img,\n        mask=mask,\n        image_ori=model_img,\n        num_samples=n_samples,\n        num_steps=n_steps,\n        image_scale=image_scale,\n        seed=seed,\n    )\n\n    image_idx = 0\n    for image in images:\n        image.save('./images_output/out_' + model_type + '_' + str(image_idx) + '.png')\n        image_idx += 1\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/OOTDiffusion/run/run_ootd.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "OOTDiffusionHD",
            "Description": "OOTDiffusionHD",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/OOTDiffusion/ootd/inference_ootd_hd.py",
            "Implementation": "import pdb\nfrom pathlib import Path\nimport sys\nPROJECT_ROOT = Path(__file__).absolute().parents[0].absolute()\nsys.path.insert(0, str(PROJECT_ROOT))\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport cv2\n\nimport random\nimport time\nimport pdb\n\nfrom pipelines_ootd.pipeline_ootd import OotdPipeline\nfrom pipelines_ootd.unet_garm_2d_condition import UNetGarm2DConditionModel\nfrom pipelines_ootd.unet_vton_2d_condition import UNetVton2DConditionModel\nfrom diffusers import UniPCMultistepScheduler\nfrom diffusers import AutoencoderKL\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoProcessor, CLIPVisionModelWithProjection\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\nVIT_PATH = \"../checkpoints/clip-vit-large-patch14\"\nVAE_PATH = \"../checkpoints/ootd\"\nUNET_PATH = \"../checkpoints/ootd/ootd_hd/checkpoint-36000\"\nMODEL_PATH = \"../checkpoints/ootd\"\n\nclass OOTDiffusionHD:\n\n    def __init__(self, gpu_id):\n        self.gpu_id = 'cuda:' + str(gpu_id)\n\n        vae = AutoencoderKL.from_pretrained(\n            VAE_PATH,\n            subfolder=\"vae\",\n            torch_dtype=torch.float16,\n        )\n\n        unet_garm = UNetGarm2DConditionModel.from_pretrained(\n            UNET_PATH,\n            subfolder=\"unet_garm\",\n            torch_dtype=torch.float16,\n            use_safetensors=True,\n        )\n        unet_vton = UNetVton2DConditionModel.from_pretrained(\n            UNET_PATH,\n            subfolder=\"unet_vton\",\n            torch_dtype=torch.float16,\n            use_safetensors=True,\n        )\n\n        self.pipe = OotdPipeline.from_pretrained(\n            MODEL_PATH,\n            unet_garm=unet_garm,\n            unet_vton=unet_vton,\n            vae=vae,\n            torch_dtype=torch.float16,\n            variant=\"fp16\",\n            use_safetensors=True,\n            safety_checker=None,\n            requires_safety_checker=False,\n        ).to(self.gpu_id)\n\n        self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)\n        \n        self.auto_processor = AutoProcessor.from_pretrained(VIT_PATH)\n        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(VIT_PATH).to(self.gpu_id)\n\n        self.tokenizer = CLIPTokenizer.from_pretrained(\n            MODEL_PATH,\n            subfolder=\"tokenizer\",\n        )\n        self.text_encoder = CLIPTextModel.from_pretrained(\n            MODEL_PATH,\n            subfolder=\"text_encoder\",\n        ).to(self.gpu_id)\n\n\n    def tokenize_captions(self, captions, max_length):\n        inputs = self.tokenizer(\n            captions, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        return inputs.input_ids\n\n\n    def __call__(self,\n                model_type='hd',\n                category='upperbody',\n                image_garm=None,\n                image_vton=None,\n                mask=None,\n                image_ori=None,\n                num_samples=1,\n                num_steps=20,\n                image_scale=1.0,\n                seed=-1,\n    ):\n        if seed == -1:\n            random.seed(time.time())\n            seed = random.randint(0, 2147483647)\n        print('Initial seed: ' + str(seed))\n        generator = torch.manual_seed(seed)\n\n        with torch.no_grad():\n            prompt_image = self.auto_processor(images=image_garm, return_tensors=\"pt\").to(self.gpu_id)\n            prompt_image = self.image_encoder(prompt_image.data['pixel_values']).image_embeds\n            prompt_image = prompt_image.unsqueeze(1)\n            if model_type == 'hd':\n                prompt_embeds = self.text_encoder(self.tokenize_captions([\"\"], 2).to(self.gpu_id))[0]\n                prompt_embeds[:, 1:] = prompt_image[:]\n            elif model_type == 'dc':\n                prompt_embeds = self.text_encoder(self.tokenize_captions([category], 3).to(self.gpu_id))[0]\n                prompt_embeds = torch.cat([prompt_embeds, prompt_image], dim=1)\n            else:\n                raise ValueError(\"model_type must be \\'hd\\' or \\'dc\\'!\")\n\n            images = self.pipe(prompt_embeds=prompt_embeds,\n                        image_garm=image_garm,\n                        image_vton=image_vton, \n                        mask=mask,\n                        image_ori=image_ori,\n                        num_inference_steps=num_steps,\n                        image_guidance_scale=image_scale,\n                        num_images_per_prompt=num_samples,\n                        generator=generator,\n            ).images\n\n        return images\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}