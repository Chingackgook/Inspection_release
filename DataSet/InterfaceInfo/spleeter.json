{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/spleeter",
    "API_Calls": [
        {
            "Name": "Separator_call",
            "Description": "test Separator",
            "Code": "#!/usr/bin/env python\n# coding: utf8\n\n\"\"\" Unit testing for Separator class. \"\"\"\n\n__email__ = \"spleeter@deezer.com\"\n__author__ = \"Deezer Research\"\n__license__ = \"MIT License\"\n\nimport itertools\nfrom os.path import basename, exists, join, splitext\nfrom tempfile import TemporaryDirectory\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf  # type: ignore\n\nfrom spleeter import SpleeterError\nfrom spleeter.audio.adapter import AudioAdapter\nfrom spleeter.separator import Separator\n\nTEST_AUDIO_DESCRIPTORS = [\"audio_example.mp3\", \"audio_example_mono.mp3\"]\nMODELS = [\"spleeter:2stems\", \"spleeter:4stems\", \"spleeter:5stems\"]\n\nMODEL_TO_INST = {\n    \"spleeter:2stems\": (\"vocals\", \"accompaniment\"),\n    \"spleeter:4stems\": (\"vocals\", \"drums\", \"bass\", \"other\"),\n    \"spleeter:5stems\": (\"vocals\", \"drums\", \"bass\", \"piano\", \"other\"),\n}\n\n\nMODELS_AND_TEST_FILES = list(itertools.product(TEST_AUDIO_DESCRIPTORS, MODELS))\nTEST_CONFIGURATIONS = list(itertools.product(TEST_AUDIO_DESCRIPTORS, MODELS))\n\n\nprint(\"RUNNING TESTS WITH TF VERSION {}\".format(tf.__version__))\n\n\n@pytest.mark.parametrize(\"test_file, configuration\", TEST_CONFIGURATIONS)\ndef test_separate(test_file, configuration):\n    \"\"\"Test separation from raw data.\"\"\"\n    instruments = MODEL_TO_INST[configuration]\n    adapter = AudioAdapter.default()\n    waveform, _ = adapter.load(test_file)\n    separator = Separator(configuration, multiprocess=False)\n    prediction = separator.separate(waveform, test_file)\n    assert len(prediction) == len(instruments)\n    for instrument in instruments:\n        assert instrument in prediction\n    for instrument in instruments:\n        track = prediction[instrument]\n        assert waveform.shape[:-1] == track.shape[:-1]\n        assert not np.allclose(waveform, track)\n        for compared in instruments:\n            if instrument != compared:\n                assert not np.allclose(track, prediction[compared])\n\n\n@pytest.mark.parametrize(\"test_file, configuration\", TEST_CONFIGURATIONS)\ndef test_separate_to_file(test_file, configuration):\n    \"\"\"Test file based separation.\"\"\"\n    instruments = MODEL_TO_INST[configuration]\n    separator = Separator(configuration, multiprocess=False)\n    name = splitext(basename(test_file))[0]\n    with TemporaryDirectory() as directory:\n        separator.separate_to_file(test_file, directory)\n        for instrument in instruments:\n            assert exists(join(directory, \"{}/{}.wav\".format(name, instrument)))\n\n\n@pytest.mark.parametrize(\"test_file, configuration\", TEST_CONFIGURATIONS)\ndef test_filename_format(test_file, configuration):\n    \"\"\"Test custom filename format.\"\"\"\n    instruments = MODEL_TO_INST[configuration]\n    separator = Separator(configuration, multiprocess=False)\n    name = splitext(basename(test_file))[0]\n    with TemporaryDirectory() as directory:\n        separator.separate_to_file(\n            test_file,\n            directory,\n            filename_format=\"export/{filename}/{instrument}.{codec}\",\n        )\n        for instrument in instruments:\n            assert exists(join(directory, \"export/{}/{}.wav\".format(name, instrument)))\n\n\n@pytest.mark.parametrize(\"test_file, configuration\", MODELS_AND_TEST_FILES)\ndef test_filename_conflict(test_file, configuration):\n    \"\"\"Test error handling with static pattern.\"\"\"\n    separator = Separator(configuration, multiprocess=False)\n    with TemporaryDirectory() as directory:\n        with pytest.raises(SpleeterError):\n            separator.separate_to_file(\n                test_file, directory, filename_format=\"I wanna be your lover\"\n            )\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/spleeter/tests/test_separator.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class_Separator",
            "Description": "Separator用于执行音频源分离，它封装了深度学习模型的加载、推理和后处理流程，支持将音频文件分离为多个独立的音轨（如人声、伴奏等）。",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/spleeter/spleeter/separator.py",
            "Implementation": "#!/usr/bin/env python\n# coding: utf8\n\n\"\"\"\nModule that provides a class wrapper for source separation.\n\nExamples:\n\n```python\n>>> from spleeter.separator import Separator\n>>> separator = Separator('spleeter:2stems')\n>>> separator.separate(waveform, lambda instrument, data: ...)\n>>> separator.separate_to_file(...)\n```\n\"\"\"\n\nimport atexit\nimport os\nfrom multiprocessing import Pool\nfrom os.path import basename, dirname, join, splitext\nfrom typing import Any, Dict, Generator, List, Optional\n\n# pyright: reportMissingImports=false\n# pylint: disable=import-error\nimport numpy as np\nimport tensorflow as tf  # type: ignore\n\nfrom spleeter import SpleeterError\nfrom spleeter.audio import Codec\nfrom spleeter.audio.adapter import AudioAdapter\nfrom spleeter.audio.convertor import to_stereo\nfrom spleeter.model import EstimatorSpecBuilder, InputProviderFactory, model_fn\nfrom spleeter.model.provider import ModelProvider\nfrom spleeter.types import AudioDescriptor\nfrom spleeter.utils.configuration import load_configuration\n\n# pylint: enable=import-error\n\n__email__ = \"spleeter@deezer.com\"\n__author__ = \"Deezer Research\"\n__license__ = \"MIT License\"\n\n\ndef create_estimator(params: Dict, MWF: bool) -> tf.Tensor:\n    \"\"\"\n    Initialize tensorflow estimator that will perform separation\n\n    Parameters:\n        params (Dict):\n            A dictionary of parameters for building the model\n        MWF (bool):\n            Wiener filter enabled?\n\n    Returns:\n        tf.Tensor:\n            A tensorflow estimator\n    \"\"\"\n    # Load model.\n    provider: ModelProvider = ModelProvider.default()\n    params[\"model_dir\"] = provider.get(params[\"model_dir\"])\n    params[\"MWF\"] = MWF\n    # Setup config\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.7\n    config = tf.estimator.RunConfig(session_config=session_config)\n    # Setup estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=params[\"model_dir\"], params=params, config=config\n    )\n    return estimator\n\n\nclass Separator(object):\n    \"\"\"A wrapper class for performing separation.\"\"\"\n\n    def __init__(\n        self,\n        params_descriptor: str,\n        MWF: bool = False,\n        multiprocess: bool = True,\n    ) -> None:\n        \"\"\"\n        Default constructor.\n\n        Parameters:\n            params_descriptor (str):\n                Descriptor for TF params to be used.\n            MWF (bool):\n                (Optional) `True` if MWF should be used, `False` otherwise.\n            multiprocess (bool):\n                (Optional) Enable multi-processing.\n        \"\"\"\n        self._params = load_configuration(params_descriptor)\n        self._sample_rate = self._params[\"sample_rate\"]\n        self._MWF = MWF\n        self._tf_graph = tf.Graph()\n        self._prediction_generator: Optional[Generator] = None\n        self._input_provider = None\n        self._builder = None\n        self._features = None\n        self._session = None\n        if multiprocess:\n            self._pool: Optional[Any] = Pool()\n            atexit.register(self._pool.close)\n        else:\n            self._pool = None\n        self._tasks: List = []\n        self.estimator = None\n\n    def _get_prediction_generator(self, data: dict) -> Generator:\n        \"\"\"\n        Lazy loading access method for internal prediction generator\n        returned by the predict method of a tensorflow estimator.\n\n        Returns:\n            Generator:\n                Generator of prediction.\n        \"\"\"\n        if not self.estimator:\n            self.estimator = create_estimator(self._params, self._MWF)\n\n        def get_dataset():\n            return tf.data.Dataset.from_tensors(data)\n\n        return self.estimator.predict(get_dataset, yield_single_examples=False)\n\n    def join(self, timeout: int = 200) -> None:\n        \"\"\"\n        Wait for all pending tasks to be finished.\n\n        Parameters:\n            timeout (int):\n                (Optional) Task waiting timeout.\n        \"\"\"\n        while len(self._tasks) > 0:\n            task = self._tasks.pop()\n            task.get()\n            task.wait(timeout=timeout)\n\n    def _get_input_provider(self):\n        if self._input_provider is None:\n            self._input_provider = InputProviderFactory.get(self._params)\n        return self._input_provider\n\n    def _get_features(self):\n        if self._features is None:\n            provider = self._get_input_provider()\n            self._features = provider.get_input_dict_placeholders()\n        return self._features\n\n    def _get_builder(self):\n        if self._builder is None:\n            self._builder = EstimatorSpecBuilder(self._get_features(), self._params)\n        return self._builder\n\n    def _get_session(self):\n        if self._session is None:\n            saver = tf.compat.v1.train.Saver()\n            provider = ModelProvider.default()\n            model_directory: str = provider.get(self._params[\"model_dir\"])\n            latest_checkpoint = tf.train.latest_checkpoint(model_directory)\n            self._session = tf.compat.v1.Session()\n            saver.restore(self._session, latest_checkpoint)\n        return self._session\n\n    def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict:\n        \"\"\"\n        Performs source separation over the given waveform with tensorflow\n        backend.\n\n        Parameters:\n            waveform (np.ndarray):\n                Waveform to be separated (as a numpy array)\n            audio_descriptor (AudioDescriptor):\n                Audio descriptor to be used.\n\n        Returns:\n            Dict:\n                Separated waveforms.\n        \"\"\"\n        if not waveform.shape[-1] == 2:\n            waveform = to_stereo(waveform)\n        prediction_generator = self._get_prediction_generator(\n            {\"waveform\": waveform, \"audio_id\": np.array(audio_descriptor)}\n        )\n        # NOTE: perform separation.\n        prediction = next(prediction_generator)\n        prediction.pop(\"audio_id\")\n        return prediction\n\n    def separate(\n        self, waveform: np.ndarray, audio_descriptor: Optional[str] = \"\"\n    ) -> Dict:\n        \"\"\"\n        Performs separation on a waveform.\n\n        Parameters:\n            waveform (np.ndarray):\n                Waveform to be separated (as a numpy array)\n            audio_descriptor (Optional[str]):\n                (Optional) string describing the waveform (e.g. filename).\n\n        Returns:\n            Dict:\n                Separated waveforms.\n        \"\"\"\n        return self._separate_tensorflow(waveform, audio_descriptor)\n\n    def separate_to_file(\n        self,\n        audio_descriptor: AudioDescriptor,\n        destination: str,\n        audio_adapter: Optional[AudioAdapter] = None,\n        offset: float = 0,\n        duration: float = 600.0,\n        codec: Codec = Codec.WAV,\n        bitrate: str = \"128k\",\n        filename_format: str = \"{filename}/{instrument}.{codec}\",\n        synchronous: bool = True,\n    ) -> None:\n        \"\"\"\n        Performs source separation and export result to file using\n        given audio adapter.\n\n        Filename format should be a Python formattable string that could\n        use following parameters :\n\n        - {instrument}\n        - {filename}\n        - {foldername}\n        - {codec}.\n\n        Parameters:\n            audio_descriptor (AudioDescriptor):\n                Describe song to separate, used by audio adapter to\n                retrieve and load audio data, in case of file based\n                audio adapter, such descriptor would be a file path.\n            destination (str):\n                Target directory to write output to.\n            audio_adapter (AudioAdapter):\n                (Optional) Audio adapter to use for I/O.\n            offset (int):\n                (Optional) Offset of loaded song.\n            duration (float):\n                (Optional) Duration of loaded song (default: 600s).\n            codec (Codec):\n                (Optional) Export codec.\n            bitrate (str):\n                (Optional) Export bitrate.\n            filename_format (str):\n                (Optional) Filename format.\n            synchronous (bool):\n                (Optional) True is should by synchronous.\n        \"\"\"\n        if audio_adapter is None:\n            audio_adapter = AudioAdapter.default()\n        waveform, _ = audio_adapter.load(\n            audio_descriptor,\n            offset=offset,\n            duration=duration,\n            sample_rate=self._sample_rate,\n        )\n        sources = self.separate(waveform, audio_descriptor)\n        self.save_to_file(\n            sources,\n            audio_descriptor,\n            destination,\n            filename_format,\n            codec,\n            audio_adapter,\n            bitrate,\n            synchronous,\n        )\n\n    def save_to_file(\n        self,\n        sources: Dict,\n        audio_descriptor: AudioDescriptor,\n        destination: str,\n        filename_format: str = \"{filename}/{instrument}.{codec}\",\n        codec: Codec = Codec.WAV,\n        audio_adapter: Optional[AudioAdapter] = None,\n        bitrate: str = \"128k\",\n        synchronous: bool = True,\n    ) -> None:\n        \"\"\"\n        Export dictionary of sources to files.\n\n        Parameters:\n            sources (Dict):\n                Dictionary of sources to be exported. The keys are the name\n                of the instruments, and the values are `N x 2` numpy arrays\n                containing the corresponding intrument waveform, as\n                returned by the separate method\n            audio_descriptor (AudioDescriptor):\n                Describe song to separate, used by audio adapter to\n                retrieve and load audio data, in case of file based audio\n                adapter, such descriptor would be a file path.\n            destination (str):\n                Target directory to write output to.\n            filename_format (str):\n                (Optional) Filename format.\n            codec (Codec):\n                (Optional) Export codec.\n            audio_adapter (Optional[AudioAdapter]):\n                (Optional) Audio adapter to use for I/O.\n            bitrate (str):\n                (Optional) Export bitrate.\n            synchronous (bool):\n                (Optional) True is should by synchronous.\n        \"\"\"\n        if audio_adapter is None:\n            audio_adapter = AudioAdapter.default()\n        foldername = basename(dirname(audio_descriptor))\n        filename = splitext(basename(audio_descriptor))[0]\n        generated = []\n        for instrument, data in sources.items():\n            path = join(\n                destination,\n                filename_format.format(\n                    filename=filename,\n                    instrument=instrument,\n                    foldername=foldername,\n                    codec=codec,\n                ),\n            )\n            directory = os.path.dirname(path)\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            if path in generated:\n                raise SpleeterError(\n                    (\n                        f\"Separated source path conflict : {path},\"\n                        \"please check your filename format\"\n                    )\n                )\n            generated.append(path)\n            if self._pool:\n                task = self._pool.apply_async(\n                    audio_adapter.save, (path, data, self._sample_rate, codec, bitrate)\n                )\n                self._tasks.append(task)\n            else:\n                audio_adapter.save(path, data, self._sample_rate, codec, bitrate)\n        if synchronous and self._pool:\n            self.join()\n",
            "Example": [
                "from spleeter.separator import Separator\nfrom spleeter.audio.adapter import AudioAdapter\nimport os\n\ndef test_audio_separation():\n    # 定义要使用的模型参数描述符，这里使用 2stems 模型，将音频分离为声乐和伴奏\n    params_descriptor = \"spleeter:2stems\"\n    \n    # 创建 Separator 实例\n    separator = Separator(params_descriptor, multiprocess=False)\n\n    # 获取默认的音频适配器\n    audio_adapter = AudioAdapter.default()\n\n    # 定义要分离的音频文件路径\n    audio_file = \"audio_example.mp3\"\n    # 加载音频文件，返回音频波形和采样率\n    waveform, sample_rate = audio_adapter.load(audio_file)\n\n    # 进行音频分离\n    separated_sources = separator.separate(waveform, audio_file)\n\n    # 定义分离后音频文件的保存目录\n    output_directory = \"separated_output\"\n    if not os.path.exists(output_directory):\n        os.makedirs(output_directory)\n\n    # 保存分离后的音频文件\n    for instrument, data in separated_sources.items():\n        output_path = os.path.join(output_directory, f\"{instrument}.wav\")\n        audio_adapter.save(output_path, data, sample_rate)\n        print(f\"{instrument} 已保存到 {output_path}\")\n\nif __name__ == \"__main__\":\n    test_audio_separation()\n"
            ]
        }
    ]
}