{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/stable-dreamfusion",
    "API_Calls": [
        {
            "Name": "call_DPT",
            "Description": "call DPT",
            "Code": "import os\nimport sys\nimport cv2\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\n\n\nclass BackgroundRemoval():\n    def __init__(self, device='cuda'):\n\n        from carvekit.api.high import HiInterface\n        self.interface = HiInterface(\n            object_type=\"object\",  # Can be \"object\" or \"hairs-like\".\n            batch_size_seg=5,\n            batch_size_matting=1,\n            device=device,\n            seg_mask_size=640,  # Use 640 for Tracer B7 and 320 for U2Net\n            matting_mask_size=2048,\n            trimap_prob_threshold=231,\n            trimap_dilation=30,\n            trimap_erosion_iters=5,\n            fp16=True,\n        )\n\n    @torch.no_grad()\n    def __call__(self, image):\n        # image: [H, W, 3] array in [0, 255].\n        image = Image.fromarray(image)\n\n        image = self.interface([image])[0]\n        image = np.array(image)\n\n        return image\n\nclass BLIP2():\n    def __init__(self, device='cuda'):\n        self.device = device\n        from transformers import AutoProcessor, Blip2ForConditionalGeneration\n        self.processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n        self.model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16).to(device)\n\n    @torch.no_grad()\n    def __call__(self, image):\n        image = Image.fromarray(image)\n        inputs = self.processor(image, return_tensors=\"pt\").to(self.device, torch.float16)\n\n        generated_ids = self.model.generate(**inputs, max_new_tokens=20)\n        generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n\n        return generated_text\n\n\nclass DPT():\n    def __init__(self, task='depth', device='cuda'):\n\n        self.task = task\n        self.device = device\n\n        from dpt import DPTDepthModel\n\n        if task == 'depth':\n            path = 'pretrained/omnidata/omnidata_dpt_depth_v2.ckpt'\n            self.model = DPTDepthModel(backbone='vitb_rn50_384')\n            self.aug = transforms.Compose([\n                transforms.Resize((384, 384)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=0.5, std=0.5)\n            ])\n\n        else: # normal\n            path = 'pretrained/omnidata/omnidata_dpt_normal_v2.ckpt'\n            self.model = DPTDepthModel(backbone='vitb_rn50_384', num_channels=3)\n            self.aug = transforms.Compose([\n                transforms.Resize((384, 384)),\n                transforms.ToTensor()\n            ])\n\n        # load model\n        checkpoint = torch.load(path, map_location='cpu', weights_only=False)\n        if 'state_dict' in checkpoint:\n            state_dict = {}\n            for k, v in checkpoint['state_dict'].items():\n                state_dict[k[6:]] = v\n        else:\n            state_dict = checkpoint\n        self.model.load_state_dict(state_dict)\n        self.model.eval().to(device)\n\n\n    @torch.no_grad()\n    def __call__(self, image):\n        # image: np.ndarray, uint8, [H, W, 3]\n        H, W = image.shape[:2]\n        image = Image.fromarray(image)\n\n        image = self.aug(image).unsqueeze(0).to(self.device)\n\n        if self.task == 'depth':\n            depth = self.model(image).clamp(0, 1)\n            depth = F.interpolate(depth.unsqueeze(1), size=(H, W), mode='bicubic', align_corners=False)\n            depth = depth.squeeze(1).cpu().numpy()\n            return depth\n        else:\n            normal = self.model(image).clamp(0, 1)\n            normal = F.interpolate(normal, size=(H, W), mode='bicubic', align_corners=False)\n            normal = normal.cpu().numpy()\n            return normal\n\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('path', type=str, help=\"path to image (png, jpeg, etc.)\")\n    parser.add_argument('--size', default=256, type=int, help=\"output resolution\")\n    parser.add_argument('--border_ratio', default=0.2, type=float, help=\"output border ratio\")\n    parser.add_argument('--recenter', type=bool, default=True, help=\"recenter, potentially not helpful for multiview zero123\")\n    parser.add_argument('--dont_recenter', dest='recenter', action='store_false')\n    opt = parser.parse_args()\n\n    out_dir = os.path.dirname(opt.path)\n    out_rgba = os.path.join(out_dir, os.path.basename(opt.path).split('.')[0] + '_rgba.png')\n    out_depth = os.path.join(out_dir, os.path.basename(opt.path).split('.')[0] + '_depth.png')\n    out_normal = os.path.join(out_dir, os.path.basename(opt.path).split('.')[0] + '_normal.png')\n    out_caption = os.path.join(out_dir, os.path.basename(opt.path).split('.')[0] + '_caption.txt')\n\n    # load image\n    print(f'[INFO] loading image...')\n    image = cv2.imread(opt.path, cv2.IMREAD_UNCHANGED)\n    if image.shape[-1] == 4:\n        image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n    else:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # carve background\n    print(f'[INFO] background removal...')\n    carved_image = BackgroundRemoval()(image) # [H, W, 4]\n    mask = carved_image[..., -1] > 0\n\n    # predict depth\n    print(f'[INFO] depth estimation...')\n    dpt_depth_model = DPT(task='depth')\n    depth = dpt_depth_model(image)[0]\n    depth[mask] = (depth[mask] - depth[mask].min()) / (depth[mask].max() - depth[mask].min() + 1e-9)\n    depth[~mask] = 0\n    depth = (depth * 255).astype(np.uint8)\n    del dpt_depth_model\n\n    # predict normal\n    print(f'[INFO] normal estimation...')\n    dpt_normal_model = DPT(task='normal')\n    normal = dpt_normal_model(image)[0]\n    normal = (normal * 255).astype(np.uint8).transpose(1, 2, 0)\n    normal[~mask] = 0\n    del dpt_normal_model\n\n    # recenter\n    if opt.recenter:\n        print(f'[INFO] recenter...')\n        final_rgba = np.zeros((opt.size, opt.size, 4), dtype=np.uint8)\n        final_depth = np.zeros((opt.size, opt.size), dtype=np.uint8)\n        final_normal = np.zeros((opt.size, opt.size, 3), dtype=np.uint8)\n\n        coords = np.nonzero(mask)\n        x_min, x_max = coords[0].min(), coords[0].max()\n        y_min, y_max = coords[1].min(), coords[1].max()\n        h = x_max - x_min\n        w = y_max - y_min\n        desired_size = int(opt.size * (1 - opt.border_ratio))\n        scale = desired_size / max(h, w)\n        h2 = int(h * scale)\n        w2 = int(w * scale)\n        x2_min = (opt.size - h2) // 2\n        x2_max = x2_min + h2\n        y2_min = (opt.size - w2) // 2\n        y2_max = y2_min + w2\n        final_rgba[x2_min:x2_max, y2_min:y2_max] = cv2.resize(carved_image[x_min:x_max, y_min:y_max], (w2, h2), interpolation=cv2.INTER_AREA)\n        final_depth[x2_min:x2_max, y2_min:y2_max] = cv2.resize(depth[x_min:x_max, y_min:y_max], (w2, h2), interpolation=cv2.INTER_AREA)\n        final_normal[x2_min:x2_max, y2_min:y2_max] = cv2.resize(normal[x_min:x_max, y_min:y_max], (w2, h2), interpolation=cv2.INTER_AREA)\n\n    else:\n        final_rgba = carved_image\n        final_depth = depth\n        final_normal = normal\n\n    # write output\n    cv2.imwrite(out_rgba, cv2.cvtColor(final_rgba, cv2.COLOR_RGBA2BGRA))\n    cv2.imwrite(out_depth, final_depth)\n    cv2.imwrite(out_normal, final_normal)\n\n    # predict caption (it's too slow... use your brain instead)\n    # print(f'[INFO] captioning...')\n    # blip2 = BLIP2()\n    # caption = blip2(image)\n    # with open(out_caption, 'w') as f:\n    #     f.write(caption)\n\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/stable-dreamfusion/preprocess_image.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "DPT",
            "Description": "DPT impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/stable-dreamfusion/preprocess_image.py",
            "Implementation": "import os\nimport sys\nimport cv2\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\n\nclass DPT():\n    def __init__(self, task='depth', device='cuda'):\n\n        self.task = task\n        self.device = device\n\n        from dpt import DPTDepthModel\n\n        if task == 'depth':\n            path = 'pretrained/omnidata/omnidata_dpt_depth_v2.ckpt'\n            self.model = DPTDepthModel(backbone='vitb_rn50_384')\n            self.aug = transforms.Compose([\n                transforms.Resize((384, 384)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=0.5, std=0.5)\n            ])\n\n        else: # normal\n            path = 'pretrained/omnidata/omnidata_dpt_normal_v2.ckpt'\n            self.model = DPTDepthModel(backbone='vitb_rn50_384', num_channels=3)\n            self.aug = transforms.Compose([\n                transforms.Resize((384, 384)),\n                transforms.ToTensor()\n            ])\n\n        # load model\n        checkpoint = torch.load(path, map_location='cpu', weights_only=False)\n        if 'state_dict' in checkpoint:\n            state_dict = {}\n            for k, v in checkpoint['state_dict'].items():\n                state_dict[k[6:]] = v\n        else:\n            state_dict = checkpoint\n        self.model.load_state_dict(state_dict)\n        self.model.eval().to(device)\n\n\n    @torch.no_grad()\n    def __call__(self, image):\n        # image: np.ndarray, uint8, [H, W, 3]\n        H, W = image.shape[:2]\n        image = Image.fromarray(image)\n\n        image = self.aug(image).unsqueeze(0).to(self.device)\n\n        if self.task == 'depth':\n            depth = self.model(image).clamp(0, 1)\n            depth = F.interpolate(depth.unsqueeze(1), size=(H, W), mode='bicubic', align_corners=False)\n            depth = depth.squeeze(1).cpu().numpy()\n            return depth\n        else:\n            normal = self.model(image).clamp(0, 1)\n            normal = F.interpolate(normal, size=(H, W), mode='bicubic', align_corners=False)\n            normal = normal.cpu().numpy()\n            return normal\n\n\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}