{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/GFPGAN",
    "API_Calls": [
        {
            "Name": "inference_gfpgan",
            "Description": "调用GFPGAN实现批量图像的人脸检测、超分辨率修复与背景增强，自动输出完整修复图、人脸对比图。",
            "Code": "import argparse\nimport cv2\nimport glob\nimport numpy as np\nimport os\nimport torch\nfrom basicsr.utils import imwrite\n\nfrom gfpgan import GFPGANer\n\n\ndef main():\n    \"\"\"Inference demo for GFPGAN (for users).\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '-i',\n        '--input',\n        type=str,\n        default='inputs/whole_imgs',\n        help='Input image or folder. Default: inputs/whole_imgs')\n    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder. Default: results')\n    # we use version to select models, which is more user-friendly\n    parser.add_argument(\n        '-v', '--version', type=str, default='1.3', help='GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3')\n    parser.add_argument(\n        '-s', '--upscale', type=int, default=2, help='The final upsampling scale of the image. Default: 2')\n\n    parser.add_argument(\n        '--bg_upsampler', type=str, default='realesrgan', help='background upsampler. Default: realesrgan')\n    parser.add_argument(\n        '--bg_tile',\n        type=int,\n        default=400,\n        help='Tile size for background sampler, 0 for no tile during testing. Default: 400')\n    parser.add_argument('--suffix', type=str, default=None, help='Suffix of the restored faces')\n    parser.add_argument('--only_center_face', action='store_true', help='Only restore the center face')\n    parser.add_argument('--aligned', action='store_true', help='Input are aligned faces')\n    parser.add_argument(\n        '--ext',\n        type=str,\n        default='auto',\n        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto')\n    parser.add_argument('-w', '--weight', type=float, default=0.5, help='Adjustable weights.')\n    args = parser.parse_args()\n\n    args = parser.parse_args()\n\n    # ------------------------ input & output ------------------------\n    if args.input.endswith('/'):\n        args.input = args.input[:-1]\n    if os.path.isfile(args.input):\n        img_list = [args.input]\n    else:\n        img_list = sorted(glob.glob(os.path.join(args.input, '*')))\n\n    os.makedirs(args.output, exist_ok=True)\n\n    # ------------------------ set up background upsampler ------------------------\n    if args.bg_upsampler == 'realesrgan':\n        if not torch.cuda.is_available():  # CPU\n            import warnings\n            warnings.warn('The unoptimized RealESRGAN is slow on CPU. We do not use it. '\n                          'If you really want to use it, please modify the corresponding codes.')\n            bg_upsampler = None\n        else:\n            from basicsr.archs.rrdbnet_arch import RRDBNet\n            from realesrgan import RealESRGANer\n            model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n            bg_upsampler = RealESRGANer(\n                scale=2,\n                model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n                model=model,\n                tile=args.bg_tile,\n                tile_pad=10,\n                pre_pad=0,\n                half=True)  # need to set False in CPU mode\n    else:\n        bg_upsampler = None\n\n    # ------------------------ set up GFPGAN restorer ------------------------\n    if args.version == '1':\n        arch = 'original'\n        channel_multiplier = 1\n        model_name = 'GFPGANv1'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth'\n    elif args.version == '1.2':\n        arch = 'clean'\n        channel_multiplier = 2\n        model_name = 'GFPGANCleanv1-NoCE-C2'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth'\n    elif args.version == '1.3':\n        arch = 'clean'\n        channel_multiplier = 2\n        model_name = 'GFPGANv1.3'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth'\n    elif args.version == '1.4':\n        arch = 'clean'\n        channel_multiplier = 2\n        model_name = 'GFPGANv1.4'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth'\n    elif args.version == 'RestoreFormer':\n        arch = 'RestoreFormer'\n        channel_multiplier = 2\n        model_name = 'RestoreFormer'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth'\n    else:\n        raise ValueError(f'Wrong model version {args.version}.')\n\n    # determine model paths\n    model_path = os.path.join('experiments/pretrained_models', model_name + '.pth')\n    if not os.path.isfile(model_path):\n        model_path = os.path.join('gfpgan/weights', model_name + '.pth')\n    if not os.path.isfile(model_path):\n        # download pre-trained models from url\n        model_path = url\n\n    restorer = GFPGANer(\n        model_path=model_path,\n        upscale=args.upscale,\n        arch=arch,\n        channel_multiplier=channel_multiplier,\n        bg_upsampler=bg_upsampler)\n\n    # ------------------------ restore ------------------------\n    for img_path in img_list:\n        # read image\n        img_name = os.path.basename(img_path)\n        print(f'Processing {img_name} ...')\n        basename, ext = os.path.splitext(img_name)\n        input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n\n        # restore faces and background if necessary\n        cropped_faces, restored_faces, restored_img = restorer.enhance(\n            input_img,\n            has_aligned=args.aligned,\n            only_center_face=args.only_center_face,\n            paste_back=True,\n            weight=args.weight)\n\n        # save faces\n        for idx, (cropped_face, restored_face) in enumerate(zip(cropped_faces, restored_faces)):\n            # save cropped face\n            save_crop_path = os.path.join(args.output, 'cropped_faces', f'{basename}_{idx:02d}.png')\n            imwrite(cropped_face, save_crop_path)\n            # save restored face\n            if args.suffix is not None:\n                save_face_name = f'{basename}_{idx:02d}_{args.suffix}.png'\n            else:\n                save_face_name = f'{basename}_{idx:02d}.png'\n            save_restore_path = os.path.join(args.output, 'restored_faces', save_face_name)\n            imwrite(restored_face, save_restore_path)\n            # save comparison image\n            cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n            imwrite(cmp_img, os.path.join(args.output, 'cmp', f'{basename}_{idx:02d}.png'))\n\n        # save restored img\n        if restored_img is not None:\n            if args.ext == 'auto':\n                extension = ext[1:]\n            else:\n                extension = args.ext\n\n            if args.suffix is not None:\n                save_restore_path = os.path.join(args.output, 'restored_imgs', f'{basename}_{args.suffix}.{extension}')\n            else:\n                save_restore_path = os.path.join(args.output, 'restored_imgs', f'{basename}.{extension}')\n            imwrite(restored_img, save_restore_path)\n\n    print(f'Results are in the [{args.output}] folder.')\n\n\nif __name__ == '__main__':\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/GFPGAN/inference_gfpgan.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class GFPGANer",
            "Description": "基于GFPGAN模型实现人脸检测、修复与背景增强，将低质量图像中的人脸超分辨率重建并融合至优化后的背景",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/GFPGAN/gfpgan/utils.py",
            "Implementation": "\nclass GFPGANer():\n    \"\"\"Helper for restoration with GFPGAN.\n\n    It will detect and crop faces, and then resize the faces to 512x512.\n    GFPGAN is used to restored the resized faces.\n    The background is upsampled with the bg_upsampler.\n    Finally, the faces will be pasted back to the upsample background image.\n\n    Args:\n        model_path (str): The path to the GFPGAN model. It can be urls (will first download it automatically).\n        upscale (float): The upscale of the final output. Default: 2.\n        arch (str): The GFPGAN architecture. Option: clean | original. Default: clean.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        bg_upsampler (nn.Module): The upsampler for the background. Default: None.\n    \"\"\"\n\n    def __init__(self, model_path, upscale=2, arch='clean', channel_multiplier=2, bg_upsampler=None, device=None):\n        self.upscale = upscale\n        self.bg_upsampler = bg_upsampler\n\n        # initialize model\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n        # initialize the GFP-GAN\n        if arch == 'clean':\n            self.gfpgan = GFPGANv1Clean(\n                out_size=512,\n                num_style_feat=512,\n                channel_multiplier=channel_multiplier,\n                decoder_load_path=None,\n                fix_decoder=False,\n                num_mlp=8,\n                input_is_latent=True,\n                different_w=True,\n                narrow=1,\n                sft_half=True)\n        elif arch == 'bilinear':\n            self.gfpgan = GFPGANBilinear(\n                out_size=512,\n                num_style_feat=512,\n                channel_multiplier=channel_multiplier,\n                decoder_load_path=None,\n                fix_decoder=False,\n                num_mlp=8,\n                input_is_latent=True,\n                different_w=True,\n                narrow=1,\n                sft_half=True)\n        elif arch == 'original':\n            self.gfpgan = GFPGANv1(\n                out_size=512,\n                num_style_feat=512,\n                channel_multiplier=channel_multiplier,\n                decoder_load_path=None,\n                fix_decoder=True,\n                num_mlp=8,\n                input_is_latent=True,\n                different_w=True,\n                narrow=1,\n                sft_half=True)\n        elif arch == 'RestoreFormer':\n            from gfpgan.archs.restoreformer_arch import RestoreFormer\n            self.gfpgan = RestoreFormer()\n        # initialize face helper\n        self.face_helper = FaceRestoreHelper(\n            upscale,\n            face_size=512,\n            crop_ratio=(1, 1),\n            det_model='retinaface_resnet50',\n            save_ext='png',\n            use_parse=True,\n            device=self.device,\n            model_rootpath='gfpgan/weights')\n\n        if model_path.startswith('https://'):\n            model_path = load_file_from_url(\n                url=model_path, model_dir=os.path.join(ROOT_DIR, 'gfpgan/weights'), progress=True, file_name=None)\n        loadnet = torch.load(model_path)\n        if 'params_ema' in loadnet:\n            keyname = 'params_ema'\n        else:\n            keyname = 'params'\n        self.gfpgan.load_state_dict(loadnet[keyname], strict=True)\n        self.gfpgan.eval()\n        self.gfpgan = self.gfpgan.to(self.device)\n\n    @torch.no_grad()\n    def enhance(self, img, has_aligned=False, only_center_face=False, paste_back=True, weight=0.5):\n        self.face_helper.clean_all()\n\n        if has_aligned:  # the inputs are already aligned\n            img = cv2.resize(img, (512, 512))\n            self.face_helper.cropped_faces = [img]\n        else:\n            self.face_helper.read_image(img)\n            # get face landmarks for each face\n            self.face_helper.get_face_landmarks_5(only_center_face=only_center_face, eye_dist_threshold=5)\n            # eye_dist_threshold=5: skip faces whose eye distance is smaller than 5 pixels\n            # TODO: even with eye_dist_threshold, it will still introduce wrong detections and restorations.\n            # align and warp each face\n            self.face_helper.align_warp_face()\n\n        # face restoration\n        for cropped_face in self.face_helper.cropped_faces:\n            # prepare data\n            cropped_face_t = img2tensor(cropped_face / 255., bgr2rgb=True, float32=True)\n            normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n            cropped_face_t = cropped_face_t.unsqueeze(0).to(self.device)\n\n            try:\n                output = self.gfpgan(cropped_face_t, return_rgb=False, weight=weight)[0]\n                # convert to image\n                restored_face = tensor2img(output.squeeze(0), rgb2bgr=True, min_max=(-1, 1))\n            except RuntimeError as error:\n                print(f'\\tFailed inference for GFPGAN: {error}.')\n                restored_face = cropped_face\n\n            restored_face = restored_face.astype('uint8')\n            self.face_helper.add_restored_face(restored_face)\n\n        if not has_aligned and paste_back:\n            # upsample the background\n            if self.bg_upsampler is not None:\n                # Now only support RealESRGAN for upsampling background\n                bg_img = self.bg_upsampler.enhance(img, outscale=self.upscale)[0]\n            else:\n                bg_img = None\n\n            self.face_helper.get_inverse_affine(None)\n            # paste each restored face to the input image\n            restored_img = self.face_helper.paste_faces_to_input_image(upsample_img=bg_img)\n            return self.face_helper.cropped_faces, self.face_helper.restored_faces, restored_img\n        else:\n            return self.face_helper.cropped_faces, self.face_helper.restored_faces, None\n",
            "Example": [
            ]
        }
    ]
}