{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/RobustVideoMatting",
    "API_Calls": [
        {
            "Name": "MattingNetwork",
            "Description": "call MattingNetwork",
            "Code": "\"\"\"\npython inference.py \\\n    --variant mobilenetv3 \\\n    --checkpoint \"CHECKPOINT\" \\\n    --device cuda \\\n    --input-source \"input.mp4\" \\\n    --output-type video \\\n    --output-composition \"composition.mp4\" \\\n    --output-alpha \"alpha.mp4\" \\\n    --output-foreground \"foreground.mp4\" \\\n    --output-video-mbps 4 \\\n    --seq-chunk 1\n\"\"\"\n\nimport torch\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom typing import Optional, Tuple\nfrom tqdm.auto import tqdm\n\nfrom inference_utils import VideoReader, VideoWriter, ImageSequenceReader, ImageSequenceWriter\n\ndef convert_video(model,\n                  input_source: str,\n                  input_resize: Optional[Tuple[int, int]] = None,\n                  downsample_ratio: Optional[float] = None,\n                  output_type: str = 'video',\n                  output_composition: Optional[str] = None,\n                  output_alpha: Optional[str] = None,\n                  output_foreground: Optional[str] = None,\n                  output_video_mbps: Optional[float] = None,\n                  seq_chunk: int = 1,\n                  num_workers: int = 0,\n                  progress: bool = True,\n                  device: Optional[str] = None,\n                  dtype: Optional[torch.dtype] = None):\n    \n    \"\"\"\n    Args:\n        input_source:A video file, or an image sequence directory. Images must be sorted in accending order, support png and jpg.\n        input_resize: If provided, the input are first resized to (w, h).\n        downsample_ratio: The model's downsample_ratio hyperparameter. If not provided, model automatically set one.\n        output_type: Options: [\"video\", \"png_sequence\"].\n        output_composition:\n            The composition output path. File path if output_type == 'video'. Directory path if output_type == 'png_sequence'.\n            If output_type == 'video', the composition has green screen background.\n            If output_type == 'png_sequence'. the composition is RGBA png images.\n        output_alpha: The alpha output from the model.\n        output_foreground: The foreground output from the model.\n        seq_chunk: Number of frames to process at once. Increase it for better parallelism.\n        num_workers: PyTorch's DataLoader workers. Only use >0 for image input.\n        progress: Show progress bar.\n        device: Only need to manually provide if model is a TorchScript freezed model.\n        dtype: Only need to manually provide if model is a TorchScript freezed model.\n    \"\"\"\n    \n    assert downsample_ratio is None or (downsample_ratio > 0 and downsample_ratio <= 1), 'Downsample ratio must be between 0 (exclusive) and 1 (inclusive).'\n    assert any([output_composition, output_alpha, output_foreground]), 'Must provide at least one output.'\n    assert output_type in ['video', 'png_sequence'], 'Only support \"video\" and \"png_sequence\" output modes.'\n    assert seq_chunk >= 1, 'Sequence chunk must be >= 1'\n    assert num_workers >= 0, 'Number of workers must be >= 0'\n    \n    # Initialize transform\n    if input_resize is not None:\n        transform = transforms.Compose([\n            transforms.Resize(input_resize[::-1]),\n            transforms.ToTensor()\n        ])\n    else:\n        transform = transforms.ToTensor()\n\n    # Initialize reader\n    if os.path.isfile(input_source):\n        source = VideoReader(input_source, transform)\n    else:\n        source = ImageSequenceReader(input_source, transform)\n    reader = DataLoader(source, batch_size=seq_chunk, pin_memory=True, num_workers=num_workers)\n    \n    # Initialize writers\n    if output_type == 'video':\n        frame_rate = source.frame_rate if isinstance(source, VideoReader) else 30\n        output_video_mbps = 1 if output_video_mbps is None else output_video_mbps\n        if output_composition is not None:\n            writer_com = VideoWriter(\n                path=output_composition,\n                frame_rate=frame_rate,\n                bit_rate=int(output_video_mbps * 1000000))\n        if output_alpha is not None:\n            writer_pha = VideoWriter(\n                path=output_alpha,\n                frame_rate=frame_rate,\n                bit_rate=int(output_video_mbps * 1000000))\n        if output_foreground is not None:\n            writer_fgr = VideoWriter(\n                path=output_foreground,\n                frame_rate=frame_rate,\n                bit_rate=int(output_video_mbps * 1000000))\n    else:\n        if output_composition is not None:\n            writer_com = ImageSequenceWriter(output_composition, 'png')\n        if output_alpha is not None:\n            writer_pha = ImageSequenceWriter(output_alpha, 'png')\n        if output_foreground is not None:\n            writer_fgr = ImageSequenceWriter(output_foreground, 'png')\n\n    # Inference\n    model = model.eval()\n    if device is None or dtype is None:\n        param = next(model.parameters())\n        dtype = param.dtype\n        device = param.device\n    \n    if (output_composition is not None) and (output_type == 'video'):\n        bgr = torch.tensor([120, 255, 155], device=device, dtype=dtype).div(255).view(1, 1, 3, 1, 1)\n    \n    try:\n        with torch.no_grad():\n            bar = tqdm(total=len(source), disable=not progress, dynamic_ncols=True)\n            rec = [None] * 4\n            for src in reader:\n\n                if downsample_ratio is None:\n                    downsample_ratio = auto_downsample_ratio(*src.shape[2:])\n\n                src = src.to(device, dtype, non_blocking=True).unsqueeze(0) # [B, T, C, H, W]\n                fgr, pha, *rec = model(src, *rec, downsample_ratio)\n\n                if output_foreground is not None:\n                    writer_fgr.write(fgr[0])\n                if output_alpha is not None:\n                    writer_pha.write(pha[0])\n                if output_composition is not None:\n                    if output_type == 'video':\n                        com = fgr * pha + bgr * (1 - pha)\n                    else:\n                        fgr = fgr * pha.gt(0)\n                        com = torch.cat([fgr, pha], dim=-3)\n                    writer_com.write(com[0])\n                \n                bar.update(src.size(1))\n\n    finally:\n        # Clean up\n        if output_composition is not None:\n            writer_com.close()\n        if output_alpha is not None:\n            writer_pha.close()\n        if output_foreground is not None:\n            writer_fgr.close()\n\n\ndef auto_downsample_ratio(h, w):\n    \"\"\"\n    Automatically find a downsample ratio so that the largest side of the resolution be 512px.\n    \"\"\"\n    return min(512 / max(h, w), 1)\n\n\nclass Converter:\n    def __init__(self, variant: str, checkpoint: str, device: str):\n        self.model = MattingNetwork(variant).eval().to(device)\n        self.model.load_state_dict(torch.load(checkpoint, map_location=device))\n        self.model = torch.jit.script(self.model)\n        self.model = torch.jit.freeze(self.model)\n        self.device = device\n    \n    def convert(self, *args, **kwargs):\n        convert_video(self.model, device=self.device, dtype=torch.float32, *args, **kwargs)\n    \nif __name__ == '__main__':\n    import argparse\n    from model import MattingNetwork\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('--variant', type=str, required=True, choices=['mobilenetv3', 'resnet50'])\n    parser.add_argument('--checkpoint', type=str, required=True)\n    parser.add_argument('--device', type=str, required=True)\n    parser.add_argument('--input-source', type=str, required=True)\n    parser.add_argument('--input-resize', type=int, default=None, nargs=2)\n    parser.add_argument('--downsample-ratio', type=float)\n    parser.add_argument('--output-composition', type=str)\n    parser.add_argument('--output-alpha', type=str)\n    parser.add_argument('--output-foreground', type=str)\n    parser.add_argument('--output-type', type=str, required=True, choices=['video', 'png_sequence'])\n    parser.add_argument('--output-video-mbps', type=int, default=1)\n    parser.add_argument('--seq-chunk', type=int, default=1)\n    parser.add_argument('--num-workers', type=int, default=0)\n    parser.add_argument('--disable-progress', action='store_true')\n    args = parser.parse_args()\n    \n    converter = Converter(args.variant, args.checkpoint, args.device)\n    converter.convert(\n        input_source=args.input_source,\n        input_resize=args.input_resize,\n        downsample_ratio=args.downsample_ratio,\n        output_type=args.output_type,\n        output_composition=args.output_composition,\n        output_alpha=args.output_alpha,\n        output_foreground=args.output_foreground,\n        output_video_mbps=args.output_video_mbps,\n        seq_chunk=args.seq_chunk,\n        num_workers=args.num_workers,\n        progress=not args.disable_progress\n    )\n    \n    \n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/RobustVideoMatting/inference.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "MattingNetwork",
            "Description": "impl MattingNetwork",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/RobustVideoMatting/model/model.py",
            "Implementation": "import torch\nfrom torch import Tensor\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom typing import Optional, List\n\nfrom .mobilenetv3 import MobileNetV3LargeEncoder\nfrom .resnet import ResNet50Encoder\nfrom .lraspp import LRASPP\nfrom .decoder import RecurrentDecoder, Projection\nfrom .fast_guided_filter import FastGuidedFilterRefiner\nfrom .deep_guided_filter import DeepGuidedFilterRefiner\n\nclass MattingNetwork(nn.Module):\n    def __init__(self,\n                 variant: str = 'mobilenetv3',\n                 refiner: str = 'deep_guided_filter',\n                 pretrained_backbone: bool = False):\n        super().__init__()\n        assert variant in ['mobilenetv3', 'resnet50']\n        assert refiner in ['fast_guided_filter', 'deep_guided_filter']\n        \n        if variant == 'mobilenetv3':\n            self.backbone = MobileNetV3LargeEncoder(pretrained_backbone)\n            self.aspp = LRASPP(960, 128)\n            self.decoder = RecurrentDecoder([16, 24, 40, 128], [80, 40, 32, 16])\n        else:\n            self.backbone = ResNet50Encoder(pretrained_backbone)\n            self.aspp = LRASPP(2048, 256)\n            self.decoder = RecurrentDecoder([64, 256, 512, 256], [128, 64, 32, 16])\n            \n        self.project_mat = Projection(16, 4)\n        self.project_seg = Projection(16, 1)\n\n        if refiner == 'deep_guided_filter':\n            self.refiner = DeepGuidedFilterRefiner()\n        else:\n            self.refiner = FastGuidedFilterRefiner()\n        \n    def forward(self,\n                src: Tensor,\n                r1: Optional[Tensor] = None,\n                r2: Optional[Tensor] = None,\n                r3: Optional[Tensor] = None,\n                r4: Optional[Tensor] = None,\n                downsample_ratio: float = 1,\n                segmentation_pass: bool = False):\n        \n        if downsample_ratio != 1:\n            src_sm = self._interpolate(src, scale_factor=downsample_ratio)\n        else:\n            src_sm = src\n        \n        f1, f2, f3, f4 = self.backbone(src_sm)\n        f4 = self.aspp(f4)\n        hid, *rec = self.decoder(src_sm, f1, f2, f3, f4, r1, r2, r3, r4)\n        \n        if not segmentation_pass:\n            fgr_residual, pha = self.project_mat(hid).split([3, 1], dim=-3)\n            if downsample_ratio != 1:\n                fgr_residual, pha = self.refiner(src, src_sm, fgr_residual, pha, hid)\n            fgr = fgr_residual + src\n            fgr = fgr.clamp(0., 1.)\n            pha = pha.clamp(0., 1.)\n            return [fgr, pha, *rec]\n        else:\n            seg = self.project_seg(hid)\n            return [seg, *rec]\n\n    def _interpolate(self, x: Tensor, scale_factor: float):\n        if x.ndim == 5:\n            B, T = x.shape[:2]\n            x = F.interpolate(x.flatten(0, 1), scale_factor=scale_factor,\n                mode='bilinear', align_corners=False, recompute_scale_factor=False)\n            x = x.unflatten(0, (B, T))\n        else:\n            x = F.interpolate(x, scale_factor=scale_factor,\n                mode='bilinear', align_corners=False, recompute_scale_factor=False)\n        return x\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}