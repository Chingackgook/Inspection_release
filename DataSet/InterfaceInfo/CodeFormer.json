{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/CodeFormer",
    "API_Calls": [
        {
            "Name": "inference_codeformer",
            "Description": "调用FaceRestoreHelper 的方法实现了完整的人脸修复处理流程",
            "Code": "import os\nimport cv2\nimport argparse\nimport glob\nimport torch\nfrom torchvision.transforms.functional import normalize\nfrom basicsr.utils import imwrite, img2tensor, tensor2img\nfrom basicsr.utils.download_util import load_file_from_url\nfrom basicsr.utils.misc import gpu_is_available, get_device\nfrom facelib.utils.face_restoration_helper import FaceRestoreHelper\nfrom facelib.utils.misc import is_gray\n\nfrom basicsr.utils.registry import ARCH_REGISTRY\n\npretrain_model_url = {\n    'restoration': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth',\n}\n\ndef set_realesrgan():\n    from basicsr.archs.rrdbnet_arch import RRDBNet\n    from basicsr.utils.realesrgan_utils import RealESRGANer\n\n    use_half = False\n    if torch.cuda.is_available(): # set False in CPU/MPS mode\n        no_half_gpu_list = ['1650', '1660'] # set False for GPUs that don't support f16\n        if not True in [gpu in torch.cuda.get_device_name(0) for gpu in no_half_gpu_list]:\n            use_half = True\n\n    model = RRDBNet(\n        num_in_ch=3,\n        num_out_ch=3,\n        num_feat=64,\n        num_block=23,\n        num_grow_ch=32,\n        scale=2,\n    )\n    upsampler = RealESRGANer(\n        scale=2,\n        model_path=\"https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/RealESRGAN_x2plus.pth\",\n        model=model,\n        tile=args.bg_tile,\n        tile_pad=40,\n        pre_pad=0,\n        half=use_half\n    )\n\n    if not gpu_is_available():  # CPU\n        import warnings\n        warnings.warn('Running on CPU now! Make sure your PyTorch version matches your CUDA.'\n                        'The unoptimized RealESRGAN is slow on CPU. '\n                        'If you want to disable it, please remove `--bg_upsampler` and `--face_upsample` in command.',\n                        category=RuntimeWarning)\n    return upsampler\n\nif __name__ == '__main__':\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    device = get_device()\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-i', '--input_path', type=str, default='./inputs/whole_imgs', \n            help='Input image, video or folder. Default: inputs/whole_imgs')\n    parser.add_argument('-o', '--output_path', type=str, default=None, \n            help='Output folder. Default: results/<input_name>_<w>')\n    parser.add_argument('-w', '--fidelity_weight', type=float, default=0.5, \n            help='Balance the quality and fidelity. Default: 0.5')\n    parser.add_argument('-s', '--upscale', type=int, default=2, \n            help='The final upsampling scale of the image. Default: 2')\n    parser.add_argument('--has_aligned', action='store_true', help='Input are cropped and aligned faces. Default: False')\n    parser.add_argument('--only_center_face', action='store_true', help='Only restore the center face. Default: False')\n    parser.add_argument('--draw_box', action='store_true', help='Draw the bounding box for the detected faces. Default: False')\n    # large det_model: 'YOLOv5l', 'retinaface_resnet50'\n    # small det_model: 'YOLOv5n', 'retinaface_mobile0.25'\n    parser.add_argument('--detection_model', type=str, default='retinaface_resnet50', \n            help='Face detector. Optional: retinaface_resnet50, retinaface_mobile0.25, YOLOv5l, YOLOv5n, dlib. \\\n                Default: retinaface_resnet50')\n    parser.add_argument('--bg_upsampler', type=str, default='None', help='Background upsampler. Optional: realesrgan')\n    parser.add_argument('--face_upsample', action='store_true', help='Face upsampler after enhancement. Default: False')\n    parser.add_argument('--bg_tile', type=int, default=400, help='Tile size for background sampler. Default: 400')\n    parser.add_argument('--suffix', type=str, default=None, help='Suffix of the restored faces. Default: None')\n    parser.add_argument('--save_video_fps', type=float, default=None, help='Frame rate for saving video. Default: None')\n\n    args = parser.parse_args()\n\n    # ------------------------ input & output ------------------------\n    w = args.fidelity_weight\n    input_video = False\n    if args.input_path.endswith(('jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG')): # input single img path\n        input_img_list = [args.input_path]\n        result_root = f'test_results/test_img_{w}'\n    elif args.input_path.endswith(('mp4', 'mov', 'avi', 'MP4', 'MOV', 'AVI')): # input video path\n        from basicsr.utils.video_util import VideoReader, VideoWriter\n        input_img_list = []\n        vidreader = VideoReader(args.input_path)\n        image = vidreader.get_frame()\n        while image is not None:\n            input_img_list.append(image)\n            image = vidreader.get_frame()\n        audio = vidreader.get_audio()\n        fps = vidreader.get_fps() if args.save_video_fps is None else args.save_video_fps   \n        video_name = os.path.basename(args.input_path)[:-4]\n        result_root = f'test_results/{video_name}_{w}'\n        input_video = True\n        vidreader.close()\n    else: # input img folder\n        if args.input_path.endswith('/'):  # solve when path ends with /\n            args.input_path = args.input_path[:-1]\n        # scan all the jpg and png images\n        input_img_list = sorted(glob.glob(os.path.join(args.input_path, '*.[jpJP][pnPN]*[gG]')))\n        result_root = f'test_results/{os.path.basename(args.input_path)}_{w}'\n\n    if not args.output_path is None: # set output path\n        result_root = args.output_path\n\n    test_img_num = len(input_img_list)\n    if test_img_num == 0:\n        raise FileNotFoundError('No input image/video is found...\\n' \n            '\\tNote that --input_path for video should end with .mp4|.mov|.avi')\n\n    # ------------------ set up background upsampler ------------------\n    if args.bg_upsampler == 'realesrgan':\n        bg_upsampler = set_realesrgan()\n    else:\n        bg_upsampler = None\n\n    # ------------------ set up face upsampler ------------------\n    if args.face_upsample:\n        if bg_upsampler is not None:\n            face_upsampler = bg_upsampler\n        else:\n            face_upsampler = set_realesrgan()\n    else:\n        face_upsampler = None\n\n    # ------------------ set up CodeFormer restorer -------------------\n\n\n    \n    net = ARCH_REGISTRY.get('CodeFormer')(dim_embd=512, codebook_size=1024, n_head=8, n_layers=9, \n                                            connect_list=['32', '64', '128', '256']).to(device)\n    \n    # ckpt_path = 'weights/CodeFormer/codeformer.pth'\n    ckpt_path = load_file_from_url(url=pretrain_model_url['restoration'], \n                                    model_dir='weights/CodeFormer', progress=True, file_name=None)\n    checkpoint = torch.load(ckpt_path)['params_ema']\n    net.load_state_dict(checkpoint)\n    net.eval()\n\n    # ------------------ set up FaceRestoreHelper -------------------\n    # large det_model: 'YOLOv5l', 'retinaface_resnet50'\n    # small det_model: 'YOLOv5n', 'retinaface_mobile0.25'\n    if not args.has_aligned: \n        print(f'Face detection model: {args.detection_model}')\n    if bg_upsampler is not None: \n        print(f'Background upsampling: True, Face upsampling: {args.face_upsample}')\n    else:\n        print(f'Background upsampling: False, Face upsampling: {args.face_upsample}')\n\n    face_helper = FaceRestoreHelper(\n        args.upscale,\n        face_size=512,\n        crop_ratio=(1, 1),\n        det_model = args.detection_model,\n        save_ext='png',\n        use_parse=True,\n        device=device)\n\n    # -------------------- start to processing ---------------------\n    for i, img_path in enumerate(input_img_list):\n        # clean all the intermediate results to process the next image\n        face_helper.clean_all()\n        \n        if isinstance(img_path, str):\n            img_name = os.path.basename(img_path)\n            basename, ext = os.path.splitext(img_name)\n            print(f'[{i+1}/{test_img_num}] Processing: {img_name}')\n            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        else: # for video processing\n            basename = str(i).zfill(6)\n            img_name = f'{video_name}_{basename}' if input_video else basename\n            print(f'[{i+1}/{test_img_num}] Processing: {img_name}')\n            img = img_path\n\n        if args.has_aligned: \n            # the input faces are already cropped and aligned\n            img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_LINEAR)\n            face_helper.is_gray = is_gray(img, threshold=10)\n            if face_helper.is_gray:\n                print('Grayscale input: True')\n            face_helper.cropped_faces = [img]\n        else:\n            face_helper.read_image(img)\n            # get face landmarks for each face\n            num_det_faces = face_helper.get_face_landmarks_5(\n                only_center_face=args.only_center_face, resize=640, eye_dist_threshold=5)\n            print(f'\\tdetect {num_det_faces} faces')\n            # align and warp each face\n            face_helper.align_warp_face()\n\n        # face restoration for each cropped face\n        for idx, cropped_face in enumerate(face_helper.cropped_faces):\n            # prepare data\n            cropped_face_t = img2tensor(cropped_face / 255., bgr2rgb=True, float32=True)\n            normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n            cropped_face_t = cropped_face_t.unsqueeze(0).to(device)\n\n            try:\n                with torch.no_grad():\n                    output = net(cropped_face_t, w=w, adain=True)[0]\n                    restored_face = tensor2img(output, rgb2bgr=True, min_max=(-1, 1))\n                del output\n                torch.cuda.empty_cache()\n            except Exception as error:\n                print(f'\\tFailed inference for CodeFormer: {error}')\n                restored_face = tensor2img(cropped_face_t, rgb2bgr=True, min_max=(-1, 1))\n\n            restored_face = restored_face.astype('uint8')\n            face_helper.add_restored_face(restored_face, cropped_face)\n\n        # paste_back\n        if not args.has_aligned:\n            # upsample the background\n            if bg_upsampler is not None:\n                # Now only support RealESRGAN for upsampling background\n                bg_img = bg_upsampler.enhance(img, outscale=args.upscale)[0]\n            else:\n                bg_img = None\n            face_helper.get_inverse_affine(None)\n            # paste each restored face to the input image\n            if args.face_upsample and face_upsampler is not None: \n                restored_img = face_helper.paste_faces_to_input_image(upsample_img=bg_img, draw_box=args.draw_box, face_upsampler=face_upsampler)\n            else:\n                restored_img = face_helper.paste_faces_to_input_image(upsample_img=bg_img, draw_box=args.draw_box)\n\n        # save faces\n        for idx, (cropped_face, restored_face) in enumerate(zip(face_helper.cropped_faces, face_helper.restored_faces)):\n            # save cropped face\n            if not args.has_aligned: \n                save_crop_path = os.path.join(result_root, 'cropped_faces', f'{basename}_{idx:02d}.png')\n                imwrite(cropped_face, save_crop_path)\n            # save restored face\n            if args.has_aligned:\n                save_face_name = f'{basename}.png'\n            else:\n                save_face_name = f'{basename}_{idx:02d}.png'\n            if args.suffix is not None:\n                save_face_name = f'{save_face_name[:-4]}_{args.suffix}.png'\n            save_restore_path = os.path.join(result_root, 'restored_faces', save_face_name)\n            imwrite(restored_face, save_restore_path)\n\n        # save restored img\n        if not args.has_aligned and restored_img is not None:\n            if args.suffix is not None:\n                basename = f'{basename}_{args.suffix}'\n            save_restore_path = os.path.join(result_root, 'final_results', f'{basename}.png')\n            imwrite(restored_img, save_restore_path)\n\n    # save enhanced video\n    if input_video:\n        print('Video Saving...')\n        # load images\n        video_frames = []\n        img_list = sorted(glob.glob(os.path.join(result_root, 'final_results', '*.[jp][pn]g')))\n        for img_path in img_list:\n            img = cv2.imread(img_path)\n            video_frames.append(img)\n        # write images to video\n        height, width = video_frames[0].shape[:2]\n        if args.suffix is not None:\n            video_name = f'{video_name}_{args.suffix}.png'\n        save_restore_path = os.path.join(result_root, f'{video_name}.mp4')\n        vidwriter = VideoWriter(save_restore_path, height, width, fps, audio)\n         \n        for f in video_frames:\n            vidwriter.write_frame(f)\n        vidwriter.close()\n\n    print(f'\\nAll results are saved in {result_root}')",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/CodeFormer/inference_codeformer.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class FaceRestoreHelper",
            "Description": "封装从原始图像到最终修复的完整处理流程,实现了​​人脸修复流程的全链路管理​​，通过标准化的人脸检测-对齐-修复-逆向融合流程，结合多模型协作（检测/解析/超分）和自适应参数配置，完成高质量的人脸修复与自然融合",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/CodeFormer/facelib/utils/face_restoration_helper.py",
            "Implementation": "class FaceRestoreHelper(object):\n    \"\"\"Helper for the face restoration pipeline (base class).\"\"\"\n\n    def __init__(self,\n                 upscale_factor,\n                 face_size=512,\n                 crop_ratio=(1, 1),\n                 det_model='retinaface_resnet50',\n                 save_ext='png',\n                 template_3points=False,\n                 pad_blur=False,\n                 use_parse=False,\n                 device=None):\n        self.template_3points = template_3points  # improve robustness\n        self.upscale_factor = int(upscale_factor)\n        # the cropped face ratio based on the square face\n        self.crop_ratio = crop_ratio  # (h, w)\n        assert (self.crop_ratio[0] >= 1 and self.crop_ratio[1] >= 1), 'crop ration only supports >=1'\n        self.face_size = (int(face_size * self.crop_ratio[1]), int(face_size * self.crop_ratio[0]))\n        self.det_model = det_model\n\n        if self.det_model == 'dlib':\n            # standard 5 landmarks for FFHQ faces with 1024 x 1024\n            self.face_template = np.array([[686.77227723, 488.62376238], [586.77227723, 493.59405941],\n                                        [337.91089109, 488.38613861], [437.95049505, 493.51485149],\n                                        [513.58415842, 678.5049505]])\n            self.face_template = self.face_template / (1024 // face_size)\n        elif self.template_3points:\n            self.face_template = np.array([[192, 240], [319, 240], [257, 371]])\n        else:\n            # standard 5 landmarks for FFHQ faces with 512 x 512 \n            # facexlib\n            self.face_template = np.array([[192.98138, 239.94708], [318.90277, 240.1936], [256.63416, 314.01935],\n                                           [201.26117, 371.41043], [313.08905, 371.15118]])\n\n            # dlib: left_eye: 36:41  right_eye: 42:47  nose: 30,32,33,34  left mouth corner: 48  right mouth corner: 54\n            # self.face_template = np.array([[193.65928, 242.98541], [318.32558, 243.06108], [255.67984, 328.82894],\n            #                                 [198.22603, 372.82502], [313.91018, 372.75659]])\n\n        self.face_template = self.face_template * (face_size / 512.0)\n        if self.crop_ratio[0] > 1:\n            self.face_template[:, 1] += face_size * (self.crop_ratio[0] - 1) / 2\n        if self.crop_ratio[1] > 1:\n            self.face_template[:, 0] += face_size * (self.crop_ratio[1] - 1) / 2\n        self.save_ext = save_ext\n        self.pad_blur = pad_blur\n        if self.pad_blur is True:\n            self.template_3points = False\n\n        self.all_landmarks_5 = []\n        self.det_faces = []\n        self.affine_matrices = []\n        self.inverse_affine_matrices = []\n        self.cropped_faces = []\n        self.restored_faces = []\n        self.pad_input_imgs = []\n\n        if device is None:\n            # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n            self.device = get_device()\n        else:\n            self.device = device\n\n        # init face detection model\n        if self.det_model == 'dlib':\n            self.face_detector, self.shape_predictor_5 = self.init_dlib(dlib_model_url['face_detector'], dlib_model_url['shape_predictor_5'])\n        else:\n            self.face_detector = init_detection_model(det_model, half=False, device=self.device)\n\n        # init face parsing model\n        self.use_parse = use_parse\n        self.face_parse = init_parsing_model(model_name='parsenet', device=self.device)\n\n    def set_upscale_factor(self, upscale_factor):\n        self.upscale_factor = upscale_factor\n\n    def read_image(self, img):\n        \"\"\"img can be image path or cv2 loaded image.\"\"\"\n        # self.input_img is Numpy array, (h, w, c), BGR, uint8, [0, 255]\n        if isinstance(img, str):\n            img = cv2.imread(img)\n\n        if np.max(img) > 256:  # 16-bit image\n            img = img / 65535 * 255\n        if len(img.shape) == 2:  # gray image\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n        elif img.shape[2] == 4:  # BGRA image with alpha channel\n            img = img[:, :, 0:3]\n\n        self.input_img = img\n        self.is_gray = is_gray(img, threshold=10)\n        if self.is_gray:\n            print('Grayscale input: True')\n\n        if min(self.input_img.shape[:2])<512:\n            f = 512.0/min(self.input_img.shape[:2])\n            self.input_img = cv2.resize(self.input_img, (0,0), fx=f, fy=f, interpolation=cv2.INTER_LINEAR)\n\n    def init_dlib(self, detection_path, landmark5_path):\n        \"\"\"Initialize the dlib detectors and predictors.\"\"\"\n        try:\n            import dlib\n        except ImportError:\n            print('Please install dlib by running:' 'conda install -c conda-forge dlib')\n        detection_path = load_file_from_url(url=detection_path, model_dir='weights/dlib', progress=True, file_name=None)\n        landmark5_path = load_file_from_url(url=landmark5_path, model_dir='weights/dlib', progress=True, file_name=None)\n        face_detector = dlib.cnn_face_detection_model_v1(detection_path)\n        shape_predictor_5 = dlib.shape_predictor(landmark5_path)\n        return face_detector, shape_predictor_5\n\n    def get_face_landmarks_5_dlib(self,\n                                only_keep_largest=False,\n                                scale=1):\n        det_faces = self.face_detector(self.input_img, scale)\n\n        if len(det_faces) == 0:\n            print('No face detected. Try to increase upsample_num_times.')\n            return 0\n        else:\n            if only_keep_largest:\n                print('Detect several faces and only keep the largest.')\n                face_areas = []\n                for i in range(len(det_faces)):\n                    face_area = (det_faces[i].rect.right() - det_faces[i].rect.left()) * (\n                        det_faces[i].rect.bottom() - det_faces[i].rect.top())\n                    face_areas.append(face_area)\n                largest_idx = face_areas.index(max(face_areas))\n                self.det_faces = [det_faces[largest_idx]]\n            else:\n                self.det_faces = det_faces\n\n        if len(self.det_faces) == 0:\n            return 0\n\n        for face in self.det_faces:\n            shape = self.shape_predictor_5(self.input_img, face.rect)\n            landmark = np.array([[part.x, part.y] for part in shape.parts()])\n            self.all_landmarks_5.append(landmark)\n\n        return len(self.all_landmarks_5)\n\n\n    def get_face_landmarks_5(self,\n                             only_keep_largest=False,\n                             only_center_face=False,\n                             resize=None,\n                             blur_ratio=0.01,\n                             eye_dist_threshold=None):\n        if self.det_model == 'dlib':\n            return self.get_face_landmarks_5_dlib(only_keep_largest)\n\n        if resize is None:\n            scale = 1\n            input_img = self.input_img\n        else:\n            h, w = self.input_img.shape[0:2]\n            scale = resize / min(h, w)\n            # scale = max(1, scale) # always scale up; comment this out for HD images, e.g., AIGC faces.\n            h, w = int(h * scale), int(w * scale)\n            interp = cv2.INTER_AREA if scale < 1 else cv2.INTER_LINEAR\n            input_img = cv2.resize(self.input_img, (w, h), interpolation=interp)\n\n        with torch.no_grad():\n            bboxes = self.face_detector.detect_faces(input_img)\n\n        if bboxes is None or bboxes.shape[0] == 0:\n            return 0\n        else:\n            bboxes = bboxes / scale\n\n        for bbox in bboxes:\n            # remove faces with too small eye distance: side faces or too small faces\n            eye_dist = np.linalg.norm([bbox[6] - bbox[8], bbox[7] - bbox[9]])\n            if eye_dist_threshold is not None and (eye_dist < eye_dist_threshold):\n                continue\n\n            if self.template_3points:\n                landmark = np.array([[bbox[i], bbox[i + 1]] for i in range(5, 11, 2)])\n            else:\n                landmark = np.array([[bbox[i], bbox[i + 1]] for i in range(5, 15, 2)])\n            self.all_landmarks_5.append(landmark)\n            self.det_faces.append(bbox[0:5])\n            \n        if len(self.det_faces) == 0:\n            return 0\n        if only_keep_largest:\n            h, w, _ = self.input_img.shape\n            self.det_faces, largest_idx = get_largest_face(self.det_faces, h, w)\n            self.all_landmarks_5 = [self.all_landmarks_5[largest_idx]]\n        elif only_center_face:\n            h, w, _ = self.input_img.shape\n            self.det_faces, center_idx = get_center_face(self.det_faces, h, w)\n            self.all_landmarks_5 = [self.all_landmarks_5[center_idx]]\n\n        # pad blurry images\n        if self.pad_blur:\n            self.pad_input_imgs = []\n            for landmarks in self.all_landmarks_5:\n                # get landmarks\n                eye_left = landmarks[0, :]\n                eye_right = landmarks[1, :]\n                eye_avg = (eye_left + eye_right) * 0.5\n                mouth_avg = (landmarks[3, :] + landmarks[4, :]) * 0.5\n                eye_to_eye = eye_right - eye_left\n                eye_to_mouth = mouth_avg - eye_avg\n\n                # Get the oriented crop rectangle\n                # x: half width of the oriented crop rectangle\n                x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n                #  - np.flipud(eye_to_mouth) * [-1, 1]: rotate 90 clockwise\n                # norm with the hypotenuse: get the direction\n                x /= np.hypot(*x)  # get the hypotenuse of a right triangle\n                rect_scale = 1.5\n                x *= max(np.hypot(*eye_to_eye) * 2.0 * rect_scale, np.hypot(*eye_to_mouth) * 1.8 * rect_scale)\n                # y: half height of the oriented crop rectangle\n                y = np.flipud(x) * [-1, 1]\n\n                # c: center\n                c = eye_avg + eye_to_mouth * 0.1\n                # quad: (left_top, left_bottom, right_bottom, right_top)\n                quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n                # qsize: side length of the square\n                qsize = np.hypot(*x) * 2\n                border = max(int(np.rint(qsize * 0.1)), 3)\n\n                # get pad\n                # pad: (width_left, height_top, width_right, height_bottom)\n                pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n                       int(np.ceil(max(quad[:, 1]))))\n                pad = [\n                    max(-pad[0] + border, 1),\n                    max(-pad[1] + border, 1),\n                    max(pad[2] - self.input_img.shape[0] + border, 1),\n                    max(pad[3] - self.input_img.shape[1] + border, 1)\n                ]\n\n                if max(pad) > 1:\n                    # pad image\n                    pad_img = np.pad(self.input_img, ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n                    # modify landmark coords\n                    landmarks[:, 0] += pad[0]\n                    landmarks[:, 1] += pad[1]\n                    # blur pad images\n                    h, w, _ = pad_img.shape\n                    y, x, _ = np.ogrid[:h, :w, :1]\n                    mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0],\n                                                       np.float32(w - 1 - x) / pad[2]),\n                                      1.0 - np.minimum(np.float32(y) / pad[1],\n                                                       np.float32(h - 1 - y) / pad[3]))\n                    blur = int(qsize * blur_ratio)\n                    if blur % 2 == 0:\n                        blur += 1\n                    blur_img = cv2.boxFilter(pad_img, 0, ksize=(blur, blur))\n                    # blur_img = cv2.GaussianBlur(pad_img, (blur, blur), 0)\n\n                    pad_img = pad_img.astype('float32')\n                    pad_img += (blur_img - pad_img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n                    pad_img += (np.median(pad_img, axis=(0, 1)) - pad_img) * np.clip(mask, 0.0, 1.0)\n                    pad_img = np.clip(pad_img, 0, 255)  # float32, [0, 255]\n                    self.pad_input_imgs.append(pad_img)\n                else:\n                    self.pad_input_imgs.append(np.copy(self.input_img))\n\n        return len(self.all_landmarks_5)\n\n    def align_warp_face(self, save_cropped_path=None, border_mode='constant'):\n        \"\"\"Align and warp faces with face template.\n        \"\"\"\n        if self.pad_blur:\n            assert len(self.pad_input_imgs) == len(\n                self.all_landmarks_5), f'Mismatched samples: {len(self.pad_input_imgs)} and {len(self.all_landmarks_5)}'\n        for idx, landmark in enumerate(self.all_landmarks_5):\n            # use 5 landmarks to get affine matrix\n            # use cv2.LMEDS method for the equivalence to skimage transform\n            # ref: https://blog.csdn.net/yichxi/article/details/115827338\n            affine_matrix = cv2.estimateAffinePartial2D(landmark, self.face_template, method=cv2.LMEDS)[0]\n            self.affine_matrices.append(affine_matrix)\n            # warp and crop faces\n            if border_mode == 'constant':\n                border_mode = cv2.BORDER_CONSTANT\n            elif border_mode == 'reflect101':\n                border_mode = cv2.BORDER_REFLECT101\n            elif border_mode == 'reflect':\n                border_mode = cv2.BORDER_REFLECT\n            if self.pad_blur:\n                input_img = self.pad_input_imgs[idx]\n            else:\n                input_img = self.input_img\n            cropped_face = cv2.warpAffine(\n                input_img, affine_matrix, self.face_size, borderMode=border_mode, borderValue=(135, 133, 132))  # gray\n            self.cropped_faces.append(cropped_face)\n            # save the cropped face\n            if save_cropped_path is not None:\n                path = os.path.splitext(save_cropped_path)[0]\n                save_path = f'{path}_{idx:02d}.{self.save_ext}'\n                imwrite(cropped_face, save_path)\n\n    def get_inverse_affine(self, save_inverse_affine_path=None):\n        \"\"\"Get inverse affine matrix.\"\"\"\n        for idx, affine_matrix in enumerate(self.affine_matrices):\n            inverse_affine = cv2.invertAffineTransform(affine_matrix)\n            inverse_affine *= self.upscale_factor\n            self.inverse_affine_matrices.append(inverse_affine)\n            # save inverse affine matrices\n            if save_inverse_affine_path is not None:\n                path, _ = os.path.splitext(save_inverse_affine_path)\n                save_path = f'{path}_{idx:02d}.pth'\n                torch.save(inverse_affine, save_path)\n\n\n    def add_restored_face(self, restored_face, input_face=None):\n        if self.is_gray:\n            restored_face = bgr2gray(restored_face) # convert img into grayscale\n            if input_face is not None:\n                restored_face = adain_npy(restored_face, input_face) # transfer the color\n        self.restored_faces.append(restored_face)\n\n\n    def paste_faces_to_input_image(self, save_path=None, upsample_img=None, draw_box=False, face_upsampler=None):\n        h, w, _ = self.input_img.shape\n        h_up, w_up = int(h * self.upscale_factor), int(w * self.upscale_factor)\n\n        if upsample_img is None:\n            # simply resize the background\n            # upsample_img = cv2.resize(self.input_img, (w_up, h_up), interpolation=cv2.INTER_LANCZOS4)\n            upsample_img = cv2.resize(self.input_img, (w_up, h_up), interpolation=cv2.INTER_LINEAR)\n        else:\n            upsample_img = cv2.resize(upsample_img, (w_up, h_up), interpolation=cv2.INTER_LANCZOS4)\n\n        assert len(self.restored_faces) == len(\n            self.inverse_affine_matrices), ('length of restored_faces and affine_matrices are different.')\n        \n        inv_mask_borders = []\n        for restored_face, inverse_affine in zip(self.restored_faces, self.inverse_affine_matrices):\n            if face_upsampler is not None:\n                restored_face = face_upsampler.enhance(restored_face, outscale=self.upscale_factor)[0]\n                inverse_affine /= self.upscale_factor\n                inverse_affine[:, 2] *= self.upscale_factor\n                face_size = (self.face_size[0]*self.upscale_factor, self.face_size[1]*self.upscale_factor)\n            else:\n                # Add an offset to inverse affine matrix, for more precise back alignment\n                if self.upscale_factor > 1:\n                    extra_offset = 0.5 * self.upscale_factor\n                else:\n                    extra_offset = 0\n                inverse_affine[:, 2] += extra_offset\n                face_size = self.face_size\n            inv_restored = cv2.warpAffine(restored_face, inverse_affine, (w_up, h_up))\n\n            # if draw_box or not self.use_parse:  # use square parse maps\n            #     mask = np.ones(face_size, dtype=np.float32)\n            #     inv_mask = cv2.warpAffine(mask, inverse_affine, (w_up, h_up))\n            #     # remove the black borders\n            #     inv_mask_erosion = cv2.erode(\n            #         inv_mask, np.ones((int(2 * self.upscale_factor), int(2 * self.upscale_factor)), np.uint8))\n            #     pasted_face = inv_mask_erosion[:, :, None] * inv_restored\n            #     total_face_area = np.sum(inv_mask_erosion)  # // 3\n            #     # add border\n            #     if draw_box:\n            #         h, w = face_size\n            #         mask_border = np.ones((h, w, 3), dtype=np.float32)\n            #         border = int(1400/np.sqrt(total_face_area))\n            #         mask_border[border:h-border, border:w-border,:] = 0\n            #         inv_mask_border = cv2.warpAffine(mask_border, inverse_affine, (w_up, h_up))\n            #         inv_mask_borders.append(inv_mask_border)\n            #     if not self.use_parse:\n            #         # compute the fusion edge based on the area of face\n            #         w_edge = int(total_face_area**0.5) // 20\n            #         erosion_radius = w_edge * 2\n            #         inv_mask_center = cv2.erode(inv_mask_erosion, np.ones((erosion_radius, erosion_radius), np.uint8))\n            #         blur_size = w_edge * 2\n            #         inv_soft_mask = cv2.GaussianBlur(inv_mask_center, (blur_size + 1, blur_size + 1), 0)\n            #         if len(upsample_img.shape) == 2:  # upsample_img is gray image\n            #             upsample_img = upsample_img[:, :, None]\n            #         inv_soft_mask = inv_soft_mask[:, :, None]\n\n            # always use square mask\n            mask = np.ones(face_size, dtype=np.float32)\n            inv_mask = cv2.warpAffine(mask, inverse_affine, (w_up, h_up))\n            # remove the black borders\n            inv_mask_erosion = cv2.erode(\n                inv_mask, np.ones((int(2 * self.upscale_factor), int(2 * self.upscale_factor)), np.uint8))\n            pasted_face = inv_mask_erosion[:, :, None] * inv_restored\n            total_face_area = np.sum(inv_mask_erosion)  # // 3\n            # add border\n            if draw_box:\n                h, w = face_size\n                mask_border = np.ones((h, w, 3), dtype=np.float32)\n                border = int(1400/np.sqrt(total_face_area))\n                mask_border[border:h-border, border:w-border,:] = 0\n                inv_mask_border = cv2.warpAffine(mask_border, inverse_affine, (w_up, h_up))\n                inv_mask_borders.append(inv_mask_border)\n            # compute the fusion edge based on the area of face\n            w_edge = int(total_face_area**0.5) // 20\n            erosion_radius = w_edge * 2\n            inv_mask_center = cv2.erode(inv_mask_erosion, np.ones((erosion_radius, erosion_radius), np.uint8))\n            blur_size = w_edge * 2\n            inv_soft_mask = cv2.GaussianBlur(inv_mask_center, (blur_size + 1, blur_size + 1), 0)\n            if len(upsample_img.shape) == 2:  # upsample_img is gray image\n                upsample_img = upsample_img[:, :, None]\n            inv_soft_mask = inv_soft_mask[:, :, None]\n\n            # parse mask\n            if self.use_parse:\n                # inference\n                face_input = cv2.resize(restored_face, (512, 512), interpolation=cv2.INTER_LINEAR)\n                face_input = img2tensor(face_input.astype('float32') / 255., bgr2rgb=True, float32=True)\n                normalize(face_input, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n                face_input = torch.unsqueeze(face_input, 0).to(self.device)\n                with torch.no_grad():\n                    out = self.face_parse(face_input)[0]\n                out = out.argmax(dim=1).squeeze().cpu().numpy()\n\n                parse_mask = np.zeros(out.shape)\n                MASK_COLORMAP = [0, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 255, 0, 0, 0]\n                for idx, color in enumerate(MASK_COLORMAP):\n                    parse_mask[out == idx] = color\n                #  blur the mask\n                parse_mask = cv2.GaussianBlur(parse_mask, (101, 101), 11)\n                parse_mask = cv2.GaussianBlur(parse_mask, (101, 101), 11)\n                # remove the black borders\n                thres = 10\n                parse_mask[:thres, :] = 0\n                parse_mask[-thres:, :] = 0\n                parse_mask[:, :thres] = 0\n                parse_mask[:, -thres:] = 0\n                parse_mask = parse_mask / 255.\n\n                parse_mask = cv2.resize(parse_mask, face_size)\n                parse_mask = cv2.warpAffine(parse_mask, inverse_affine, (w_up, h_up), flags=3)\n                inv_soft_parse_mask = parse_mask[:, :, None]\n                # pasted_face = inv_restored\n                fuse_mask = (inv_soft_parse_mask<inv_soft_mask).astype('int')\n                inv_soft_mask = inv_soft_parse_mask*fuse_mask + inv_soft_mask*(1-fuse_mask)\n\n            if len(upsample_img.shape) == 3 and upsample_img.shape[2] == 4:  # alpha channel\n                alpha = upsample_img[:, :, 3:]\n                upsample_img = inv_soft_mask * pasted_face + (1 - inv_soft_mask) * upsample_img[:, :, 0:3]\n                upsample_img = np.concatenate((upsample_img, alpha), axis=2)\n            else:\n                upsample_img = inv_soft_mask * pasted_face + (1 - inv_soft_mask) * upsample_img\n\n        if np.max(upsample_img) > 256:  # 16-bit image\n            upsample_img = upsample_img.astype(np.uint16)\n        else:\n            upsample_img = upsample_img.astype(np.uint8)\n\n        # draw bounding box\n        if draw_box:\n            # upsample_input_img = cv2.resize(input_img, (w_up, h_up))\n            img_color = np.ones([*upsample_img.shape], dtype=np.float32)\n            img_color[:,:,0] = 0\n            img_color[:,:,1] = 255\n            img_color[:,:,2] = 0\n            for inv_mask_border in inv_mask_borders:\n                upsample_img = inv_mask_border * img_color + (1 - inv_mask_border) * upsample_img\n                # upsample_input_img = inv_mask_border * img_color + (1 - inv_mask_border) * upsample_input_img\n\n        if save_path is not None:\n            path = os.path.splitext(save_path)[0]\n            save_path = f'{path}.{self.save_ext}'\n            imwrite(upsample_img, save_path)\n        return upsample_img\n\n    def clean_all(self):\n        self.all_landmarks_5 = []\n        self.restored_faces = []\n        self.affine_matrices = []\n        self.cropped_faces = []\n        self.inverse_affine_matrices = []\n        self.det_faces = []\n        self.pad_input_imgs = []",
            "Example": [
                "\n"
            ]
        }
    ]
}