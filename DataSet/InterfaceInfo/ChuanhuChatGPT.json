{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/ChuanhuChatGPT",
    "API_Calls": [
        {
            "Name": "call_openai_make_dialogue",
            "Description": "调用OpenAIVisionClient接口使用openai模型与 OpenAI 视觉模型进行交互，支持两种回答生成模式：一次性获取完整回答和流式迭代获取部分回答。",
            "Code": "from __future__ import annotations\n\nimport logging\nimport os\n\nimport colorama\nimport commentjson as cjson\n\nfrom modules import config\n\nfrom modules.index_func import *\nfrom modules.presets import *\nfrom modules.utils import *\nfrom modules.models.base_model import BaseLLMModel, ModelType\n\n\ndef get_model(\n    model_name,\n    lora_model_path=None,\n    access_key=None,\n    temperature=None,\n    top_p=None,\n    system_prompt=None,\n    user_name=\"\",\n    original_model = None\n) -> BaseLLMModel:\n    msg = i18n(\"模型设置为了：\") + f\" {model_name}\"\n    model_type = ModelType.get_type(model_name)\n    lora_selector_visibility = False\n    lora_choices = [\"No LoRA\"]\n    dont_change_lora_selector = False\n    if model_type != ModelType.OpenAI:\n        config.local_embedding = True\n    # del current_model.model\n    model = original_model\n    try:\n        if model_type == ModelType.OpenAIVision or model_type == ModelType.OpenAI:\n            logging.info(f\"正在加载 OpenAI 模型: {model_name}\")\n            from modules.models.OpenAIVision import OpenAIVisionClient\n            access_key = os.environ.get(\"OPENAI_API_KEY\", access_key)\n            model = OpenAIVisionClient(\n                model_name, api_key=access_key, user_name=user_name)\n        elif model_type == ModelType.DeepSeek:\n            logging.info(f\"正在加载 DeepSeek 模型: {model_name}\")\n            from modules.models.OpenAIVision import OpenAIVisionClient\n            access_key = os.environ.get(\"DEEPSEEK_API_KEY\", access_key)\n            logging.info(access_key)\n            model = OpenAIVisionClient(\n                model_name, api_key=access_key, user_name=user_name)\n        elif model_type == ModelType.OpenAIInstruct:\n            logging.info(f\"正在加载OpenAI Instruct模型: {model_name}\")\n            from modules.models.OpenAIInstruct import OpenAI_Instruct_Client\n            access_key = os.environ.get(\"OPENAI_API_KEY\", access_key)\n            model = OpenAI_Instruct_Client(\n                model_name, api_key=access_key, user_name=user_name)\n        elif model_type == ModelType.ChatGLM:\n            logging.info(f\"正在加载ChatGLM模型: {model_name}\")\n            from modules.models.ChatGLM import ChatGLM_Client\n            model = ChatGLM_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.Groq:\n            logging.info(f\"正在加载Groq模型: {model_name}\")\n            from modules.models.Groq import Groq_Client\n            model = Groq_Client(model_name, access_key, user_name=user_name)\n        elif model_type == ModelType.LLaMA and lora_model_path == \"\":\n            msg = f\"现在请为 {model_name} 选择LoRA模型\"\n            logging.info(msg)\n            lora_selector_visibility = True\n            if os.path.isdir(\"lora\"):\n                lora_choices = [\"No LoRA\"] + get_file_names_by_pinyin(\"lora\", filetypes=[\"\"])\n        elif model_type == ModelType.LLaMA and lora_model_path != \"\":\n            logging.info(f\"正在加载LLaMA模型: {model_name} + {lora_model_path}\")\n            from modules.models.LLaMA import LLaMA_Client\n            dont_change_lora_selector = True\n            if lora_model_path == \"No LoRA\":\n                lora_model_path = None\n                msg += \" + No LoRA\"\n            else:\n                msg += f\" + {lora_model_path}\"\n            model = LLaMA_Client(\n                model_name, lora_model_path, user_name=user_name)\n        elif model_type == ModelType.XMChat:\n            from modules.models.XMChat import XMChat\n            if os.environ.get(\"XMCHAT_API_KEY\") != \"\":\n                access_key = os.environ.get(\"XMCHAT_API_KEY\")\n            model = XMChat(api_key=access_key, user_name=user_name)\n        elif model_type == ModelType.StableLM:\n            from modules.models.StableLM import StableLM_Client\n            model = StableLM_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.MOSS:\n            from modules.models.MOSS import MOSS_Client\n            model = MOSS_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.YuanAI:\n            from modules.models.inspurai import Yuan_Client\n            model = Yuan_Client(model_name, api_key=access_key,\n                                user_name=user_name, system_prompt=system_prompt)\n        elif model_type == ModelType.Minimax:\n            from modules.models.minimax import MiniMax_Client\n            if os.environ.get(\"MINIMAX_API_KEY\") != \"\":\n                access_key = os.environ.get(\"MINIMAX_API_KEY\")\n            model = MiniMax_Client(\n                model_name, api_key=access_key, user_name=user_name, system_prompt=system_prompt)\n        elif model_type == ModelType.ChuanhuAgent:\n            from modules.models.ChuanhuAgent import ChuanhuAgent_Client\n            model = ChuanhuAgent_Client(model_name, access_key, user_name=user_name)\n            msg = i18n(\"启用的工具：\") + \", \".join([i.name for i in model.tools])\n        elif model_type == ModelType.GooglePaLM:\n            from modules.models.GooglePaLM import Google_PaLM_Client\n            access_key = os.environ.get(\"GOOGLE_GENAI_API_KEY\", access_key)\n            model = Google_PaLM_Client(\n                model_name, access_key, user_name=user_name)\n        elif model_type == ModelType.GoogleGemini:\n            from modules.models.GoogleGemini import GoogleGeminiClient\n            access_key = os.environ.get(\"GOOGLE_GENAI_API_KEY\", access_key)\n            model = GoogleGeminiClient(\n                model_name, access_key, user_name=user_name)\n        elif model_type == ModelType.LangchainChat:\n            from modules.models.Azure import Azure_OpenAI_Client\n            model = Azure_OpenAI_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.Midjourney:\n            from modules.models.midjourney import Midjourney_Client\n            mj_proxy_api_secret = os.getenv(\"MIDJOURNEY_PROXY_API_SECRET\")\n            model = Midjourney_Client(\n                model_name, mj_proxy_api_secret, user_name=user_name)\n        elif model_type == ModelType.Spark:\n            from modules.models.spark import Spark_Client\n            model = Spark_Client(model_name, os.getenv(\"SPARK_APPID\"), os.getenv(\n                \"SPARK_API_KEY\"), os.getenv(\"SPARK_API_SECRET\"), user_name=user_name)\n        elif model_type == ModelType.Claude:\n            from modules.models.Claude import Claude_Client\n            model = Claude_Client(model_name=model_name, api_secret=os.getenv(\"CLAUDE_API_SECRET\"))\n        elif model_type == ModelType.Qwen:\n            from modules.models.Qwen import Qwen_Client\n            model = Qwen_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.ERNIE:\n            from modules.models.ERNIE import ERNIE_Client\n            model = ERNIE_Client(model_name, api_key=os.getenv(\"ERNIE_APIKEY\"),secret_key=os.getenv(\"ERNIE_SECRETKEY\"))\n        elif model_type == ModelType.DALLE3:\n            from modules.models.DALLE3 import OpenAI_DALLE3_Client\n            access_key = os.environ.get(\"OPENAI_API_KEY\", access_key)\n            model = OpenAI_DALLE3_Client(model_name, api_key=access_key, user_name=user_name)\n        elif model_type == ModelType.Ollama:\n            from modules.models.Ollama import OllamaClient\n            ollama_host = os.environ.get(\"OLLAMA_HOST\", access_key)\n            model = OllamaClient(model_name, user_name=user_name, backend_model=lora_model_path)\n            model_list = model.get_model_list()\n            lora_selector_visibility = True\n            lora_choices = [i[\"name\"] for i in model_list[\"models\"]]\n        elif model_type == ModelType.GoogleGemma:\n            from modules.models.GoogleGemma import GoogleGemmaClient\n            model = GoogleGemmaClient(\n                model_name, access_key, user_name=user_name)\n        elif model_type == ModelType.Unknown:\n            raise ValueError(f\"Unknown model: {model_name}\")\n        else:\n            raise ValueError(f\"Unimplemented model type: {model_type}\")\n        logging.info(msg)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        msg = f\"{STANDARD_ERROR_MSG}: {e}\"\n    modelDescription = i18n(model.description)\n    presudo_key = hide_middle_chars(access_key)\n    if original_model is not None and model is not None:\n        model.history = original_model.history\n        model.history_file_path = original_model.history_file_path\n        model.system_prompt = original_model.system_prompt\n    if dont_change_lora_selector:\n        return model, msg, gr.update(label=model_name, placeholder=setPlaceholder(model=model)), gr.update(), access_key, presudo_key, modelDescription, model.stream\n    else:\n        return model, msg, gr.update(label=model_name, placeholder=setPlaceholder(model=model)), gr.Dropdown(choices=lora_choices, visible=lora_selector_visibility), access_key, presudo_key, modelDescription, model.stream\n\n\nif __name__ == \"__main__\":\n    with open(\"config.json\", \"r\", encoding=\"utf-8\") as f:\n        openai_api_key = cjson.load(f)[\"openai_api_key\"]\n    # set logging level to debug\n    logging.basicConfig(level=logging.DEBUG)\n    # client = ModelManager(model_name=\"gpt-3.5-turbo\", access_key=openai_api_key)\n    client = get_model(model_name=\"chatglm-6b-int4\")\n    chatbot = []\n    stream = False\n    # 测试账单功能\n    logging.info(colorama.Back.GREEN + \"测试账单功能\" + colorama.Back.RESET)\n    logging.info(client.billing_info())\n    # 测试问答\n    logging.info(colorama.Back.GREEN + \"测试问答\" + colorama.Back.RESET)\n    question = \"巴黎是中国的首都吗？\"\n    for i in client.predict(inputs=question, chatbot=chatbot, stream=stream):\n        logging.info(i)\n    logging.info(f\"测试问答后history : {client.history}\")\n    # 测试记忆力\n    logging.info(colorama.Back.GREEN + \"测试记忆力\" + colorama.Back.RESET)\n    question = \"我刚刚问了你什么问题？\"\n    for i in client.predict(inputs=question, chatbot=chatbot, stream=stream):\n        logging.info(i)\n    logging.info(f\"测试记忆力后history : {client.history}\")\n    # 测试重试功能\n    logging.info(colorama.Back.GREEN + \"测试重试功能\" + colorama.Back.RESET)\n    for i in client.retry(chatbot=chatbot, stream=stream):\n        logging.info(i)\n    logging.info(f\"重试后history : {client.history}\")\n    # # 测试总结功能\n    # print(colorama.Back.GREEN + \"测试总结功能\" + colorama.Back.RESET)\n    # chatbot, msg = client.reduce_token_size(chatbot=chatbot)\n    # print(chatbot, msg)\n    # print(f\"总结后history: {client.history}\")\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/ChuanhuChatGPT/modules/models/models.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class_OpenAIVisionClient",
            "Description": "OpenAIVision 类是本项目中用于与 OpenAI 视觉模型进行交互的接口，它继承自BaseLLMModel基类，提供了生成回答的核心功能。该接口支持两种回答生成模式：一次性获取完整回答和流式迭代获取部分回答。",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/ChuanhuChatGPT/modules/models/OpenAIVision.py",
            "Implementation": "from __future__ import annotations\n\nimport json\nimport logging\nimport traceback\nimport base64\nfrom math import ceil\n\nimport colorama\nimport requests\nfrom io import BytesIO\nimport time\n\nimport requests\nfrom PIL import Image\n\nfrom modules import shared\nfrom modules.config import retrieve_proxy, sensitive_id, usage_limit\nfrom modules.index_func import *\nfrom modules.presets import *\nfrom modules.utils import *\nfrom modules.models.base_model import BaseLLMModel\n\n\nclass OpenAIVisionClient(BaseLLMModel):\n    def __init__(\n        self,\n        model_name,\n        api_key,\n        user_name=\"\"\n    ) -> None:\n        super().__init__(\n            model_name=model_name,\n            user=user_name,\n            config={\n                \"api_key\": api_key\n            }\n        )\n        if self.api_host is not None:\n            self.chat_completion_url, self.images_completion_url, self.openai_api_base, self.balance_api_url, self.usage_api_url = shared.format_openai_host(self.api_host)\n        else:\n            self.api_host, self.chat_completion_url, self.images_completion_url, self.openai_api_base, self.balance_api_url, self.usage_api_url = shared.state.api_host, shared.state.chat_completion_url, shared.state.images_completion_url, shared.state.openai_api_base, shared.state.balance_api_url, shared.state.usage_api_url\n        self._refresh_header()\n\n    def get_answer_stream_iter(self):\n        response = self._get_response(stream=True)\n        if response is not None:\n            iter = self._decode_chat_response(response)\n            partial_text = \"\"\n            reasoning_text = \"\"\n            reasoning_start_time = None\n\n            for content_delta, reasoning_delta in iter:\n                if content_delta:\n                    partial_text += content_delta\n\n                if reasoning_delta:\n                    if reasoning_start_time is None:\n                        reasoning_start_time = time.time()\n                        elapsed_seconds = 0\n                    reasoning_text += reasoning_delta\n                if reasoning_text:\n                    if reasoning_delta:\n                        elapsed_seconds = int(time.time() - reasoning_start_time)\n                        reasoning_preview = reasoning_text[-20:].replace(\"\\n\", \"\")\n                        yield f'<details open>\\n<summary>Thinking ({elapsed_seconds}s)</summary>\\n{reasoning_text}</details>\\n\\n' + partial_text\n                    else:\n                        yield f'<details>\\n<summary>Thought for {elapsed_seconds} s</summary>\\n{reasoning_text}</details>\\n\\n' + partial_text\n                else:\n                    yield partial_text\n        else:\n            yield STANDARD_ERROR_MSG + GENERAL_ERROR_MSG\n\n    def get_answer_at_once(self):\n        response = self._get_response()\n        response = json.loads(response.text)\n        content = response[\"choices\"][0][\"message\"][\"content\"]\n        total_token_count = response[\"usage\"][\"total_tokens\"]\n        return content, total_token_count\n\n\n    def count_token(self, user_input):\n        input_token_count = count_token(construct_user(user_input))\n        if self.system_prompt is not None and len(self.all_token_counts) == 0:\n            system_prompt_token_count = count_token(\n                construct_system(self.system_prompt)\n            )\n            return input_token_count + system_prompt_token_count\n        return input_token_count\n\n    def count_image_tokens(self, width: int, height: int):\n        h = ceil(height / 512)\n        w = ceil(width / 512)\n        n = w * h\n        total = 85 + 170 * n\n        return total\n\n    def billing_info(self):\n        try:\n            curr_time = datetime.datetime.now()\n            last_day_of_month = get_last_day_of_month(\n                curr_time).strftime(\"%Y-%m-%d\")\n            first_day_of_month = curr_time.replace(day=1).strftime(\"%Y-%m-%d\")\n            usage_url = f\"{shared.state.usage_api_url}?start_date={first_day_of_month}&end_date={last_day_of_month}\"\n            try:\n                usage_data = self._get_billing_data(usage_url)\n            except Exception as e:\n                # logging.error(f\"获取API使用情况失败: \" + str(e))\n                if \"Invalid authorization header\" in str(e):\n                    return i18n(\"**获取API使用情况失败**，需在填写`config.json`中正确填写sensitive_id\")\n                elif \"Incorrect API key provided: sess\" in str(e):\n                    return i18n(\"**获取API使用情况失败**，sensitive_id错误或已过期\")\n                return i18n(\"**获取API使用情况失败**\")\n            # rounded_usage = \"{:.5f}\".format(usage_data[\"total_usage\"] / 100)\n            rounded_usage = round(usage_data[\"total_usage\"] / 100, 5)\n            usage_percent = round(usage_data[\"total_usage\"] / usage_limit, 2)\n            from modules.webui import get_html\n\n            # return i18n(\"**本月使用金额** \") + f\"\\u3000 ${rounded_usage}\"\n            return get_html(\"billing_info.html\").format(\n                    label = i18n(\"本月使用金额\"),\n                    usage_percent = usage_percent,\n                    rounded_usage = rounded_usage,\n                    usage_limit = usage_limit\n                )\n        except requests.exceptions.ConnectTimeout:\n            status_text = (\n                STANDARD_ERROR_MSG + CONNECTION_TIMEOUT_MSG + ERROR_RETRIEVE_MSG\n            )\n            return status_text\n        except requests.exceptions.ReadTimeout:\n            status_text = STANDARD_ERROR_MSG + READ_TIMEOUT_MSG + ERROR_RETRIEVE_MSG\n            return status_text\n        except Exception as e:\n            import traceback\n            traceback.print_exc()\n            logging.error(i18n(\"获取API使用情况失败:\") + str(e))\n            return STANDARD_ERROR_MSG + ERROR_RETRIEVE_MSG\n\n    def _get_gpt4v_style_history(self):\n        history = []\n        image_buffer = []\n        for message in self.history:\n            if message[\"role\"] == \"user\":\n                content = []\n                if image_buffer:\n                    for image in image_buffer:\n                        content.append(\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/{self.get_image_type(image)};base64,{self.get_base64_image(image)}\",\n                                }\n                            },\n                        )\n                if content:\n                    content.insert(0, {\"type\": \"text\", \"text\": message[\"content\"]})\n                    history.append(construct_user(content))\n                    image_buffer = []\n                else:\n                    history.append(message)\n            elif message[\"role\"] == \"assistant\":\n                history.append(message)\n            elif message[\"role\"] == \"image\":\n                image_buffer.append(message[\"content\"])\n        return history\n\n\n    @shared.state.switching_api_key  # 在不开启多账号模式的时候，这个装饰器不会起作用\n    def _get_response(self, stream=False):\n        openai_api_key = self.api_key\n        system_prompt = self.system_prompt\n        history = self._get_gpt4v_style_history()\n\n        logging.debug(colorama.Fore.YELLOW +\n                      f\"{history}\" + colorama.Fore.RESET)\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {openai_api_key}\",\n        }\n\n        if system_prompt is not None and \"o1\" not in self.model_name:\n            history = [construct_system(system_prompt), *history]\n\n        payload = {\n            \"model\": self.model_name,\n            \"messages\": history,\n            \"temperature\": self.temperature,\n            \"top_p\": self.top_p,\n            \"n\": self.n_choices,\n            \"stream\": stream,\n        }\n\n        if self.max_generation_token:\n            payload[\"max_tokens\"] = self.max_generation_token\n        if self.presence_penalty:\n            payload[\"presence_penalty\"] = self.presence_penalty\n        if self.frequency_penalty:\n            payload[\"frequency_penalty\"] = self.frequency_penalty\n        if self.stop_sequence:\n            payload[\"stop\"] = self.stop_sequence\n        if self.logit_bias is not None:\n            payload[\"logit_bias\"] = self.encoded_logit_bias()\n        if self.user_identifier:\n            payload[\"user\"] = self.user_identifier\n\n        if stream:\n            timeout = TIMEOUT_STREAMING\n        else:\n            timeout = TIMEOUT_ALL\n\n        with retrieve_proxy():\n            try:\n                response = requests.post(\n                    self.chat_completion_url,\n                    headers=headers,\n                    json=payload,\n                    stream=stream,\n                    timeout=timeout,\n                )\n            except:\n                traceback.print_exc()\n                return None\n        return response\n\n    def _refresh_header(self):\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {sensitive_id}\",\n        }\n\n\n    def _get_billing_data(self, billing_url):\n        with retrieve_proxy():\n            response = requests.get(\n                billing_url,\n                headers=self.headers,\n                timeout=TIMEOUT_ALL,\n            )\n\n        if response.status_code == 200:\n            data = response.json()\n            return data\n        else:\n            raise Exception(\n                f\"API request failed with status code {response.status_code}: {response.text}\"\n            )\n\n    def _decode_chat_response(self, response):\n        error_msg = \"\"\n        for chunk in response.iter_lines():\n            if chunk:\n                chunk = chunk.decode()\n                if chunk == \": keep-alive\":\n                    continue\n                chunk_length = len(chunk)\n                try:\n                    chunk = json.loads(chunk[6:])\n                except:\n                    print(i18n(\"JSON解析错误,收到的内容: \") + f\"{chunk}\")\n                    error_msg += chunk\n                    continue\n                try:\n                    if chunk_length > 6 and \"delta\" in chunk[\"choices\"][0]:\n                        if \"finish_details\" in chunk[\"choices\"][0]:\n                            finish_reason = chunk[\"choices\"][0][\"finish_details\"]\n                        elif \"finish_reason\" in chunk[\"choices\"][0]:\n                            finish_reason = chunk[\"choices\"][0][\"finish_reason\"]\n                        else:\n                            finish_reason = chunk[\"finish_details\"]\n                        if finish_reason == \"stop\":\n                            break\n                        try:\n                            if \"reasoning_content\" in chunk[\"choices\"][0][\"delta\"]:\n                                reasoning_content = chunk[\"choices\"][0][\"delta\"][\"reasoning_content\"]\n                            else:\n                                reasoning_content = None\n                            yield chunk[\"choices\"][0][\"delta\"][\"content\"], reasoning_content\n                        except Exception as e:\n                            # logging.error(f\"Error: {e}\")\n                            continue\n                except:\n                    traceback.print_exc()\n                    print(f\"ERROR: {chunk}\")\n                    continue\n        if error_msg and not error_msg==\"data: [DONE]\":\n            raise Exception(error_msg)\n\n    def set_key(self, new_access_key):\n        ret = super().set_key(new_access_key)\n        self._refresh_header()\n        return ret\n\n    def _single_query_at_once(self, history, temperature=1.0):\n        timeout = TIMEOUT_ALL\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"temperature\": f\"{temperature}\",\n        }\n        payload = {\n            \"model\": RENAME_MODEL if RENAME_MODEL is not None else self.model_name,\n            \"messages\": history,\n        }\n\n        with retrieve_proxy():\n            response = requests.post(\n                self.chat_completion_url,\n                headers=headers,\n                json=payload,\n                stream=False,\n                timeout=timeout,\n            )\n\n        return response\n\n    def auto_name_chat_history(self, name_chat_method, user_question, single_turn_checkbox):\n        if len(self.history) == 2 and not single_turn_checkbox and not hide_history_when_not_logged_in:\n            user_question = self.history[0][\"content\"]\n            if name_chat_method == i18n(\"模型自动总结（消耗tokens）\"):\n                ai_answer = self.history[1][\"content\"]\n                try:\n                    history = [\n                        { \"role\": \"system\", \"content\": SUMMARY_CHAT_SYSTEM_PROMPT},\n                        { \"role\": \"user\", \"content\": f\"Please write a title based on the following conversation:\\n---\\nUser: {user_question}\\nAssistant: {ai_answer}\"}\n                    ]\n                    response = self._single_query_at_once(history, temperature=0.0)\n                    response = json.loads(response.text)\n                    content = response[\"choices\"][0][\"message\"][\"content\"]\n                    filename = replace_special_symbols(content) + \".json\"\n                except Exception as e:\n                    logging.info(f\"自动命名失败。{e}\")\n                    filename = replace_special_symbols(user_question)[:16] + \".json\"\n                return self.rename_chat_history(filename)\n            elif name_chat_method == i18n(\"第一条提问\"):\n                filename = replace_special_symbols(user_question)[:16] + \".json\"\n                return self.rename_chat_history(filename)\n            else:\n                return gr.update()\n        else:\n            return gr.update()\n",
            "Examples": []
        },
        {
            "Name": "BaseLLMModel",
            "Description": "class BaseLLMModel implement info",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/ChuanhuChatGPT/modules/models/base_model.py",
            "Implementation": "from __future__ import annotations\n\nimport base64\nimport json\nimport time\nimport logging\nimport os\nimport shutil\nimport time\nimport traceback\nfrom collections import deque\nfrom enum import Enum\nfrom io import BytesIO\nfrom itertools import islice\nfrom threading import Condition, Thread\nfrom typing import Any, Dict, List, Optional\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union\nfrom uuid import UUID\nfrom langchain_core.outputs import ChatGenerationChunk, GenerationChunk\nfrom gradio.utils import get_upload_folder\nfrom gradio.processing_utils import save_file_to_cache\n\nimport colorama\nimport PIL\nimport urllib3\nfrom duckduckgo_search import DDGS\nfrom huggingface_hub import hf_hub_download\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import (AgentAction, AgentFinish, AIMessage, BaseMessage,\n                              HumanMessage, SystemMessage)\n\nfrom .. import shared\nfrom ..config import retrieve_proxy, auth_list\nfrom ..index_func import *\nfrom ..presets import *\nfrom ..utils import *\n\nclass BaseLLMModel:\n    def __init__(\n        self,\n        model_name,\n        user=\"\",\n        config=None,\n    ) -> None:\n\n        if config is not None:\n            temp = MODEL_METADATA[model_name].copy()\n            keys_with_diff_values = {key: temp[key] for key in temp if key in DEFAULT_METADATA and temp[key] != DEFAULT_METADATA[key]}\n            config.update(keys_with_diff_values)\n            temp.update(config)\n            config = temp\n        else:\n            config = MODEL_METADATA[model_name]\n\n        self.model_name = config[\"model_name\"]\n        self.multimodal = config[\"multimodal\"]\n        self.description = config[\"description\"]\n        self.placeholder = config[\"placeholder\"]\n        self.token_upper_limit = config[\"token_limit\"]\n        self.system_prompt = config[\"system\"]\n        self.api_key = config[\"api_key\"]\n        self.api_host = config[\"api_host\"]\n        self.stream = config[\"stream\"]\n\n        self.interrupted = False\n        self.need_api_key = self.api_key is not None\n        self.history = []\n        self.all_token_counts = []\n        self.model_type = ModelType.get_type(model_name)\n        self.history_file_path = get_first_history_name(user)\n        self.user_name = user\n        self.chatbot = []\n\n        self.default_single_turn = config[\"single_turn\"]\n        self.default_temperature = config[\"temperature\"]\n        self.default_top_p = config[\"top_p\"]\n        self.default_n_choices = config[\"n_choices\"]\n        self.default_stop_sequence = config[\"stop\"]\n        self.default_max_generation_token = config[\"max_generation\"]\n        self.default_presence_penalty = config[\"presence_penalty\"]\n        self.default_frequency_penalty = config[\"frequency_penalty\"]\n        self.default_logit_bias = config[\"logit_bias\"]\n        self.default_user_identifier = user\n        self.default_stream = config[\"stream\"]\n\n        self.single_turn = self.default_single_turn\n        self.temperature = self.default_temperature\n        self.top_p = self.default_top_p\n        self.n_choices = self.default_n_choices\n        self.stop_sequence = self.default_stop_sequence\n        self.max_generation_token = self.default_max_generation_token\n        self.presence_penalty = self.default_presence_penalty\n        self.frequency_penalty = self.default_frequency_penalty\n        self.logit_bias = self.default_logit_bias\n        self.user_identifier = user\n\n        self.metadata = config[\"metadata\"]\n\n    def predict(\n        self,\n        inputs,\n        chatbot,\n        use_websearch=False,\n        files=None,\n        reply_language=\"中文\",\n        should_check_token_count=True,\n    ):  # repetition_penalty, top_k\n        status_text = \"开始生成回答……\"\n        if type(inputs) == list:\n            logging.info(\n                \"用户\"\n                + f\"{self.user_name}\"\n                + \"的输入为：\"\n                + colorama.Fore.BLUE\n                + \"(\"\n                + str(len(inputs) - 1)\n                + \" images) \"\n                + f\"{inputs[0]['text']}\"\n                + colorama.Style.RESET_ALL\n            )\n        else:\n            logging.info(\n                \"用户\"\n                + f\"{self.user_name}\"\n                + \"的输入为：\"\n                + colorama.Fore.BLUE\n                + f\"{inputs}\"\n                + colorama.Style.RESET_ALL\n            )\n        if should_check_token_count:\n            if type(inputs) == list:\n                yield chatbot + [(inputs[0][\"text\"], \"\")], status_text\n            else:\n                yield chatbot + [(inputs, \"\")], status_text\n        if reply_language == \"跟随问题语言（不稳定）\":\n            reply_language = \"the same language as the question, such as English, 中文, 日本語, Español, Français, or Deutsch.\"\n\n        (\n            limited_context,\n            fake_inputs,\n            display_append,\n            inputs,\n            chatbot,\n        ) = self.prepare_inputs(\n            real_inputs=inputs,\n            use_websearch=use_websearch,\n            files=files,\n            reply_language=reply_language,\n            chatbot=chatbot,\n        )\n        yield chatbot + [(fake_inputs, \"\")], status_text\n\n        if (\n            self.need_api_key\n            and self.api_key is None\n            and not shared.state.multi_api_key\n        ):\n            status_text = STANDARD_ERROR_MSG + NO_APIKEY_MSG\n            logging.info(status_text)\n            chatbot.append((fake_inputs, \"\"))\n            if len(self.history) == 0:\n                self.history.append(construct_user(fake_inputs))\n                self.history.append(\"\")\n                self.all_token_counts.append(0)\n            else:\n                self.history[-2] = construct_user(fake_inputs)\n            yield chatbot + [(fake_inputs, \"\")], status_text\n            return\n        elif len(fake_inputs.strip()) == 0:\n            status_text = STANDARD_ERROR_MSG + NO_INPUT_MSG\n            logging.info(status_text)\n            yield chatbot + [(fake_inputs, \"\")], status_text\n            return\n\n        if self.single_turn:\n            self.history = []\n            self.all_token_counts = []\n        if type(inputs) == list:\n            self.history.append(inputs)\n        else:\n            self.history.append(construct_user(inputs))\n\n        start_time = time.time()\n        try:\n            if self.stream:\n                logging.debug(\"使用流式传输\")\n                iter = self.stream_next_chatbot(\n                    inputs,\n                    chatbot,\n                    fake_input=fake_inputs,\n                    display_append=display_append,\n                )\n                for chatbot, status_text in iter:\n                    yield chatbot, status_text\n            else:\n                logging.debug(\"不使用流式传输\")\n                chatbot, status_text = self.next_chatbot_at_once(\n                    inputs,\n                    chatbot,\n                    fake_input=fake_inputs,\n                    display_append=display_append,\n                )\n                yield chatbot, status_text\n        except Exception as e:\n            traceback.print_exc()\n            status_text = STANDARD_ERROR_MSG + beautify_err_msg(str(e))\n            yield chatbot, status_text\n        end_time = time.time()\n        if len(self.history) > 1 and self.history[-1][\"content\"] != fake_inputs:\n            logging.info(\n                \"回答为：\"\n                + colorama.Fore.BLUE\n                + f\"{self.history[-1]['content']}\"\n                + colorama.Style.RESET_ALL\n            )\n            logging.info(i18n(\"Tokens per second：{token_generation_speed}\").format(token_generation_speed=str(self.all_token_counts[-1] / (end_time - start_time))))\n\n        if limited_context:\n            # self.history = self.history[-4:]\n            # self.all_token_counts = self.all_token_counts[-2:]\n            self.history = []\n            self.all_token_counts = []\n\n        max_token = self.token_upper_limit - TOKEN_OFFSET\n\n        if sum(self.all_token_counts) > max_token and should_check_token_count:\n            count = 0\n            while (\n                sum(self.all_token_counts)\n                > self.token_upper_limit * REDUCE_TOKEN_FACTOR\n                and sum(self.all_token_counts) > 0\n            ):\n                count += 1\n                del self.all_token_counts[0]\n                del self.history[:2]\n            logging.info(status_text)\n            status_text = f\"为了防止token超限，模型忘记了早期的 {count} 轮对话\"\n            yield chatbot, status_text\n\n        self.chatbot = chatbot\n        self.auto_save(chatbot)\n\n    def retry(\n        self,\n        chatbot,\n        use_websearch=False,\n        files=None,\n        reply_language=\"中文\",\n    ):\n        logging.debug(\"重试中……\")\n        if len(self.history) > 1:\n            inputs = self.history[-2][\"content\"]\n            del self.history[-2:]\n            if len(self.all_token_counts) > 0:\n                self.all_token_counts.pop()\n        elif len(chatbot) > 0:\n            inputs = chatbot[-1][0]\n            if '<div class=\"user-message\">' in inputs:\n                inputs = inputs.split('<div class=\"user-message\">')[1]\n                inputs = inputs.split(\"</div>\")[0]\n        elif len(self.history) == 1:\n            inputs = self.history[-1][\"content\"]\n            del self.history[-1]\n        else:\n            yield chatbot, f\"{STANDARD_ERROR_MSG}上下文是空的\"\n            return\n\n        iter = self.predict(\n            inputs,\n            chatbot,\n            use_websearch=use_websearch,\n            files=files,\n            reply_language=reply_language,\n        )\n        for x in iter:\n            yield x\n        logging.debug(\"重试完毕\")\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}