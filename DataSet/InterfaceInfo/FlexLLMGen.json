{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/FlexLLMGen",
    "API_Calls": [
        {
            "Name": "generate_text",
            "Description": "调用OptLM类的目的是实现批量文本补全和问答任务，通过配置灵活的内存分配策略（支持权重在GPU/CPU间的智能分配）和启用4位量化压缩技术，在单GPU环境下高效运行大型语言模型（如OPT-1.3b），同时处理多个输入提示（包括问答和信息提取任务），通过采样生成和温度控制确保输出质量，最终实现高吞吐量的批量文本生成。",
            "Code": "\"\"\"Complete sentences with FlexLLMGen and OPT models.\"\"\"\nimport argparse\n\nfrom flexllmgen.flex_opt import (Policy, OptLM, ExecutionEnv, CompressionConfig,\n        str2bool)\nfrom transformers import AutoTokenizer\n\n\ndef main(args):\n    # Prompts\n    prompts = [\n        \"Question: Where were the 2004 Olympics held?\\n\"\n        \"Answer: Athens, Greece\\n\"\n        \"Question: What is the longest river on the earth?\\n\"\n        \"Answer:\",\n\n        \"Extract the airport codes from this text.\\n\"\n        \"Text: \\\"I want a flight from New York to San Francisco.\\\"\\n\"\n        \"Airport codes: JFK, SFO.\\n\"\n        \"Text: \\\"I want you to book a flight from Phoenix to Las Vegas.\\\"\\n\"\n        \"Airport codes:\",\n    ]\n\n    # Initialize environment\n    env = ExecutionEnv.create(args.offload_dir)\n\n    # Offloading policy\n    policy = Policy(len(prompts), 1,\n                    args.percent[0], args.percent[1],\n                    args.percent[2], args.percent[3],\n                    args.percent[4], args.percent[5],\n                    overlap=True, sep_layer=True, pin_weight=args.pin_weight,\n                    cpu_cache_compute=args.cpu_cache_compute, attn_sparsity=1.0,\n                    compress_weight=args.compress_weight,\n                    comp_weight_config=CompressionConfig(\n                        num_bits=4, group_size=64,\n                        group_dim=0, symmetric=False),\n                    compress_cache=args.compress_cache,\n                    comp_cache_config=CompressionConfig(\n                        num_bits=4, group_size=64,\n                        group_dim=2, symmetric=False))\n\n    # Model\n    print(\"Initialize...\")\n    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\", padding_side=\"left\")\n    tokenizer.add_bos_token = False\n    stop = tokenizer(\"\\n\").input_ids[0]\n\n    model = OptLM(args.model, env, args.path, policy)\n\n    # Generate\n    print(\"Generate...\")\n    inputs = tokenizer(prompts, padding=\"max_length\", max_length=128)\n    output_ids = model.generate(\n        inputs.input_ids,\n        do_sample=True,\n        temperature=0.7,\n        max_new_tokens=32,\n        stop=stop)\n    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n    print(\"Outputs:\\n\" + 70 * '-')\n    for i in [0, len(outputs)-1]:\n        print(f\"{i}: {outputs[i]}\")\n        print(\"-\" * 70)\n\n    # Shutdown\n    print(\"Shutdown...\")\n    env.close_copy_threads()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, default=\"facebook/opt-1.3b\",\n        help=\"The model name.\")\n    parser.add_argument(\"--path\", type=str, default=\"~/opt_weights\",\n        help=\"The path to the model weights. If there are no cached weights, \"\n             \"FlexLLMGen will automatically download them from HuggingFace.\")\n    parser.add_argument(\"--offload-dir\", type=str, default=\"~/flexllmgen_offload_dir\",\n        help=\"The directory to offload tensors. \")\n    parser.add_argument(\"--percent\", nargs=\"+\", type=int,\n        default=[100, 0, 100, 0, 100, 0],\n        help=\"Six numbers. They are \"\n         \"the percentage of weight on GPU, \"\n         \"the percentage of weight on CPU, \"\n         \"the percentage of attention cache on GPU, \"\n         \"the percentage of attention cache on CPU, \"\n         \"the percentage of activations on GPU, \"\n         \"the percentage of activations on CPU\")\n    parser.add_argument(\"--pin-weight\", type=str2bool, nargs=\"?\",\n        const=True, default=True)\n    parser.add_argument(\"--cpu-cache-compute\", action=\"store_true\")\n    parser.add_argument(\"--compress-weight\", action=\"store_true\",\n        help=\"Whether to compress weight.\")\n    parser.add_argument(\"--compress-cache\", action=\"store_true\",\n        help=\"Whether to compress cache.\")\n    args = parser.parse_args()\n\n    assert len(args.percent) == 6\n\n    main(args)\n\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/FlexLLMGen/flexllmgen/apps/completion.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class_OptLM",
            "Description": "OptLM 类封装了大型语言模型的完整推理流程，提供简洁的generate()方法接收输入文本序列并返回生成的token序列，同时通过Policy配置类支持灵活的内存分配策略（可精确控制权重、缓存、激活在GPU/CPU/磁盘上的分配比例），内置4位量化压缩功能，支持采样和贪婪解码两种生成模式，具备IO重叠优化和批量处理能力，能够自动处理模型权重的动态加载/卸载",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/FlexLLMGen/flexllmgen/flex_opt.py",
            "Implementation": "\"\"\"\nUsage:\npython3 -m flexllmgen.flex_opt --model facebook/opt-1.3b --gpu-batch-size 32 --percent 100 0 100 0 100 0\n\"\"\"\n\nimport argparse\nimport dataclasses\nimport os\nimport pickle\nimport time\nfrom typing import Union, List, Optional\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoTokenizer\n\nfrom flexllmgen.compression import CompressionConfig\nfrom flexllmgen.opt_config import OptConfig, get_opt_config, download_opt_weights\nfrom flexllmgen.pytorch_backend import (TorchDevice, TorchDisk, TorchLink,\n    TorchMixedDevice, DeviceType, general_copy, fix_recursive_import)\nfrom flexllmgen.timer import timers\nfrom flexllmgen.utils import (Task, ExecutionEnv, GB, T, ValueHolder,\n    array_1d, array_2d, array_3d, str2bool, project_decode_latency,\n    torch_mem_stats, torch_dtype_to_np_dtype, write_benchmark_log,\n    read_benchmark_log)\n\nfix_recursive_import()\n\nDUMMY_WEIGHT = \"_DUMMY_\"  # Use dummy weights for benchmark purposes\n\n\nclass OptLM:\n    def __init__(self,\n                 config: Union[str, OptConfig],\n                 env: ExecutionEnv,\n                 path: str,\n                 policy: Policy):\n        if isinstance(config, str):\n            config = get_opt_config(config)\n        self.config = config\n        self.env = env\n        self.path = path\n        self.policy = policy\n        self.num_gpu_batches = policy.num_gpu_batches\n\n        layers = []\n        layers.append(InputEmbed(self.config, self.env, self.policy))\n        for i in range(self.config.num_hidden_layers):\n            if policy.sep_layer:\n                layers.append(SelfAttention(self.config, self.env, self.policy, i))\n                layers.append(MLP(self.config, self.env, self.policy, i))\n            else:\n                layers.append(TransformerLayer(self.config, self.env, self.policy, i))\n        layers.append(OutputEmbed(self.config, self.env, self.policy))\n        self.layers = layers\n        self.num_layers = len(layers)\n\n        if self.policy.act_gpu_percent == 100:\n            self.act_home = self.env.gpu\n        elif self.policy.act_cpu_percent == 100:\n            self.act_home = self.env.cpu\n        elif self.policy.act_disk_percent == 100:\n            self.act_home = self.env.disk\n        else:\n            raise NotImplementedError()\n\n        # CUDA streams\n        self.load_weight_stream = torch.cuda.Stream()\n        self.load_cache_stream = torch.cuda.Stream()\n        self.store_cache_stream = torch.cuda.Stream()\n\n        # Intermediate tensors\n        # The following buffers store values used\n        # for the i-th token, j-th layer, k-th gpu batch.\n        num_layers, num_gpu_batches = self.num_layers, self.policy.num_gpu_batches\n\n        # cache[j][k]\n        self.cache_home = array_2d(num_layers, num_gpu_batches, ValueHolder)\n        self.cache_read_buf = array_2d(num_layers, num_gpu_batches, ValueHolder)\n        self.cache_write_buf = array_2d(num_layers, num_gpu_batches, ValueHolder)\n        # weight[j]\n        self.weight_read_buf = array_1d(num_layers, ValueHolder)\n        # attention_mask[k]\n        self.attention_mask = array_1d(num_gpu_batches, ValueHolder)\n\n        self.task = None\n        self.init_all_weights()\n\n    def set_task(self, task):\n        self.task = task\n        for l in self.layers:\n            l.set_task(task)\n\n    def init_weight(self, j):\n        expanded_path = os.path.abspath(os.path.expanduser(\n            os.path.join(self.path, f\"{self.config.name}-np\")))\n        check_path = os.path.join(expanded_path, \"decoder.embed_positions.weight\")\n        if not os.path.exists(check_path) and DUMMY_WEIGHT not in check_path:\n            download_opt_weights(self.config.name, self.path)\n\n        self.layers[j].init_weight(self.weight_home[j], expanded_path)\n\n    def load_weight(self, i, j, k, overlap=True):\n        # Handle corner cases\n        if j == self.num_layers:\n            j = 0\n            i += 1\n            if i == self.execute_gen_len:\n                return\n\n        # Load from weight_home to weight_read_buf\n        if overlap:\n            with torch.cuda.stream(self.load_weight_stream):\n                self.layers[j].load_weight(self.weight_home[j], self.weight_read_buf[j], k)\n        else:\n            self.layers[j].load_weight(self.weight_home[j], self.weight_read_buf[j], k)\n\n    def delete_weight(self, j, k):\n        if k == 0:\n            for x in self.weight_home[j].pop():\n                if isinstance(x, ValueHolder):\n                    for y in x.pop():\n                        y.delete()\n                else:\n                    x.delete()\n\n    def init_cache(self, j, k):\n        self.layers[j].init_cache_one_gpu_batch(self.cache_home[j][k])\n\n    def load_cache(self, i, j, k, overlap=True):\n        # Handle corner cases\n        if i == 0:  # prefill, no cache\n            return\n        if k == self.num_gpu_batches:\n            k = 0\n            j += 1\n        if j == self.num_layers:\n            j = 0\n            i += 1\n            if i == self.execute_gen_len:\n                return\n\n        # Load from cache_home to cache_read_buf\n        if overlap:\n            with torch.cuda.stream(self.load_cache_stream):\n                self.layers[j].load_cache(self.cache_home[j][k], self.cache_read_buf[j][k], i)\n        else:\n            self.layers[j].load_cache(self.cache_home[j][k], self.cache_read_buf[j][k], i)\n\n    def store_cache(self, i, j, k, overlap=True):\n        # Handle corner cases\n        if k == -1:\n            k = self.num_gpu_batches - 1\n            j -= 1\n        if j == -1:\n            j = self.num_layers - 1\n            i -= 1\n            if i == -1:\n                return\n        if i == self.task.gen_len - 1:  # last token, no need to store cache\n            self.cache_write_buf[j][k].pop()\n            return\n\n        # Store cache_write_buf to cache_home\n        # Delete cache_write_buf\n        if overlap:\n            with torch.cuda.stream(self.store_cache_stream):\n                self.layers[j].store_cache(self.cache_home[j][k], self.cache_write_buf[j][k], i)\n        else:\n            self.layers[j].store_cache(self.cache_home[j][k], self.cache_write_buf[j][k], i)\n\n    def delete_cache(self, j, k):\n        v = self.cache_home[j][k].pop()\n        if v:\n            for x in v:\n                x.delete()\n\n    def load_hidden(self, i, j, k):\n        # Handle corner cases\n        if k == self.num_gpu_batches:\n            k = 0\n            j += 1\n        if j == self.num_layers:\n            j = 0\n            i += 1\n            if i == self.execute_gen_len:\n                return\n\n        # Load to hidden states buffers\n        dst = self.layers[j].compute\n        if j == 0:\n            gpu_batch_size = self.policy.gpu_batch_size\n            left, right = k * gpu_batch_size, (k + 1) * gpu_batch_size\n            if i == 0:  # load from the input ids\n                val = dst.allocate((gpu_batch_size, self.task.prompt_len), np.int32)\n                val.load_from_np(self.output_ids[left:right, :self.task.prompt_len])\n            else:  # load from the last generated token\n                pos = self.task.prompt_len + i\n                val = dst.allocate((gpu_batch_size, 1), np.int32)\n                val.load_from_np(self.output_ids[left:right, pos-1:pos])\n        else:  # load from the last layer\n            val = self.hidden[i][j-1][k].pop().move(dst)\n        self.hidden[i][j][k].store(val)\n\n    def store_hidden(self, i, j, k):\n        # Handle corner cases\n        if k == -1:\n            k = self.num_gpu_batches - 1\n            j -= 1\n        if j == -1:\n            j = self.num_layers - 1\n            i -= 1\n            if i == -1:\n                return\n\n        # Store to hidden states buffers\n        if j == self.num_layers - 1:  # store to output\n            gpu_batch_size = self.policy.gpu_batch_size\n            left, right = k * gpu_batch_size, (k + 1) * gpu_batch_size\n            ids = self.hidden[i][j][k].pop().data.detach().cpu().numpy()\n            pos = self.task.prompt_len + i\n            if self.task.stop:\n                stopped = self.stopped[left:right]\n                self.output_ids[left:right, pos:pos+1] = np.where(\n                    stopped, self.config.pad_token_id, ids)\n                stopped[:] = np.logical_or(stopped, ids == self.task.stop)\n            else:\n                self.output_ids[left:right, pos:pos+1] = ids\n        else:  # move to home\n            x = self.hidden[i][j][k]\n            if x.val:  # x may already be moved due to overlapping\n                x.val = x.val.move(self.act_home)\n\n    def compute_layer(self, i, j, k):\n        # Update the hidden in place\n        # Clear the weight_read_buf if it is the last gpu batch\n        # Clear the cache_read_buf\n        # Run layer computation\n        self.layers[j].forward(self.hidden[i][j][k], self.cache_read_buf[j][k],\n            self.weight_read_buf[j], self.attention_mask[k],\n            self.cache_write_buf[j][k], i, k)\n\n    def sync(self):\n        self.env.disk.synchronize()\n        torch.cuda.synchronize()\n\n    def init_all_weights(self):\n        self.weight_home = array_1d(self.num_layers, ValueHolder)\n        for j in range(self.num_layers):\n            self.init_weight(j)\n\n    def delete_all_weights(self):\n        for j in range(self.num_layers):\n            self.delete_weight(j, 0)\n\n    def update_attention_mask(self, i, k):\n        if i > 0:\n            mask = self.attention_mask[k]\n            assert mask.val is not None\n            mask.val = mask.val.device.extend_attention_mask(mask.val, [True])\n            return\n\n        gpu_batch_size = self.policy.gpu_batch_size\n        left = k * gpu_batch_size\n        right = left + gpu_batch_size\n        input_ids = self.output_ids[left:right, :self.task.prompt_len]\n\n        attention_compute = (self.env.cpu if self.policy.cpu_cache_compute\n            else self.env.gpu)\n        val = attention_compute.allocate(\n            (self.policy.gpu_batch_size, self.task.prompt_len), bool)\n        val.load_from_np((input_ids != self.config.pad_token_id))\n        self.attention_mask[k].store(val)\n\n    def generate(self,\n                 inputs: Union[np.array, List[List[int]]],\n                 max_new_tokens: int = 32,\n                 do_sample: bool = False,\n                 temperature: float = 1.0,\n                 stop: Optional[int] = None,\n                 debug_mode: Optional[str] = None,\n                 cut_gen_len: Optional[int] = None,\n                 verbose: int = 0):\n        task = Task(\n            inputs=inputs,\n            prompt_len=len(inputs[0]),\n            gen_len=max_new_tokens,\n            cut_gen_len=cut_gen_len,\n            do_sample=do_sample,\n            temperature=temperature,\n            stop=stop,\n        )\n        num_layers = self.num_layers\n        num_gpu_batches = self.num_gpu_batches\n        gpu_batch_size = self.policy.gpu_batch_size\n        overlap = self.policy.overlap\n        prompt_len, gen_len = task.prompt_len, task.gen_len\n        self.execute_gen_len = task.cut_gen_len if task.cut_gen_len else task.gen_len\n\n        # Output token ids\n        self.output_ids = np.full((len(task.inputs), prompt_len + gen_len),\n            self.config.pad_token_id, dtype=np.int32)\n        self.stopped = np.zeros((len(task.inputs), 1), dtype=bool)\n        self.output_ids[:, :prompt_len] = np.asarray(task.inputs)\n        assert gpu_batch_size * num_gpu_batches == len(task.inputs)\n\n        # Intermediate tensors\n        # The following buffers store values used\n        # for the i-th token, j-th layer, k-th gpu batch.\n        num_layers, num_gpu_batches = self.num_layers, self.policy.num_gpu_batches\n        for j in range(num_layers):\n            for k in range(num_gpu_batches):\n                self.cache_home[j][k].clear()\n                self.cache_read_buf[j][k].clear()\n                self.cache_write_buf[j][k].clear()\n        for j in range(num_layers):\n            self.weight_read_buf[j].clear()\n        for k in range(num_gpu_batches):\n            self.attention_mask[k].clear()\n        self.hidden = array_3d(gen_len, num_layers, num_gpu_batches, ValueHolder)\n\n        # Init cache\n        self.set_task(task)\n        for j in range(num_layers):\n            for k in range(num_gpu_batches):\n                self.init_cache(j, k)\n        if self.policy.cpu_cache_compute:\n            self.env.cpu.init_attention_compute_workspace(self.config, self.task, self.policy)\n\n        # Generate\n        if debug_mode is None:\n            if not overlap:\n                # No overlap, easy to understand, suitable for debugging\n                self.generation_loop_normal()\n            else:\n                # Overlap I/O and compute\n                if num_gpu_batches == 1:\n                    self.generation_loop_overlap_single_batch()\n                else:\n                    self.generation_loop_overlap_multi_batch()\n        elif debug_mode == \"fewer_batch\":\n            # Run fewer layeres and batches for debugging\n            if num_gpu_batches == 1:\n                self.generation_loop_debug_single_batch()\n            else:\n                self.generation_loop_debug_multi_batch()\n        elif debug_mode == \"breakdown\":\n            # No overlap, fewer batches, execution time breakdown\n            self.generation_loop_debug_normal()\n        else:\n            raise ValueError(\"Invalid debug mode: {debug_mode}\")\n\n        # Delete cache\n        for j in range(num_layers):\n            for k in range(num_gpu_batches):\n                self.delete_cache(j, k)\n        if self.policy.cpu_cache_compute:\n            self.env.cpu.del_attention_compute_workspace()\n\n        return self.output_ids\n\n    def generation_loop_normal(self):\n        for i in range(self.execute_gen_len):\n            timers(\"generate\").start()\n            for k in range(self.num_gpu_batches):\n                self.update_attention_mask(i, k)\n            for j in range(self.num_layers):\n                for k in range(self.num_gpu_batches):\n                    self.load_weight(i, j, k, overlap=False)\n\n                for k in range(self.num_gpu_batches):\n                    self.load_cache(i, j, k, overlap=False)\n                    self.load_hidden(i, j, k)\n                    self.compute_layer(i, j, k)\n                    self.store_hidden(i, j, k)\n                    self.store_cache(i, j, k, overlap=False)\n            timers(\"generate\").stop()\n\n    def generation_loop_debug_normal(self):\n        execute_num_batches = 20\n        batch_ct = 0\n        pbar = tqdm(total=execute_num_batches)\n        timers(\"prefill_total\").reset()\n        timers(\"decoding_gpu_batch\").reset()\n\n        timers(\"load_weight\").reset()\n        timers(\"load_cache_prefill\").reset()\n        timers(\"load_cache_decoding\").reset()\n        timers(\"store_cache_prefill\").reset()\n        timers(\"store_cache_decoding\").reset()\n        timers(\"compute_layer_prefill\").reset()\n        timers(\"compute_layer_decoding\").reset()\n        load_weight_timer = timers(\"load_weight\")\n\n        for i in range(self.execute_gen_len):\n            if i == 0:\n                timers(\"prefill_total\").start()\n                load_cache_timer = timers(\"load_cache_prefill\")\n                store_cache_timer = timers(\"store_cache_prefill\")\n                compute_layer_timer = timers(\"compute_layer_prefill\")\n            else:\n                load_cache_timer = timers(\"load_cache_decoding\")\n                store_cache_timer = timers(\"store_cache_decoding\")\n                compute_layer_timer = timers(\"compute_layer_decoding\")\n\n            for k in range(self.num_gpu_batches):\n                self.update_attention_mask(i, k)\n\n            for j in range(self.num_layers):\n                if i > 0: timers(\"decoding_gpu_batch\").start()\n\n                load_weight_timer.start(self.sync)\n                for k in range(self.num_gpu_batches):\n                    self.load_weight(i, j, k)\n                load_weight_timer.stop(self.sync)\n\n                for k in range(self.num_gpu_batches):\n                    load_cache_timer.start(self.sync)\n                    self.load_cache(i, j, k)\n                    load_cache_timer.stop(self.sync)\n                    self.load_hidden(i, j, k)\n                    compute_layer_timer.start(self.sync)\n                    self.compute_layer(i, j, k)\n                    compute_layer_timer.stop(self.sync)\n                    self.store_hidden(i, j, k)\n                    store_cache_timer.start(self.sync)\n                    self.store_cache(i, j, k)\n                    store_cache_timer.stop(self.sync)\n\n                if i > 0:\n                    timers(\"decoding_gpu_batch\").stop()\n                    pbar.update(1)\n                    batch_ct += 1\n                if batch_ct >= execute_num_batches: break\n            if batch_ct >= execute_num_batches: break\n            if i == 0: timers(\"prefill_total\").stop(self.sync)\n\n        # Convert \"decoding_gpu_batch\" timer to \"generate\" timer\n        batch_cost = np.mean(timers(\"decoding_gpu_batch\").costs[10:])\n        for i in range(self.execute_gen_len):\n            if i == 0:\n                timers(\"generate\").costs.append(timers(\"prefill_total\").costs[0])\n            else:\n                timers(\"generate\").costs.append(self.num_layers * batch_cost)\n\n        # Debug the costs of individual functions\n        print(f\"#layers: {self.num_layers}\")\n\n        print(f\"#batches prefill:  \"\n              f\"{self.num_layers * self.num_gpu_batches}\")\n        print(f\"#batches decoding: \"\n              f\"{(self.task.gen_len - 1) * self.num_layers * self.num_gpu_batches}\")\n        print(f\"load_weight            (per-layer)\"\n              f\": {np.mean(timers('load_weight').costs):.6f} s\")\n        for stage in [\"prefill\", \"decoding\"]:\n            for func in [\"load_cache\", \"store_cache\", \"compute_layer\"]:\n                name = func + \"_\" + stage\n                costs = timers(name).costs\n                print(f\"{name:22s} (per-batch): {np.mean(costs):.6f} s\")\n\n    def generation_loop_overlap_single_batch(self):\n        # Prologue\n        for k in range(self.num_gpu_batches):\n            self.load_weight(0, 0, k)\n        self.sync()\n\n        # Generate\n        for i in range(self.execute_gen_len):\n            timers(\"generate\").start()\n            self.update_attention_mask(i, 0)\n            for j in range(self.num_layers):\n                self.load_weight(i, j+1, 0)\n                self.load_cache(i, j+1, 0)\n                self.load_hidden(i, j, 0)\n                self.compute_layer(i, j, 0)\n                self.store_cache(i, j-1, 0)\n                self.store_hidden(i, j, 0)\n                self.sync()\n            timers(\"generate\").stop()\n\n            if self.task.stop and np.all(self.stopped):\n                break\n\n    def generation_loop_overlap_multi_batch(self):\n        # Prologue\n        for k in range(self.num_gpu_batches):\n            self.load_weight(0, 0, k)\n        self.load_hidden(0, 0, 0)\n        self.sync()\n\n        # Generate\n        for i in range(self.execute_gen_len):\n            timers(\"generate\").start()\n            for k in range(self.num_gpu_batches):\n                self.update_attention_mask(i, k)\n            for j in range(self.num_layers):\n                for k in range(self.num_gpu_batches):\n                    self.load_weight(i, j+1, k)\n                    self.load_cache(i, j, k+1)\n                    self.store_hidden(i, j, k-1)\n                    self.load_hidden(i, j, k+1)\n                    self.compute_layer(i, j, k)\n                    self.store_cache(i, j, k-1)\n                    self.sync()\n            timers(\"generate\").stop()\n\n        # Epilogue\n        self.store_hidden(\n            self.execute_gen_len-1, self.num_layers-1, self.num_gpu_batches-1)\n\n    def generation_loop_debug_single_batch(self):\n        execute_num_batches = 20\n        batch_ct = 0\n        pbar = tqdm(total=execute_num_batches)\n        timers(\"prefill\").reset()\n        timers(\"decoding_gpu_batch\").reset()\n\n        # Prologue\n        for k in range(self.num_gpu_batches):\n            self.load_weight(0, 0, k)\n        self.sync()\n\n        # Generate\n        for i in range(self.execute_gen_len):\n            if i == 0: timers(\"prefill\").start()\n            self.update_attention_mask(i, 0)\n            for j in range(self.num_layers):\n                if i > 0: timers(\"decoding_gpu_batch\").start()\n                self.load_weight(i, j+1, 0)\n                self.load_cache(i, j+1, 0)\n                self.load_hidden(i, j, 0)\n                self.compute_layer(i, j, 0)\n                self.store_cache(i, j-1, 0)\n                self.store_hidden(i, j, 0)\n                self.sync()\n\n                if i > 0:\n                    timers(\"decoding_gpu_batch\").stop()\n                    pbar.update(1)\n                    batch_ct += 1\n                if batch_ct >= execute_num_batches: break\n            if batch_ct >= execute_num_batches: break\n            if i == 0: timers(\"prefill\").stop()\n\n        # Convert \"decoding_gpu_batch\" timer to \"generate\" timer\n        batch_cost = np.mean(timers(\"decoding_gpu_batch\").costs[10:])\n        for i in range(self.execute_gen_len):\n            if i == 0:\n                timers(\"generate\").costs.append(timers(\"prefill\").costs[0])\n            else:\n                timers(\"generate\").costs.append(self.num_layers * batch_cost)\n\n    def generation_loop_debug_multi_batch(self):\n        execute_num_batches = 20\n        batch_ct = 0\n        pbar = tqdm(total=execute_num_batches)\n        timers(\"prefill\").reset()\n        timers(\"decoding_gpu_batch\").reset()\n\n        # Prologue\n        for k in range(self.num_gpu_batches):\n            self.load_weight(0, 0, k)\n        self.load_hidden(0, 0, 0)\n        self.sync()\n\n        # Generate\n        for i in range(self.execute_gen_len):\n            if i == 0: timers(\"prefill\").start()\n            for k in range(self.num_gpu_batches):\n                self.update_attention_mask(i, k)\n            for j in range(self.num_layers):\n                if i > 0: timers(\"decoding_gpu_batch\").start()\n                for k in range(self.num_gpu_batches):\n                    self.load_weight(i, j+1, k)\n                    self.load_cache(i, j, k+1)\n                    self.store_hidden(i, j, k-1)\n                    self.load_hidden(i, j, k+1)\n                    self.compute_layer(i, j, k)\n                    self.store_cache(i, j, k-1)\n                    self.sync()\n\n                if i > 0:\n                    timers(\"decoding_gpu_batch\").stop()\n                    pbar.update(1)\n                    batch_ct += 1\n                if batch_ct >= execute_num_batches: break\n            if batch_ct >= execute_num_batches: break\n            if i == 0: timers(\"prefill\").stop()\n\n        # Convert \"decoding_gpu_batch\" timer to \"generate\" timer\n        batch_cost = np.mean(timers(\"decoding_gpu_batch\").costs[10:])\n        for i in range(self.execute_gen_len):\n            if i == 0:\n                timers(\"generate\").costs.append(timers(\"prefill\").costs[0])\n            else:\n                timers(\"generate\").costs.append(self.num_layers * batch_cost)\n\n    def __del__(self):\n        self.delete_all_weights()",
            "Examples": [
                "\n"
            ]
        }
    ]
}