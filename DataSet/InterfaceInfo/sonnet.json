{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/sonnet",
    "API_Calls": [
        {
            "Name": "call_Sequential",
            "Description": "call Sequential",
            "Code": "# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\"\"\"Trivial convnet learning MNIST.\"\"\"\n\nfrom typing import Dict\n\nfrom absl import app\nimport sonnet as snt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n\ndef mnist(split: str, batch_size: int) -> tf.data.Dataset:\n  \"\"\"Returns a tf.data.Dataset with MNIST image/label pairs.\"\"\"\n\n  def preprocess_dataset(images, labels):\n    # Mnist images are int8 [0, 255], we cast and rescale to float32 [-1, 1].\n    images = ((tf.cast(images, tf.float32) / 255.) - .5) * 2.\n    return images, labels\n\n  dataset = tfds.load(\n      name=\"mnist\",\n      split=split,\n      shuffle_files=split == \"train\",\n      as_supervised=True)\n  dataset = dataset.map(preprocess_dataset)\n  dataset = dataset.shuffle(buffer_size=4 * batch_size)\n  dataset = dataset.batch(batch_size)\n  # Cache the result of the data pipeline to avoid recomputation. The pipeline\n  # is only ~100MB so this should not be a significant cost and will afford a\n  # decent speedup.\n  dataset = dataset.cache()\n  # Prefetching batches onto the GPU will help avoid us being too input bound.\n  # We allow tf.data to determine how much to prefetch since this will vary\n  # between GPUs.\n  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n  return dataset\n\n\ndef train_step(\n    model: snt.Module,\n    optimizer: snt.Optimizer,\n    images: tf.Tensor,\n    labels: tf.Tensor,\n) -> tf.Tensor:\n  \"\"\"Runs a single training step of the model on the given input.\"\"\"\n  with tf.GradientTape() as tape:\n    logits = model(images)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits)\n    loss = tf.reduce_mean(loss)\n  variables = model.trainable_variables\n  gradients = tape.gradient(loss, variables)\n  optimizer.apply(gradients, variables)\n  return loss\n\n\n@tf.function\ndef train_epoch(\n    model: snt.Module,\n    optimizer: snt.Optimizer,\n    dataset: tf.data.Dataset,\n) -> tf.Tensor:\n  loss = 0.\n  for images, labels in dataset:\n    loss = train_step(model, optimizer, images, labels)\n  return loss\n\n\n@tf.function\ndef test_accuracy(\n    model: snt.Module,\n    dataset: tf.data.Dataset,\n) -> Dict[str, tf.Tensor]:\n  \"\"\"Computes accuracy on the test set.\"\"\"\n  correct, total = 0, 0\n  for images, labels in dataset:\n    preds = tf.argmax(model(images), axis=1)\n    correct += tf.math.count_nonzero(tf.equal(preds, labels), dtype=tf.int32)\n    total += tf.shape(labels)[0]\n  accuracy = (correct / tf.cast(total, tf.int32)) * 100.\n  return {\"accuracy\": accuracy, \"incorrect\": total - correct}\n\n\ndef main(unused_argv):\n  del unused_argv\n\n  model = snt.Sequential([\n      snt.Conv2D(32, 3, 1),\n      tf.nn.relu,\n      snt.Conv2D(32, 3, 1),\n      tf.nn.relu,\n      snt.Flatten(),\n      snt.Linear(10),\n  ])\n\n  optimizer = snt.optimizers.SGD(0.1)\n\n  train_data = mnist(\"train\", batch_size=128)\n  test_data = mnist(\"test\", batch_size=1000)\n\n  for epoch in range(5):\n    train_loss = train_epoch(model, optimizer, train_data)\n    test_metrics = test_accuracy(model, test_data)\n    print(\"[Epoch %d] train loss: %.05f, test acc: %.02f%% (%d wrong)\" %\n          (epoch, train_loss, test_metrics[\"accuracy\"],\n           test_metrics[\"incorrect\"]))\n\n\nif __name__ == \"__main__\":\n  app.run(main)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/sonnet/examples/simple_mnist.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "Sequential",
            "Description": "Sequential",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/sonnet/sonnet/src/sequential.py",
            "Implementation": "# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\"\"\"Sequential applies a linear sequence of layers.\"\"\"\n\nfrom typing import Any, Callable, Iterable, Optional\n\nfrom sonnet.src import base\n\n\nclass Sequential(base.Module):\n  \"\"\"Sequential applies a linear chain of modules / callables.\n\n      >>> mlp = snt.Sequential([\n      ...     snt.Linear(1024),\n      ...     tf.nn.relu,\n      ...     snt.Linear(10),\n      ... ])\n      >>> mlp(tf.random.normal([8, 100]))\n      <tf.Tensor: ...>\n\n  Note that `Sequential` is limited in the range of possible architectures\n  it can handle. This is a deliberate design decision; `Sequential` is only\n  meant to be used for the simple case of fusing together modules/ops where\n  the input of a particular module/op is the output of the previous one.\n\n  Another restriction is that it is not possible to have extra arguments in the\n  `__call__` method that are passed to the constituents of the module - for\n  example, if there is a `BatchNorm` module in `Sequential` and the user wishes\n  to switch the `is_training` flag. If this is the desired use case, the\n  recommended solution is to subclass `snt.Module` and implement `__call__`:\n\n      >>> class CustomModule(snt.Module):\n      ...   def __init__(self, name=None):\n      ...     super(CustomModule, self).__init__(name=name)\n      ...     self.conv2d = snt.Conv2D(32, 4, 2)\n      ...     self.bn = snt.BatchNorm()\n      ...\n      ...   def __call__(self, inputs, is_training):\n      ...     outputs = self.conv2d(inputs)\n      ...     outputs = self.bn(outputs, is_training=is_training)\n      ...     outputs = tf.nn.relu(outputs)\n      ...     return outputs\n  \"\"\"\n\n  def __init__(self,\n               layers: Optional[Iterable[Callable[..., Any]]] = None,\n               name: Optional[str] = None):\n    super().__init__(name=name)\n    self._layers = list(layers) if layers is not None else []\n\n  def __call__(self, inputs, *args, **kwargs):\n    outputs = inputs\n    for i, mod in enumerate(self._layers):\n      if i == 0:\n        # Pass additional arguments to the first layer.\n        outputs = mod(outputs, *args, **kwargs)\n      else:\n        outputs = mod(outputs)\n    return outputs\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}