{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/flair",
    "API_Calls": [
        {
            "Name": "Classifier_call",
            "Description": "A test of Classifier",
            "Code": "import pytest\n\nfrom flair.data import Sentence\nfrom flair.models.entity_mention_linking import (\n    Ab3PEntityPreprocessor,\n    BioSynEntityPreprocessor,\n    EntityMentionLinker,\n    load_dictionary,\n)\nfrom flair.nn import Classifier\n\n\ndef test_bel_dictionary():\n    \"\"\"Check data in dictionary is what we expect.\n\n    Hard to define a good test as dictionaries are DYNAMIC,\n    i.e. they can change over time.\n    \"\"\"\n    dictionary = load_dictionary(\"disease\")\n    candidate = dictionary.candidates[0]\n    assert candidate.concept_id.startswith((\"MESH:\", \"OMIM:\", \"DO:DOID\"))\n\n    dictionary = load_dictionary(\"ctd-diseases\")\n    candidate = dictionary.candidates[0]\n    assert candidate.concept_id.startswith(\"MESH:\")\n\n    dictionary = load_dictionary(\"ctd-chemicals\")\n    candidate = dictionary.candidates[0]\n    assert candidate.concept_id.startswith(\"MESH:\")\n\n    dictionary = load_dictionary(\"chemical\")\n    candidate = dictionary.candidates[0]\n    assert candidate.concept_id.startswith(\"MESH:\")\n\n    dictionary = load_dictionary(\"ncbi-taxonomy\")\n    candidate = dictionary.candidates[0]\n    assert candidate.concept_id.isdigit()\n\n    dictionary = load_dictionary(\"species\")\n    candidate = dictionary.candidates[0]\n    assert candidate.concept_id.isdigit()\n\n    dictionary = load_dictionary(\"ncbi-gene\")\n    candidate = dictionary.candidates[0]\n    assert candidate.concept_id.isdigit()\n\n    dictionary = load_dictionary(\"gene\")\n    candidate = dictionary.candidates[0]\n    assert candidate.concept_id.isdigit()\n\n\ndef test_biosyn_preprocessing():\n    \"\"\"Check preprocessing does not produce empty strings.\"\"\"\n    preprocessor = BioSynEntityPreprocessor()\n\n    # NOTE: Avoid emtpy string if mentions are just punctutations (e.g. `-` or `(`)\n    for s in [\"-\", \"(\", \")\", \"9\"]:\n        assert len(preprocessor.process_mention(s)) > 0\n        assert len(preprocessor.process_entity_name(s)) > 0\n\n\ndef test_abbrevitation_resolution():\n    \"\"\"Test abbreviation resolution works correctly.\"\"\"\n    preprocessor = Ab3PEntityPreprocessor(preprocessor=BioSynEntityPreprocessor())\n\n    sentences = [\n        Sentence(\"Features of ARCL type II overlap with those of Wrinkly skin syndrome (WSS).\"),\n        Sentence(\"Weaver-Smith syndrome (WSS) is a Mendelian disorder of the epigenetic machinery.\"),\n    ]\n\n    preprocessor.initialize(sentences)\n\n    mentions = [\"WSS\", \"WSS\"]\n    for idx, (mention, sentence) in enumerate(zip(mentions, sentences)):\n        mention = preprocessor.process_mention(mention, sentence)\n        if idx == 0:\n            assert mention == \"wrinkly skin syndrome\"\n        elif idx == 1:\n            assert mention == \"weaver smith syndrome\"\n\n\n@pytest.mark.integration()\ndef test_biomedical_entity_linking():\n    sentence = Sentence(\n        \"The mutation in the ABCD1 gene causes X-linked adrenoleukodystrophy, \"\n        \"a neurodegenerative disease, which is exacerbated by exposure to high \"\n        \"levels of mercury in dolphin populations.\",\n    )\n\n    tagger = Classifier.load(\"hunflair\")\n    tagger.predict(sentence)\n\n    linker = EntityMentionLinker.load(\"disease-linker\")\n    linker.predict(sentence)\n\n    for span in sentence.get_spans():\n        print(span)\n\n\ndef test_legacy_sequence_tagger():\n    sentence = Sentence(\"Behavioral abnormalities in the Fmr1 KO2 Mouse Model of Fragile X Syndrome\")\n\n    legacy_tagger = Classifier.load(\"hunflair\")\n    legacy_tagger.predict(sentence)\n\n    disease_linker = EntityMentionLinker.load(\"hunflair/biosyn-sapbert-ncbi-disease\")\n    disease_linker.predict(sentence, pred_label_type=\"disease-nen\")\n\n    assert disease_linker._warned_legacy_sequence_tagger\n\n\nif __name__ == \"__main__\":\n    test_bel_dictionary()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/flair/tests/test_biomedical_entity_linking.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "Classifier",
            "Description": "这是最上层的类，通过这个类load加载模型，得到模型后进行predict方法进行推理",
            "Implementation": "import inspect\nimport itertools\nimport logging\nimport typing\nfrom abc import ABC, abstractmethod\nfrom collections import Counter\nfrom pathlib import Path\nfrom typing import Any, Optional, Union\n\nimport torch.nn\nfrom torch import Tensor\nfrom torch.nn.modules.loss import _Loss\nfrom torch.utils.data.dataset import Dataset\nfrom tqdm import tqdm\n\nimport flair\nfrom flair.class_utils import get_non_abstract_subclasses\nfrom flair.data import DT, DT2, Corpus, Dictionary, Sentence, _iter_dataset\nfrom flair.datasets import DataLoader, FlairDatapointDataset\nfrom flair.distributed_utils import is_main_process\nfrom flair.embeddings import Embeddings\nfrom flair.embeddings.base import load_embeddings\nfrom flair.file_utils import Tqdm, load_torch_state\nfrom flair.training_utils import EmbeddingStorageMode, Result, store_embeddings\n\n\n\n\nclass Classifier(Model[DT], typing.Generic[DT], ReduceTransformerVocabMixin, ABC):\n    \"\"\"Abstract base class for all Flair models that do classification.\n\n    The classifier inherits from flair.nn.Model and adds unified functionality for both, single- and multi-label\n    classification and evaluation. Therefore, it is ensured to have a fair comparison between multiple classifiers.\n    \"\"\"\n\n    def evaluate(\n        self,\n        data_points: Union[list[DT], Dataset],\n        gold_label_type: str,\n        out_path: Optional[Union[str, Path]] = None,\n        embedding_storage_mode: EmbeddingStorageMode = \"none\",\n        mini_batch_size: int = 32,\n        main_evaluation_metric: tuple[str, str] = (\"micro avg\", \"f1-score\"),\n        exclude_labels: Optional[list[str]] = None,\n        gold_label_dictionary: Optional[Dictionary] = None,\n        return_loss: bool = True,\n        **kwargs,\n    ) -> Result:\n        exclude_labels = exclude_labels if exclude_labels is not None else []\n\n        import numpy as np\n        import sklearn\n\n        # make sure <unk> is contained in gold_label_dictionary, if given\n        if gold_label_dictionary and not gold_label_dictionary.add_unk:\n            raise AssertionError(\"gold_label_dictionary must have add_unk set to true in initialization.\")\n\n        # read Dataset into data loader, if list of sentences passed, make Dataset first\n        if not isinstance(data_points, Dataset):\n            data_points = FlairDatapointDataset(data_points)\n\n        with torch.no_grad():\n            # loss calculation\n            eval_loss = torch.zeros(1, device=flair.device)\n            average_over = 0\n\n            # variables for printing\n            lines: list[str] = []\n\n            # variables for computing scores\n            all_spans: set[str] = set()\n            all_true_values = {}\n            all_predicted_values = {}\n\n            loader = DataLoader(data_points, batch_size=mini_batch_size)\n\n            sentence_id = 0\n            for batch in Tqdm.tqdm(loader, disable=not is_main_process()):\n                # remove any previously predicted labels\n                for datapoint in batch:\n                    datapoint.remove_labels(\"predicted\")\n\n                # predict for batch\n                loss_and_count = self.predict(\n                    batch,\n                    embedding_storage_mode=embedding_storage_mode,\n                    mini_batch_size=mini_batch_size,\n                    label_name=\"predicted\",\n                    return_loss=return_loss,\n                )\n\n                if return_loss:\n                    if isinstance(loss_and_count, tuple):\n                        average_over += loss_and_count[1]\n                        eval_loss += loss_and_count[0]\n                    else:\n                        eval_loss += loss_and_count\n\n                # get the gold labels\n                for datapoint in batch:\n                    for gold_label in datapoint.get_labels(gold_label_type):\n                        representation = str(sentence_id) + \": \" + gold_label.unlabeled_identifier\n\n                        value = gold_label.value\n                        if gold_label_dictionary and gold_label_dictionary.get_idx_for_item(value) == 0:\n                            value = \"<unk>\"\n\n                        if representation not in all_true_values:\n                            all_true_values[representation] = [value]\n                        else:\n                            all_true_values[representation].append(value)\n\n                        if representation not in all_spans:\n                            all_spans.add(representation)\n\n                    for predicted_span in datapoint.get_labels(\"predicted\"):\n                        representation = str(sentence_id) + \": \" + predicted_span.unlabeled_identifier\n\n                        # add to all_predicted_values\n                        if representation not in all_predicted_values:\n                            all_predicted_values[representation] = [predicted_span.value]\n                        else:\n                            all_predicted_values[representation].append(predicted_span.value)\n\n                        if representation not in all_spans:\n                            all_spans.add(representation)\n\n                    sentence_id += 1\n\n                store_embeddings(batch, embedding_storage_mode)\n\n                # make printout lines\n                if out_path:\n                    lines.extend(self._print_predictions(batch, gold_label_type))\n\n            # convert true and predicted values to two span-aligned lists\n            true_values_span_aligned = []\n            predicted_values_span_aligned = []\n            for span in all_spans:\n                list_of_gold_values_for_span = all_true_values.get(span, [\"O\"])\n                # delete excluded labels if exclude_labels is given\n                for excluded_label in exclude_labels:\n                    if excluded_label in list_of_gold_values_for_span:\n                        list_of_gold_values_for_span.remove(excluded_label)\n                # if after excluding labels, no label is left, ignore the datapoint\n                if not list_of_gold_values_for_span:\n                    continue\n                true_values_span_aligned.append(list_of_gold_values_for_span)\n                predicted_values_span_aligned.append(all_predicted_values.get(span, [\"O\"]))\n\n            # write all_predicted_values to out_file if set\n            if out_path:\n                with open(Path(out_path), \"w\", encoding=\"utf-8\") as outfile:\n                    outfile.write(\"\".join(lines))\n\n            # make the evaluation dictionary\n            evaluation_label_dictionary = Dictionary(add_unk=False)\n            evaluation_label_dictionary.add_item(\"O\")\n            for true_values in all_true_values.values():\n                for label in true_values:\n                    evaluation_label_dictionary.add_item(label)\n            for predicted_values in all_predicted_values.values():\n                for label in predicted_values:\n                    evaluation_label_dictionary.add_item(label)\n\n        # check if this is a multi-label problem\n        multi_label = False\n        for true_instance, predicted_instance in zip(true_values_span_aligned, predicted_values_span_aligned):\n            if len(true_instance) > 1 or len(predicted_instance) > 1:\n                multi_label = True\n                break\n\n        log.debug(f\"Evaluating as a multi-label problem: {multi_label}\")\n\n        # compute numbers by formatting true and predicted such that Scikit-Learn can use them\n        y_true = []\n        y_pred = []\n        if multi_label:\n            # multi-label problems require a multi-hot vector for each true and predicted label\n            for true_instance in true_values_span_aligned:\n                y_true_instance = np.zeros(len(evaluation_label_dictionary), dtype=int)\n                for true_value in true_instance:\n                    y_true_instance[evaluation_label_dictionary.get_idx_for_item(true_value)] = 1\n                y_true.append(y_true_instance.tolist())\n\n            for predicted_values in predicted_values_span_aligned:\n                y_pred_instance = np.zeros(len(evaluation_label_dictionary), dtype=int)\n                for predicted_value in predicted_values:\n                    y_pred_instance[evaluation_label_dictionary.get_idx_for_item(predicted_value)] = 1\n                y_pred.append(y_pred_instance.tolist())\n        else:\n            # single-label problems can do with a single index for each true and predicted label\n            y_true = [\n                evaluation_label_dictionary.get_idx_for_item(true_instance[0])\n                for true_instance in true_values_span_aligned\n            ]\n            y_pred = [\n                evaluation_label_dictionary.get_idx_for_item(predicted_instance[0])\n                for predicted_instance in predicted_values_span_aligned\n            ]\n\n        # now, calculate evaluation numbers\n        target_names = []\n        labels = []\n\n        counter = Counter(itertools.chain.from_iterable(all_true_values.values()))\n        counter.update(list(itertools.chain.from_iterable(all_predicted_values.values())))\n\n        for label_name, _count in counter.most_common():\n            if label_name == \"O\":\n                continue\n            target_names.append(label_name)\n            labels.append(evaluation_label_dictionary.get_idx_for_item(label_name))\n\n        # there is at least one gold label or one prediction (default)\n        if len(all_true_values) + len(all_predicted_values) > 1:\n            classification_report = sklearn.metrics.classification_report(\n                y_true,\n                y_pred,\n                digits=4,\n                target_names=target_names,\n                zero_division=0,\n                labels=labels,\n            )\n\n            classification_report_dict = sklearn.metrics.classification_report(\n                y_true,\n                y_pred,\n                target_names=target_names,\n                zero_division=0,\n                output_dict=True,\n                labels=labels,\n            )\n\n            # compute accuracy separately as it is not always in classification_report (e.g. when micro avg exists)\n            accuracy_score = round(sklearn.metrics.accuracy_score(y_true, y_pred), 4)\n\n            # if there is only one label, then \"micro avg\" = \"macro avg\"\n            if len(target_names) == 1:\n                classification_report_dict[\"micro avg\"] = classification_report_dict[\"macro avg\"]\n\n            # The \"micro avg\" appears only in the classification report if no prediction is possible.\n            # Otherwise, it is identical to the \"macro avg\". In this case, we add it to the report.\n            if \"micro avg\" not in classification_report_dict:\n                classification_report_dict[\"micro avg\"] = {}\n                for metric_key in classification_report_dict[\"macro avg\"]:\n                    if metric_key != \"support\":\n                        classification_report_dict[\"micro avg\"][metric_key] = classification_report_dict[\"accuracy\"]\n                    else:\n                        classification_report_dict[\"micro avg\"][metric_key] = classification_report_dict[\"macro avg\"][\n                            \"support\"\n                        ]\n\n            detailed_result = (\n                \"\\nResults:\"\n                f\"\\n- F-score (micro) {round(classification_report_dict['micro avg']['f1-score'], 4)}\"\n                f\"\\n- F-score (macro) {round(classification_report_dict['macro avg']['f1-score'], 4)}\"\n                f\"\\n- Accuracy {accuracy_score}\"\n                \"\\n\\nBy class:\\n\" + classification_report\n            )\n\n            # Create and populate score object for logging with all evaluation values, plus the loss\n            scores: dict[Union[tuple[str, ...], str], Any] = {}\n\n            for avg_type in (\"micro avg\", \"macro avg\"):\n                for metric_type in (\"f1-score\", \"precision\", \"recall\"):\n                    scores[(avg_type, metric_type)] = classification_report_dict[avg_type][metric_type]\n\n            scores[\"accuracy\"] = accuracy_score\n\n            if average_over > 0:\n                eval_loss /= average_over\n            scores[\"loss\"] = eval_loss.item()\n\n            return Result(\n                main_score=classification_report_dict[main_evaluation_metric[0]][main_evaluation_metric[1]],\n                detailed_results=detailed_result,\n                classification_report=classification_report_dict,\n                scores=scores,\n            )\n\n        else:\n            # issue error and default all evaluation numbers to 0.\n            error_text = (\n                f\"It was not possible to compute evaluation values because: \\n\"\n                f\"- The evaluation data has no gold labels for label_type='{gold_label_type}'!\\n\"\n                f\"- And no predictions were made!\\n\"\n                \"Double check your corpus (if the test split has labels), and how you initialize the ModelTrainer!\"\n            )\n\n            return Result(\n                main_score=0.0,\n                detailed_results=error_text,\n                classification_report={},\n                scores={\"loss\": 0.0},\n            )\n\n    @abstractmethod\n    def predict(\n        self,\n        sentences: Union[list[DT], DT],\n        mini_batch_size: int = 32,\n        return_probabilities_for_all_classes: bool = False,\n        verbose: bool = False,\n        label_name: Optional[str] = None,\n        return_loss: bool = False,\n        embedding_storage_mode: EmbeddingStorageMode = \"none\",\n    ):\n        \"\"\"Uses the model to predict labels for a given set of data points.\n\n        The method does not directly return the predicted labels. Rather, labels are added as :class:`flair.data.Label` objects to\n        the respective data points. You can then access these predictions by calling :func:`flair.data.DataPoint.get_labels`\n        on each data point that you passed through this method.\n\n        Args:\n            sentences: The data points for which the model should predict labels, most commonly Sentence objects.\n            mini_batch_size: The mini batch size to use. Setting this value higher typically makes predictions faster,\n                but also costs more memory.\n            return_probabilities_for_all_classes: If set to True, the model will store probabilities for all classes\n                instead of only the predicted class.\n            verbose: If set to True, will display a progress bar while predicting. By default, this parameter is set to False.\n            return_loss: Set this to True to return loss (only possible if gold labels are set for the sentences).\n            label_name: Optional parameter that if set, changes the identifier of the label type that is predicted.  # noqa: E501\n            embedding_storage_mode: Default is 'none' which is always best. Only set to 'cpu' or 'gpu' if you wish to not only predict, but also keep the generated embeddings in CPU or GPU memory respectively. 'gpu' to store embeddings in GPU memory.  # noqa: E501\n        \"\"\"\n        raise NotImplementedError\n\n    def _print_predictions(self, batch: list[DT], gold_label_type: str) -> list[str]:\n        lines = []\n        for datapoint in batch:\n            # check if there is a label mismatch\n            g = [label.labeled_identifier for label in datapoint.get_labels(gold_label_type)]\n            p = [label.labeled_identifier for label in datapoint.get_labels(\"predicted\")]\n            g.sort()\n            p.sort()\n            correct_string = \" -> MISMATCH!\\n\" if g != p else \"\"\n            # print info\n            eval_line = (\n                f\"{datapoint.text}\\n\"\n                f\" - Gold: {', '.join(label.value if label.data_point == datapoint else label.labeled_identifier for label in datapoint.get_labels(gold_label_type))}\\n\"\n                f\" - Pred: {', '.join(label.value if label.data_point == datapoint else label.labeled_identifier for label in datapoint.get_labels('predicted'))}\\n{correct_string}\\n\"\n            )\n            lines.append(eval_line)\n        return lines\n\n    def get_used_tokens(\n        self, corpus: Corpus, context_length: int = 0, respect_document_boundaries: bool = True\n    ) -> typing.Iterable[list[str]]:\n        for sentence in _iter_dataset(corpus.get_all_sentences()):\n            yield [t.text for t in sentence]\n            yield [t.text for t in sentence.left_context(context_length, respect_document_boundaries)]\n            yield [t.text for t in sentence.right_context(context_length, respect_document_boundaries)]\n\n    @classmethod\n    def load(cls, model_path: Union[str, Path, dict[str, Any]]) -> \"Classifier\":\n        from typing import cast\n\n        return cast(\"Classifier\", super().load(model_path=model_path))",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/flair/flair/nn/model.py",
            "Examples": [
                {
                    "Description": "调用这个类的load方法，加载模型",
                    "Type": "easy_test",
                    "Code": "from flair.data import Sentence\nfrom flair.nn import Classifier\n\n# make a sentence\nsentence = Sentence('I love Berlin .')\n\n# load the NER tagger\ntagger = Classifier.load('/mnt/autor_name/haoTingDeWenJianJia/flair/load_model_path/pytorch_model.bin')\n\n# run NER over sentence\ntagger.predict(sentence)\n\n# print the sentence with all annotations\nprint(sentence)\n"
                }
            ]
        }
    ]
}