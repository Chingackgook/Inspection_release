{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/consistency_models",
    "API_Calls": [
        {
            "Name": "call_karras_sample",
            "Description": "call_karras_sample",
            "Code": "\"\"\"\nGenerate a large batch of image samples from a model and save them as a large\nnumpy array. This can be used to produce samples for FID evaluation.\n\"\"\"\n\nimport argparse\nimport os\n\nimport numpy as np\nimport torch as th\nimport torch.distributed as dist\n\nfrom cm import dist_util, logger\nfrom cm.script_util import (\n    NUM_CLASSES,\n    model_and_diffusion_defaults,\n    create_model_and_diffusion,\n    add_dict_to_argparser,\n    args_to_dict,\n)\nfrom cm.random_util import get_generator\nfrom cm.karras_diffusion import karras_sample\n\n\ndef main():\n    args = create_argparser().parse_args()\n\n    dist_util.setup_dist()\n    logger.configure()\n\n    if \"consistency\" in args.training_mode:\n        distillation = True\n    else:\n        distillation = False\n\n    logger.log(\"creating model and diffusion...\")\n    model, diffusion = create_model_and_diffusion(\n        **args_to_dict(args, model_and_diffusion_defaults().keys()),\n        distillation=distillation,\n    )\n    model.load_state_dict(\n        dist_util.load_state_dict(args.model_path, map_location=\"cpu\")\n    )\n    model.to(dist_util.dev())\n    if args.use_fp16:\n        model.convert_to_fp16()\n    model.eval()\n\n    logger.log(\"sampling...\")\n    if args.sampler == \"multistep\":\n        assert len(args.ts) > 0\n        ts = tuple(int(x) for x in args.ts.split(\",\"))\n    else:\n        ts = None\n\n    all_images = []\n    all_labels = []\n    generator = get_generator(args.generator, args.num_samples, args.seed)\n\n    while len(all_images) * args.batch_size < args.num_samples:\n        model_kwargs = {}\n        if args.class_cond:\n            classes = th.randint(\n                low=0, high=NUM_CLASSES, size=(args.batch_size,), device=dist_util.dev()\n            )\n            model_kwargs[\"y\"] = classes\n\n        sample = karras_sample(\n            diffusion,\n            model,\n            (args.batch_size, 3, args.image_size, args.image_size),\n            steps=args.steps,\n            model_kwargs=model_kwargs,\n            device=dist_util.dev(),\n            clip_denoised=args.clip_denoised,\n            sampler=args.sampler,\n            sigma_min=args.sigma_min,\n            sigma_max=args.sigma_max,\n            s_churn=args.s_churn,\n            s_tmin=args.s_tmin,\n            s_tmax=args.s_tmax,\n            s_noise=args.s_noise,\n            generator=generator,\n            ts=ts,\n        )\n        sample = ((sample + 1) * 127.5).clamp(0, 255).to(th.uint8)\n        sample = sample.permute(0, 2, 3, 1)\n        sample = sample.contiguous()\n\n        gathered_samples = [th.zeros_like(sample) for _ in range(dist.get_world_size())]\n        dist.all_gather(gathered_samples, sample)  # gather not supported with NCCL\n        all_images.extend([sample.cpu().numpy() for sample in gathered_samples])\n        if args.class_cond:\n            gathered_labels = [\n                th.zeros_like(classes) for _ in range(dist.get_world_size())\n            ]\n            dist.all_gather(gathered_labels, classes)\n            all_labels.extend([labels.cpu().numpy() for labels in gathered_labels])\n        logger.log(f\"created {len(all_images) * args.batch_size} samples\")\n\n    arr = np.concatenate(all_images, axis=0)\n    arr = arr[: args.num_samples]\n    if args.class_cond:\n        label_arr = np.concatenate(all_labels, axis=0)\n        label_arr = label_arr[: args.num_samples]\n    if dist.get_rank() == 0:\n        shape_str = \"x\".join([str(x) for x in arr.shape])\n        out_path = os.path.join(logger.get_dir(), f\"samples_{shape_str}.npz\")\n        logger.log(f\"saving to {out_path}\")\n        if args.class_cond:\n            np.savez(out_path, arr, label_arr)\n        else:\n            np.savez(out_path, arr)\n\n    dist.barrier()\n    logger.log(\"sampling complete\")\n\n\ndef create_argparser():\n    defaults = dict(\n        training_mode=\"edm\",\n        generator=\"determ\",\n        clip_denoised=True,\n        num_samples=10000,\n        batch_size=16,\n        sampler=\"heun\",\n        s_churn=0.0,\n        s_tmin=0.0,\n        s_tmax=float(\"inf\"),\n        s_noise=1.0,\n        steps=40,\n        model_path=\"\",\n        seed=42,\n        ts=\"\",\n    )\n    defaults.update(model_and_diffusion_defaults())\n    parser = argparse.ArgumentParser()\n    add_dict_to_argparser(parser, defaults)\n    return parser\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/consistency_models/scripts/image_sample.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "karras_sample",
            "Description": "karras_sample impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/consistency_models/cm/karras_diffusion.py",
            "Implementation": "\"\"\"\nBased on: https://github.com/crowsonkb/k-diffusion\n\"\"\"\nimport random\n\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom piq import LPIPS\nfrom torchvision.transforms import RandomCrop\nfrom . import dist_util\n\nfrom .nn import mean_flat, append_dims, append_zero\nfrom .random_util import get_generator\n\n\n\ndef karras_sample(\n    diffusion,\n    model,\n    shape,\n    steps,\n    clip_denoised=True,\n    progress=False,\n    callback=None,\n    model_kwargs=None,\n    device=None,\n    sigma_min=0.002,\n    sigma_max=80,  # higher for highres?\n    rho=7.0,\n    sampler=\"heun\",\n    s_churn=0.0,\n    s_tmin=0.0,\n    s_tmax=float(\"inf\"),\n    s_noise=1.0,\n    generator=None,\n    ts=None,\n):\n    if generator is None:\n        generator = get_generator(\"dummy\")\n\n    if sampler == \"progdist\":\n        sigmas = get_sigmas_karras(steps + 1, sigma_min, sigma_max, rho, device=device)\n    else:\n        sigmas = get_sigmas_karras(steps, sigma_min, sigma_max, rho, device=device)\n\n    x_T = generator.randn(*shape, device=device) * sigma_max\n\n    sample_fn = {\n        \"heun\": sample_heun,\n        \"dpm\": sample_dpm,\n        \"ancestral\": sample_euler_ancestral,\n        \"onestep\": sample_onestep,\n        \"progdist\": sample_progdist,\n        \"euler\": sample_euler,\n        \"multistep\": stochastic_iterative_sampler,\n    }[sampler]\n\n    if sampler in [\"heun\", \"dpm\"]:\n        sampler_args = dict(\n            s_churn=s_churn, s_tmin=s_tmin, s_tmax=s_tmax, s_noise=s_noise\n        )\n    elif sampler == \"multistep\":\n        sampler_args = dict(\n            ts=ts, t_min=sigma_min, t_max=sigma_max, rho=diffusion.rho, steps=steps\n        )\n    else:\n        sampler_args = {}\n\n    def denoiser(x_t, sigma):\n        _, denoised = diffusion.denoise(model, x_t, sigma, **model_kwargs)\n        if clip_denoised:\n            denoised = denoised.clamp(-1, 1)\n        return denoised\n\n    x_0 = sample_fn(\n        denoiser,\n        x_T,\n        sigmas,\n        generator,\n        progress=progress,\n        callback=callback,\n        **sampler_args,\n    )\n    return x_0.clamp(-1, 1)",
            "Examples": [
                "\n"
            ]
        }
    ]
}