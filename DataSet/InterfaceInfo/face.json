{
  "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/face",
  "API_Calls": [
    {
      "Name": "face_recognition",
      "Code": "\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport click\nimport os\nimport re\nimport face_recognition.api as face_recognition\nimport multiprocessing\nimport itertools\nimport sys\nimport PIL.Image\nimport numpy as np\n\n\ndef scan_known_people(known_people_folder):\n    known_names = []\n    known_face_encodings = []\n\n    for file in image_files_in_folder(known_people_folder):\n        basename = os.path.splitext(os.path.basename(file))[0]\n        img = face_recognition.load_image_file(file)\n        encodings = face_recognition.face_encodings(img)\n\n        if len(encodings) > 1:\n            click.echo(\"WARNING: More than one face found in {}. Only considering the first face.\".format(file))\n\n        if len(encodings) == 0:\n            click.echo(\"WARNING: No faces found in {}. Ignoring file.\".format(file))\n        else:\n            known_names.append(basename)\n            known_face_encodings.append(encodings[0])\n\n    return known_names, known_face_encodings\n\n\ndef print_result(filename, name, distance, show_distance=False):\n    if show_distance:\n        print(\"{},{},{}\".format(filename, name, distance))\n    else:\n        print(\"{},{}\".format(filename, name))\n\n\ndef test_image(image_to_check, known_names, known_face_encodings, tolerance=0.6, show_distance=False):\n    unknown_image = face_recognition.load_image_file(image_to_check)\n\n    # Scale down image if it's giant so things run a little faster\n    if max(unknown_image.shape) > 1600:\n        pil_img = PIL.Image.fromarray(unknown_image)\n        pil_img.thumbnail((1600, 1600), PIL.Image.LANCZOS)\n        unknown_image = np.array(pil_img)\n\n    unknown_encodings = face_recognition.face_encodings(unknown_image)\n\n    for unknown_encoding in unknown_encodings:\n        distances = face_recognition.face_distance(known_face_encodings, unknown_encoding)\n        result = list(distances <= tolerance)\n\n        if True in result:\n            [print_result(image_to_check, name, distance, show_distance) for is_match, name, distance in zip(result, known_names, distances) if is_match]\n        else:\n            print_result(image_to_check, \"unknown_person\", None, show_distance)\n\n    if not unknown_encodings:\n        # print out fact that no faces were found in image\n        print_result(image_to_check, \"no_persons_found\", None, show_distance)\n\n\ndef image_files_in_folder(folder):\n    return [os.path.join(folder, f) for f in os.listdir(folder) if re.match(r'.*\\.(jpg|jpeg|png)', f, flags=re.I)]\n\n\ndef process_images_in_process_pool(images_to_check, known_names, known_face_encodings, number_of_cpus, tolerance, show_distance):\n    if number_of_cpus == -1:\n        processes = None\n    else:\n        processes = number_of_cpus\n\n    # macOS will crash due to a bug in libdispatch if you don't use 'forkserver'\n    context = multiprocessing\n    if \"forkserver\" in multiprocessing.get_all_start_methods():\n        context = multiprocessing.get_context(\"forkserver\")\n\n    pool = context.Pool(processes=processes)\n\n    function_parameters = zip(\n        images_to_check,\n        itertools.repeat(known_names),\n        itertools.repeat(known_face_encodings),\n        itertools.repeat(tolerance),\n        itertools.repeat(show_distance)\n    )\n\n    pool.starmap(test_image, function_parameters)\n\n\n@click.command()\n@click.argument('known_people_folder')\n@click.argument('image_to_check')\n@click.option('--cpus', default=1, help='number of CPU cores to use in parallel (can speed up processing lots of images). -1 means \"use all in system\"')\n@click.option('--tolerance', default=0.6, help='Tolerance for face comparisons. Default is 0.6. Lower this if you get multiple matches for the same person.')\n@click.option('--show-distance', default=False, type=bool, help='Output face distance. Useful for tweaking tolerance setting.')\ndef main(known_people_folder, image_to_check, cpus, tolerance, show_distance):\n    known_names, known_face_encodings = scan_known_people(known_people_folder)\n\n    # Multi-core processing only supported on Python 3.4 or greater\n    if (sys.version_info < (3, 4)) and cpus != 1:\n        click.echo(\"WARNING: Multi-processing support requires Python 3.4 or greater. Falling back to single-threaded processing!\")\n        cpus = 1\n\n    if os.path.isdir(image_to_check):\n        if cpus == 1:\n            [test_image(image_file, known_names, known_face_encodings, tolerance, show_distance) for image_file in image_files_in_folder(image_to_check)]\n        else:\n            process_images_in_process_pool(image_files_in_folder(image_to_check), known_names, known_face_encodings, cpus, tolerance, show_distance)\n    else:\n        test_image(image_to_check, known_names, known_face_encodings, tolerance, show_distance)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "Description": "人脸识别模块的调用，主要功能是识别图片中的人脸，并输出人名和距离"
    },
    {
      "Name": "face_detection",
      "Code": "\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport click\nimport os\nimport re\nimport face_recognition.api as face_recognition\nimport multiprocessing\nimport sys\nimport itertools\n\n\ndef print_result(filename, location):\n    top, right, bottom, left = location\n    print(\"{},{},{},{},{}\".format(filename, top, right, bottom, left))\n\n\ndef test_image(image_to_check, model, upsample):\n    unknown_image = face_recognition.load_image_file(image_to_check)\n    face_locations = face_recognition.face_locations(unknown_image, number_of_times_to_upsample=upsample, model=model)\n\n    for face_location in face_locations:\n        print_result(image_to_check, face_location)\n\n\ndef image_files_in_folder(folder):\n    return [os.path.join(folder, f) for f in os.listdir(folder) if re.match(r'.*\\.(jpg|jpeg|png)', f, flags=re.I)]\n\n\ndef process_images_in_process_pool(images_to_check, number_of_cpus, model, upsample):\n    if number_of_cpus == -1:\n        processes = None\n    else:\n        processes = number_of_cpus\n\n    # macOS will crash due to a bug in libdispatch if you don't use 'forkserver'\n    context = multiprocessing\n    if \"forkserver\" in multiprocessing.get_all_start_methods():\n        context = multiprocessing.get_context(\"forkserver\")\n\n    pool = context.Pool(processes=processes)\n\n    function_parameters = zip(\n        images_to_check,\n        itertools.repeat(model),\n        itertools.repeat(upsample),\n    )\n\n    pool.starmap(test_image, function_parameters)\n\n\n@click.command()\n@click.argument('image_to_check')\n@click.option('--cpus', default=1, help='number of CPU cores to use in parallel. -1 means \"use all in system\"')\n@click.option('--model', default=\"hog\", help='Which face detection model to use. Options are \"hog\" or \"cnn\".')\n@click.option('--upsample', default=0, help='How many times to upsample the image looking for faces. Higher numbers find smaller faces.')\ndef main(image_to_check, cpus, model, upsample):\n    # Multi-core processing only supported on Python 3.4 or greater\n    if (sys.version_info < (3, 4)) and cpus != 1:\n        click.echo(\"WARNING: Multi-processing support requires Python 3.4 or greater. Falling back to single-threaded processing!\")\n        cpus = 1\n\n    if os.path.isdir(image_to_check):\n        if cpus == 1:\n            [test_image(image_file, model, upsample) for image_file in image_files_in_folder(image_to_check)]\n        else:\n            process_images_in_process_pool(image_files_in_folder(image_to_check), cpus, model, upsample)\n    else:\n        test_image(image_to_check, model, upsample)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "Description": "人脸检测模块的调用，主要功能是检测图片中的人脸，并输出人脸的位置"
    }
  ],
  "API_Implementations": [
    {
      "Implementation": "\n# -*- coding: utf-8 -*-\n\nimport PIL.Image\nimport dlib\nimport numpy as np\nfrom PIL import ImageFile\n\ntry:\n    import face_recognition_models\nexcept Exception:\n    print(\"Please install `face_recognition_models` with this command before using `face_recognition`:\n\")\n    print(\"pip install git+https://github.com/ageitgey/face_recognition_models\")\n    quit()\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nface_detector = dlib.get_frontal_face_detector()\n\npredictor_68_point_model = face_recognition_models.pose_predictor_model_location()\npose_predictor_68_point = dlib.shape_predictor(predictor_68_point_model)\n\npredictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\npose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model)\n\ncnn_face_detection_model = face_recognition_models.cnn_face_detector_model_location()\ncnn_face_detector = dlib.cnn_face_detection_model_v1(cnn_face_detection_model)\n\nface_recognition_model = face_recognition_models.face_recognition_model_location()\nface_encoder = dlib.face_recognition_model_v1(face_recognition_model)\n\n\ndef _rect_to_css(rect):\n    \"\"\"\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return rect.top(), rect.right(), rect.bottom(), rect.left()\n\n\ndef _css_to_rect(css):\n    \"\"\"\n    Convert a tuple in (top, right, bottom, left) order to a dlib `rect` object\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :return: a dlib `rect` object\n    \"\"\"\n    return dlib.rectangle(css[3], css[0], css[1], css[2])\n\n\ndef _trim_css_to_bounds(css, image_shape):\n    \"\"\"\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n\n\ndef face_distance(face_encodings, face_to_compare):\n    \"\"\"\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param face_encodings: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n    \"\"\"\n    if len(face_encodings) == 0:\n        return np.empty((0))\n\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\n\n\ndef load_image_file(file, mode='RGB'):\n    \"\"\"\n    Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array\n    \"\"\"\n    im = PIL.Image.open(file)\n    if mode:\n        im = im.convert(mode)\n    return np.array(im)\n\n\ndef _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' objects of found face locations\n    \"\"\"\n    if model == \"cnn\":\n        return cnn_face_detector(img, number_of_times_to_upsample)\n    else:\n        return face_detector(img, number_of_times_to_upsample)\n\n\ndef face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    if model == \"cnn\":\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n    else:\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]\n\n\ndef _raw_face_locations_batched(images, number_of_times_to_upsample=1, batch_size=128):\n    \"\"\"\n    Returns an 2d array of dlib rects of human faces in a image using the cnn face detector\n\n    :param images: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :return: A list of dlib 'rect' objects of found face locations\n    \"\"\"\n    return cnn_face_detector(images, number_of_times_to_upsample, batch_size=batch_size)\n\n\ndef batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n    \"\"\"\n    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param images: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    def convert_cnn_detections_to_css(detections):\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))\n\n\ndef _raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\n    if face_locations is None:\n        face_locations = _raw_face_locations(face_image)\n    else:\n        face_locations = [_css_to_rect(face_location) for face_location in face_locations]\n\n    pose_predictor = pose_predictor_68_point\n\n    if model == \"small\":\n        pose_predictor = pose_predictor_5_point\n\n    return [pose_predictor(face_image, face_location) for face_location in face_locations]\n\n\ndef face_landmarks(face_image, face_locations=None, model=\"large\"):\n    \"\"\"\n    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)\n    \"\"\"\n    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n\n    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n    if model == 'large':\n        return [{\n            \"chin\": points[0:17],\n            \"left_eyebrow\": points[17:22],\n            \"right_eyebrow\": points[22:27],\n            \"nose_bridge\": points[27:31],\n            \"nose_tip\": points[31:36],\n            \"left_eye\": points[36:42],\n            \"right_eye\": points[42:48],\n            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n        } for points in landmarks_as_tuples]\n    elif model == 'small':\n        return [{\n            \"nose_tip\": [points[4]],\n            \"left_eye\": points[2:4],\n            \"right_eye\": points[0:2],\n        } for points in landmarks_as_tuples]\n    else:\n        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")\n\n\ndef face_encodings(face_image, known_face_locations=None, num_jitters=1, model=\"small\"):\n    \"\"\"\n    Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :param model: Optional - which model to use. \"large\" or \"small\" (default) which only returns 5 points but is faster.\n    :return: A list of 128-dimensional face encodings (one for each face in the image)\n    \"\"\"\n    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model)\n    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]\n\n\ndef compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\n    \"\"\"\n    Compare a list of face encodings against a candidate encoding to see if they match.\n\n    :param known_face_encodings: A list of known face encodings\n    :param face_encoding_to_check: A single face encoding to compare against the list\n    :param tolerance: How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance.\n    :return: A list of True/False values indicating which known_face_encodings match the face encoding to check\n    \"\"\"\n    return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)\n\n",
      "Description": "face_recognition模块的实现，关键函数为face_distance,load_image_file,face_locations,face_landmarks,face_encodings,compare_faces",
      "Path": "/mnt/autor_name/haoTingDeWenJianJia/face/face_recognition/api.py",
      "Name": "face_recognition",
      "Examples": "\nimport face_recognition\nfrom PIL import Image, ImageDraw\n# 加载图片\nimage = face_recognition.load_image_file(\"YangDingwen.jpg\")\nface_locations = face_recognition.face_locations(image)\n# 将图片转换为PIL格式\npil_image = Image.fromarray(image)\ndraw = ImageDraw.Draw(pil_image)\n# 绘制每个脸部的矩形框\nfor (top, right, bottom, left) in face_locations:\n    draw.rectangle(((left, top), (right, bottom)), outline=\"red\", width=2)\n# 保存结果图片\npil_image.save(\"YangDingwen_with_faces.jpg\")\n# face_locations is now an array listing the co-ordinates of each face!\n"
    }
  ]
}