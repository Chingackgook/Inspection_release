{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/developer",
    "API_Calls": [
        {
            "Name": "main_call_generate_code_sync",
            "Description": "This script generates code based on a user-provided prompt using AI models. It processes the prompt, plans dependencies, specifies file paths, and generates code files in a specified folder. Debugging options are available for detailed output during execution.",
            "Code": "import sys\nimport time\n\nfrom smol_dev.prompts import plan, specify_file_paths, generate_code_sync\nfrom smol_dev.utils import generate_folder, write_file\nimport argparse\n\n# model = \"gpt-3.5-turbo-0613\"\ndefaultmodel = \"gpt-4o-mini\"\n\ndef main(prompt, generate_folder_path=\"generated\", debug=False, model: str = defaultmodel):\n    # create generateFolder folder if doesnt exist\n    generate_folder(generate_folder_path)\n\n    # plan shared_deps\n    if debug:\n        print(\"--------shared_deps---------\")\n    with open(f\"{generate_folder_path}/shared_deps.md\", \"wb\") as f:\n\n        start_time = time.time()\n        def stream_handler(chunk):\n            f.write(chunk)\n            if debug:\n                end_time = time.time()\n\n                sys.stdout.write(\"\\r \\033[93mChars streamed\\033[0m: {}. \\033[93mChars per second\\033[0m: {:.2f}\".format(stream_handler.count, stream_handler.count / (end_time - start_time)))\n                sys.stdout.flush()\n                stream_handler.count += len(chunk)\n\n        stream_handler.count = 0\n        stream_handler.onComplete = lambda x: sys.stdout.write(\"\\033[0m\\n\") # remove the stdout line when streaming is complete\n\n        shared_deps = plan(prompt, stream_handler, model=model)\n    if debug:\n        print(shared_deps)\n    write_file(f\"{generate_folder_path}/shared_deps.md\", shared_deps)\n    if debug:\n        print(\"--------shared_deps---------\")\n\n    # specify file_paths\n    if debug:\n        print(\"--------specify_filePaths---------\")\n    file_paths = specify_file_paths(prompt, shared_deps, model=model)\n    if debug:\n        print(file_paths)\n    if debug:\n        print(\"--------file_paths---------\")\n\n    # loop through file_paths array and generate code for each file\n    for file_path in file_paths:\n        file_path = f\"{generate_folder_path}/{file_path}\"  # just append prefix\n        if debug:\n            print(f\"--------generate_code: {file_path} ---------\")\n\n        start_time = time.time()\n        def stream_handler(chunk):\n            if debug:\n                end_time = time.time()\n                sys.stdout.write(\"\\r \\033[93mChars streamed\\033[0m: {}. \\033[93mChars per second\\033[0m: {:.2f}\".format(stream_handler.count, stream_handler.count / (end_time - start_time)))\n                sys.stdout.flush()\n                stream_handler.count += len(chunk)\n        stream_handler.count = 0\n        stream_handler.onComplete = lambda x: sys.stdout.write(\"\\033[0m\\n\") # remove the stdout line when streaming is complete\n        code = generate_code_sync(prompt, shared_deps, file_path, stream_handler, model=model)\n        if debug:\n            print(code)\n        if debug:\n            print(f\"--------generate_code: {file_path} ---------\")\n        # create file with code content\n        write_file(file_path, code)\n        \n    print(\"--------smol dev done!---------\")\n\n\n# for local testing\n# python main.py --prompt \"a simple JavaScript/HTML/CSS/Canvas app that is a one player game of PONG...\" --generate_folder_path \"generated\" --debug True\n\nif __name__ == \"__main__\":\n    prompt = \"\"\"\n  a simple JavaScript/HTML/CSS/Canvas app that is a one player game of PONG. \n  The left paddle is controlled by the player, following where the mouse goes.\n  The right paddle is controlled by a simple AI algorithm, which slowly moves the paddle toward the ball at every frame, with some probability of error.\n  Make the canvas a 400 x 400 black square and center it in the app.\n  Make the paddles 100px long, yellow and the ball small and red.\n  Make sure to render the paddles and name them so they can controlled in javascript.\n  Implement the collision detection and scoring as well.\n  Every time the ball bouncess off a paddle, the ball should move faster.\n  It is meant to run in Chrome browser, so dont use anything that is not supported by Chrome, and don't use the import and export keywords.\n  \"\"\"\n    if len(sys.argv) == 2:\n        prompt = sys.argv[1]\n    else:\n        \n        parser = argparse.ArgumentParser()\n        parser.add_argument(\"--prompt\", type=str, required=True, help=\"Prompt for the app to be created.\")\n        parser.add_argument(\"--generate_folder_path\", type=str, default=\"generated\", help=\"Path of the folder for generated code.\")\n        parser.add_argument(\"--debug\", type=bool, default=False, help=\"Enable or disable debug mode.\")\n        args = parser.parse_args()\n        if args.prompt:\n            prompt = args.prompt\n        \n    print(prompt)\n        \n    main(prompt=prompt, generate_folder_path=args.generate_folder_path, debug=args.debug)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/developer/smol_dev/main.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "generate_code",
            "Description": "This file implements functions to generate code based on user prompts using OpenAI's API. It handles planning, file path specification, and asynchronous code generation with retry logic and streaming support.",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/developer/smol_dev/prompts.py",
            "Implementation": "import asyncio\nimport re\nimport time\nfrom typing import List, Optional, Callable, Any\n\nimport openai\nfrom openai_function_call import openai_function\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nSMOL_DEV_SYSTEM_PROMPT = \"\"\"\nYou are a top tier AI developer who is trying to write a program that will generate code for the user based on their intent.\nDo not leave any todos, fully implement every feature requested.\n\nWhen writing code, add comments to explain what you intend to do and why it aligns with the program plan and specific instructions from the original prompt.\n\"\"\"\n\n\n@openai_function\ndef file_paths(files_to_edit: List[str]) -> List[str]:\n    \"\"\"\n    Construct a list of strings.\n    \"\"\"\n    # print(\"filesToEdit\", files_to_edit)\n    return files_to_edit\n\n\ndef specify_file_paths(prompt: str, plan: str, model: str = 'gpt-3.5-turbo-0613'):\n    completion = openai.ChatCompletion.create(\n        model=model,\n        temperature=0.7,\n        functions=[file_paths.openai_schema],\n        function_call={\"name\": \"file_paths\"},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"{SMOL_DEV_SYSTEM_PROMPT}\n      Given the prompt and the plan, return a list of strings corresponding to the new files that will be generated.\n                  \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\" I want a: {prompt} \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\" The plan we have agreed on is: {plan} \"\"\",\n            },\n        ],\n    )\n    result = file_paths.from_response(completion)\n    return result\n\n\ndef plan(prompt: str, stream_handler: Optional[Callable[[bytes], None]] = None, model: str='gpt-3.5-turbo-0613', extra_messages: List[Any] = []):\n    completion = openai.ChatCompletion.create(\n        model=model,\n        temperature=0.7,\n        stream=True,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"{SMOL_DEV_SYSTEM_PROMPT}\n\n    In response to the user's prompt, write a plan using GitHub Markdown syntax. Begin with a YAML description of the new files that will be created.\n  In this plan, please name and briefly describe the structure of code that will be generated, including, for each file we are generating, what variables they export, data schemas, id names of every DOM elements that javascript functions will use, message names, and function names.\n                Respond only with plans following the above schema.\n                  \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\" the app prompt is: {prompt} \"\"\",\n            },\n            *extra_messages,\n        ],\n    )\n\n    collected_messages = []\n    for chunk in completion:\n        chunk_message_dict = chunk[\"choices\"][0]\n        chunk_message = chunk_message_dict[\"delta\"]  # extract the message\n        if chunk_message_dict[\"finish_reason\"] is None:\n            collected_messages.append(chunk_message)  # save the message\n            if stream_handler:\n                try:\n                    stream_handler(chunk_message[\"content\"].encode(\"utf-8\"))\n                except Exception as err:\n                    logger.info(\"\\nstream_handler error:\", err)\n                    logger.info(chunk_message)\n    # if stream_handler and hasattr(stream_handler, \"onComplete\"): stream_handler.onComplete('done')\n    full_reply_content = \"\".join([m.get(\"content\", \"\") for m in collected_messages])\n    return full_reply_content\n\n\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\nasync def generate_code(prompt: str, plan: str, current_file: str, stream_handler: Optional[Callable[Any, Any]] = None,\n                        model: str = 'gpt-3.5-turbo-0613') -> str:\n    first = True\n    chunk_count = 0\n    start_time = time.time()\n    completion = openai.ChatCompletion.acreate(\n        model=model,\n        temperature=0.7,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"{SMOL_DEV_SYSTEM_PROMPT}\n\n  In response to the user's prompt,\n  Please name and briefly describe the structure of the app we will generate, including, for each file we are generating, what variables they export, data schemas, id names of every DOM elements that javascript functions will use, message names, and function names.\n\n  We have broken up the program into per-file generation.\n  Now your job is to generate only the code for the file: {current_file}\n\n  only write valid code for the given filepath and file type, and return only the code.\n  do not add any other explanation, only return valid code for that file type.\n                  \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\" the plan we have agreed on is: {plan} \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\" the app prompt is: {prompt} \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n    Make sure to have consistent filenames if you reference other files we are also generating.\n\n    Remember that you must obey 3 things:\n       - you are generating code for the file {current_file}\n       - do not stray from the names of the files and the plan we have decided on\n       - MOST IMPORTANT OF ALL - every line of code you generate must be valid code. Do not include code fences in your response, for example\n\n    Bad response (because it contains the code fence):\n    ```javascript\n    console.log(\"hello world\")\n    ```\n\n    Good response (because it only contains the code):\n    console.log(\"hello world\")\n\n    Begin generating the code now.\n\n    \"\"\",\n            },\n        ],\n        stream=True,\n    )\n\n    collected_messages = []\n    async for chunk in await completion:\n        chunk_message_dict = chunk[\"choices\"][0]\n        chunk_message = chunk_message_dict[\"delta\"]  # extract the message\n        if chunk_message_dict[\"finish_reason\"] is None:\n            collected_messages.append(chunk_message)  # save the message\n            if stream_handler:\n                try:\n                    stream_handler(chunk_message[\"content\"].encode(\"utf-8\"))\n                except Exception as err:\n                    logger.info(\"\\nstream_handler error:\", err)\n                    logger.info(chunk_message)\n\n    # if stream_handler and hasattr(stream_handler, \"onComplete\"): stream_handler.onComplete('done')\n    code_file = \"\".join([m.get(\"content\", \"\") for m in collected_messages])\n\n    pattern = r\"```[\\w\\s]*\\n([\\s\\S]*?)```\"  # codeblocks at start of the string, less eager\n    code_blocks = re.findall(pattern, code_file, re.MULTILINE)\n    return code_blocks[0] if code_blocks else code_file\n\n\ndef generate_code_sync(prompt: str, plan: str, current_file: str,\n                       stream_handler: Optional[Callable[Any, Any]] = None,\n                       model: str = 'gpt-3.5-turbo-0613') -> str:\n    loop = asyncio.get_event_loop()\n    return loop.run_until_complete(generate_code(prompt, plan, current_file, stream_handler, model))\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}