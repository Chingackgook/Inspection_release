{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/yolov5",
    "API_Calls": [
        {
            "Name": "easy_test",
            "Description": "这是一个调用接口的示例，使用接口执行推理识别图片/视频。",
            "Code": "import argparse\nimport csv\nimport os\nimport platform\nimport sys\nfrom pathlib import Path\n\nimport torch\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom ultralytics.utils.plotting import Annotator, colors, save_one_box\n\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (\n    LOGGER,\n    Profile,\n    check_file,\n    check_img_size,\n    check_imshow,\n    check_requirements,\n    colorstr,\n    cv2,\n    increment_path,\n    non_max_suppression,\n    print_args,\n    scale_boxes,\n    strip_optimizer,\n    xyxy2xywh,\n)\nfrom utils.torch_utils import select_device, smart_inference_mode\n\n\n@smart_inference_mode()\ndef run(\n    weights=ROOT / \"yolov5s.pt\",  # model path or triton URL\n    source=ROOT / \"data/images\",  # file/dir/URL/glob/screen/0(webcam)\n    data=ROOT / \"data/coco128.yaml\",  # dataset.yaml path\n    imgsz=(640, 640),  # inference size (height, width)\n    conf_thres=0.25,  # confidence threshold\n    iou_thres=0.45,  # NMS IOU threshold\n    max_det=1000,  # maximum detections per image\n    device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n    view_img=False,  # show results\n    save_txt=False,  # save results to *.txt\n    save_format=0,  # save boxes coordinates in YOLO format or Pascal-VOC format (0 for YOLO and 1 for Pascal-VOC)\n    save_csv=False,  # save results in CSV format\n    save_conf=False,  # save confidences in --save-txt labels\n    save_crop=False,  # save cropped prediction boxes\n    nosave=False,  # do not save images/videos\n    classes=None,  # filter by class: --class 0, or --class 0 2 3\n    agnostic_nms=False,  # class-agnostic NMS\n    augment=False,  # augmented inference\n    visualize=False,  # visualize features\n    update=False,  # update all models\n    project=ROOT / \"runs/detect\",  # save results to project/name\n    name=\"exp\",  # save results to project/name\n    exist_ok=False,  # existing project/name ok, do not increment\n    line_thickness=3,  # bounding box thickness (pixels)\n    hide_labels=False,  # hide labels\n    hide_conf=False,  # hide confidences\n    half=False,  # use FP16 half-precision inference\n    dnn=False,  # use OpenCV DNN for ONNX inference\n    vid_stride=1,  # video frame-rate stride\n):\n    \"\"\"\n    Runs YOLOv5 detection inference on various sources like images, videos, directories, streams, etc.\n\n    Args:\n        weights (str | Path): Path to the model weights file or a Triton URL. Default is 'yolov5s.pt'.\n        source (str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \"\"\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\".txt\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\"rtsp://\", \"rtmp://\", \"http://\", \"https://\"))\n    webcam = source.isnumeric() or source.endswith(\".streams\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\"screen\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \"labels\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \"predictions.csv\"\n\n        # Create or append to the CSV file\n        def write_to_csv(image_name, prediction, confidence):\n            \"\"\"Writes prediction data for an image to a CSV file, appending if the file exists.\"\"\"\n            data = {\"Image Name\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n            file_exists = os.path.isfile(csv_path)\n            with open(csv_path, mode=\"a\", newline=\"\") as f:\n                writer = csv.DictWriter(f, fieldnames=data.keys())\n                if not file_exists:\n                    writer.writeheader()\n                writer.writerow(data)\n\n        # Process predictions\n        for i, det in enumerate(pred):  # per image\n            seen += 1\n            if webcam:  # batch_size >= 1\n                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n                s += f\"{i}: \"\n            else:\n                p, im0, frame = path, im0s.copy(), getattr(dataset, \"frame\", 0)\n\n            p = Path(p)  # to Path\n            save_path = str(save_dir / p.name)  # im.jpg\n            txt_path = str(save_dir / \"labels\" / p.stem) + (\"\" if dataset.mode == \"image\" else f\"_{frame}\")  # im.txt\n            s += \"{:g}x{:g} \".format(*im.shape[2:])  # print string\n            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n            imc = im0.copy() if save_crop else im0  # for save_crop\n            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n            if len(det):\n                # Rescale boxes from img_size to im0 size\n                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n\n                # Print results\n                for c in det[:, 5].unique():\n                    n = (det[:, 5] == c).sum()  # detections per class\n                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n\n                # Write results\n                for *xyxy, conf, cls in reversed(det):\n                    c = int(cls)  # integer class\n                    label = names[c] if hide_conf else f\"{names[c]}\"\n                    confidence = float(conf)\n                    confidence_str = f\"{confidence:.2f}\"\n\n                    if save_csv:\n                        write_to_csv(p.name, label, confidence_str)\n\n                    if save_txt:  # Write to file\n                        if save_format == 0:\n                            coords = (\n                                (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n                            )  # normalized xywh\n                        else:\n                            coords = (torch.tensor(xyxy).view(1, 4) / gn).view(-1).tolist()  # xyxy\n                        line = (cls, *coords, conf) if save_conf else (cls, *coords)  # label format\n                        with open(f\"{txt_path}.txt\", \"a\") as f:\n                            f.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")\n\n                    if save_img or save_crop or view_img:  # Add bbox to image\n                        c = int(cls)  # integer class\n                        label = None if hide_labels else (names[c] if hide_conf else f\"{names[c]} {conf:.2f}\")\n                        annotator.box_label(xyxy, label, color=colors(c, True))\n                    if save_crop:\n                        save_one_box(xyxy, imc, file=save_dir / \"crops\" / names[c] / f\"{p.stem}.jpg\", BGR=True)\n\n            # Stream results\n            im0 = annotator.result()\n            if view_img:\n                if platform.system() == \"Linux\" and p not in windows:\n                    windows.append(p)\n                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n                cv2.imshow(str(p), im0)\n                cv2.waitKey(1)  # 1 millisecond\n\n            # Save results (image with detections)\n            if save_img:\n                if dataset.mode == \"image\":\n                    cv2.imwrite(save_path, im0)\n                else:  # 'video' or 'stream'\n                    if vid_path[i] != save_path:  # new video\n                        vid_path[i] = save_path\n                        if isinstance(vid_writer[i], cv2.VideoWriter):\n                            vid_writer[i].release()  # release previous video writer\n                        if vid_cap:  # video\n                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                        else:  # stream\n                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n                        save_path = str(Path(save_path).with_suffix(\".mp4\"))  # force *.mp4 suffix on results videos\n                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n                    vid_writer[i].write(im0)\n\n        # Print time (inference-only)\n        LOGGER.info(f\"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1e3:.1f}ms\")\n\n    # Print results\n    t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\n    LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}\" % t)\n    if save_txt or save_img:\n        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else \"\"\n        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n    if update:\n        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)\n\n\ndef parse_opt():\n    \"\"\"\n    Parse command-line arguments for YOLOv5 detection, allowing custom inference options and model configurations.\n\n    Args:\n        --weights (str | list[str], optional): Model path or Triton URL. Defaults to ROOT / 'yolov5s.pt'.\n        --source (str, optional): File/dir/URL/glob/screen/0(webcam). Defaults to ROOT / 'data/images'.\n        --data (str, optional): Dataset YAML path. Provides dataset configuration information.\n        --imgsz (list[int], optional): Inference size (height, width). Defaults to [640].\n        --conf-thres (float, optional): Confidence threshold. Defaults to 0.25.\n        --iou-thres (float, optional): NMS IoU threshold. Defaults to 0.45.\n        --max-det (int, optional): Maximum number of detections per image. Defaults to 1000.\n        --device (str, optional): CUDA device, i.e., '0' or '0,1,2,3' or 'cpu'. Defaults to \"\".\n        --view-img (bool, optional): Flag to display results. Defaults to False.\n        --save-txt (bool, optional): Flag to save results to *.txt files. Defaults to False.\n        --save-csv (bool, optional): Flag to save results in CSV format. Defaults to False.\n        --save-conf (bool, optional): Flag to save confidences in labels saved via --save-txt. Defaults to False.\n        --save-crop (bool, optional): Flag to save cropped prediction boxes. Defaults to False.\n        --nosave (bool, optional): Flag to prevent saving images/videos. Defaults to False.\n        --classes (list[int], optional): List of classes to filter results by, e.g., '--classes 0 2 3'. Defaults to None.\n        --agnostic-nms (bool, optional): Flag for class-agnostic NMS. Defaults to False.\n        --augment (bool, optional): Flag for augmented inference. Defaults to False.\n        --visualize (bool, optional): Flag for visualizing features. Defaults to False.\n        --update (bool, optional): Flag to update all models in the model directory. Defaults to False.\n        --project (str, optional): Directory to save results. Defaults to ROOT / 'runs/detect'.\n        --name (str, optional): Sub-directory name for saving results within --project. Defaults to 'exp'.\n        --exist-ok (bool, optional): Flag to allow overwriting if the project/name already exists. Defaults to False.\n        --line-thickness (int, optional): Thickness (in pixels) of bounding boxes. Defaults to 3.\n        --hide-labels (bool, optional): Flag to hide labels in the output. Defaults to False.\n        --hide-conf (bool, optional): Flag to hide confidences in the output. Defaults to False.\n        --half (bool, optional): Flag to use FP16 half-precision inference. Defaults to False.\n        --dnn (bool, optional): Flag to use OpenCV DNN for ONNX inference. Defaults to False.\n        --vid-stride (int, optional): Video frame-rate stride, determining the number of frames to skip in between\n            consecutive frames. Defaults to 1.\n\n    Returns:\n        argparse.Namespace: Parsed command-line arguments as an argparse.Namespace object.\n\n    Example:\n        ```python\n        from ultralytics import YOLOv5\n        args = YOLOv5.parse_opt()\n        ```\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--weights\", nargs=\"+\", type=str, default=ROOT / \"yolov5s.pt\", help=\"model path or triton URL\")\n    parser.add_argument(\"--source\", type=str, default=ROOT / \"data/images\", help=\"file/dir/URL/glob/screen/0(webcam)\")\n    parser.add_argument(\"--data\", type=str, default=ROOT / \"data/coco128.yaml\", help=\"(optional) dataset.yaml path\")\n    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", nargs=\"+\", type=int, default=[640], help=\"inference size h,w\")\n    parser.add_argument(\"--conf-thres\", type=float, default=0.25, help=\"confidence threshold\")\n    parser.add_argument(\"--iou-thres\", type=float, default=0.45, help=\"NMS IoU threshold\")\n    parser.add_argument(\"--max-det\", type=int, default=1000, help=\"maximum detections per image\")\n    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n    parser.add_argument(\"--view-img\", action=\"store_true\", help=\"show results\")\n    parser.add_argument(\"--save-txt\", action=\"store_true\", help=\"save results to *.txt\")\n    parser.add_argument(\n        \"--save-format\",\n        type=int,\n        default=0,\n        help=\"whether to save boxes coordinates in YOLO format or Pascal-VOC format when save-txt is True, 0 for YOLO and 1 for Pascal-VOC\",\n    )\n    parser.add_argument(\"--save-csv\", action=\"store_true\", help=\"save results in CSV format\")\n    parser.add_argument(\"--save-conf\", action=\"store_true\", help=\"save confidences in --save-txt labels\")\n    parser.add_argument(\"--save-crop\", action=\"store_true\", help=\"save cropped prediction boxes\")\n    parser.add_argument(\"--nosave\", action=\"store_true\", help=\"do not save images/videos\")\n    parser.add_argument(\"--classes\", nargs=\"+\", type=int, help=\"filter by class: --classes 0, or --classes 0 2 3\")\n    parser.add_argument(\"--agnostic-nms\", action=\"store_true\", help=\"class-agnostic NMS\")\n    parser.add_argument(\"--augment\", action=\"store_true\", help=\"augmented inference\")\n    parser.add_argument(\"--visualize\", action=\"store_true\", help=\"visualize features\")\n    parser.add_argument(\"--update\", action=\"store_true\", help=\"update all models\")\n    parser.add_argument(\"--project\", default=ROOT / \"runs/detect\", help=\"save results to project/name\")\n    parser.add_argument(\"--name\", default=\"exp\", help=\"save results to project/name\")\n    parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n    parser.add_argument(\"--line-thickness\", default=3, type=int, help=\"bounding box thickness (pixels)\")\n    parser.add_argument(\"--hide-labels\", default=False, action=\"store_true\", help=\"hide labels\")\n    parser.add_argument(\"--hide-conf\", default=False, action=\"store_true\", help=\"hide confidences\")\n    parser.add_argument(\"--half\", action=\"store_true\", help=\"use FP16 half-precision inference\")\n    parser.add_argument(\"--dnn\", action=\"store_true\", help=\"use OpenCV DNN for ONNX inference\")\n    parser.add_argument(\"--vid-stride\", type=int, default=1, help=\"video frame-rate stride\")\n    opt = parser.parse_args()\n    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n    print_args(vars(opt))\n    return opt\n\n\ndef main(opt):\n    \"\"\"\n    Executes YOLOv5 model inference based on provided command-line arguments, validating dependencies before running.\n\n    Args:\n        opt (argparse.Namespace): Command-line arguments for YOLOv5 detection. See function `parse_opt` for details.\n\n    Returns:\n        None\n\n    Note:\n        This function performs essential pre-execution checks and initiates the YOLOv5 detection process based on user-specified\n        options. Refer to the usage guide and examples for more information about different sources and formats at:\n        https://github.com/ultralytics/ultralytics\n\n    Example usage:\n\n    ```python\n    if __name__ == \"__main__\":\n        opt = parse_opt()\n        main(opt)\n    ```\n    \"\"\"\n    check_requirements(ROOT / \"requirements.txt\", exclude=(\"tensorboard\", \"thop\"))\n    run(**vars(opt))\n\n\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/yolov5/detect.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class_DetectMultiBackend",
            "Description": "这是一个接口类，可以识别图片/视频，通过forward接口执行推理",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/yolov5/models/common.py",
            "Implementation": "class DetectMultiBackend(nn.Module):\n\n    def __init__(self, weights=\"yolov5s.pt\", device=torch.device(\"cpu\"), dnn=False, data=None, fp16=False, fuse=True):\n        #   PyTorch:              weights = *.pt\n        #   TorchScript:                    *.torchscript\n        #   ONNX Runtime:                   *.onnx\n        #   ONNX OpenCV DNN:                *.onnx --dnn\n        #   OpenVINO:                       *_openvino_model\n        #   CoreML:                         *.mlpackage\n        #   TensorRT:                       *.engine\n        #   TensorFlow SavedModel:          *_saved_model\n        #   TensorFlow GraphDef:            *.pb\n        #   TensorFlow Lite:                *.tflite\n        #   TensorFlow Edge TPU:            *_edgetpu.tflite\n        #   PaddlePaddle:                   *_paddle_model\n        from models.experimental import attempt_download, attempt_load  # scoped to avoid circular import\n\n        super().__init__()\n        w = str(weights[0] if isinstance(weights, list) else weights)\n        pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton = self._model_type(w)\n        fp16 &= pt or jit or onnx or engine or triton  # FP16\n        nhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)\n        stride = 32  # default stride\n        cuda = torch.cuda.is_available() and device.type != \"cpu\"  # use CUDA\n        if not (pt or triton):\n            w = attempt_download(w)  # download if not local\n\n        if pt:  # PyTorch\n            model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n            stride = max(int(model.stride.max()), 32)  # model stride\n            names = model.module.names if hasattr(model, \"module\") else model.names  # get class names\n            model.half() if fp16 else model.float()\n            self.model = model  # explicitly assign for to(), cpu(), cuda(), half()\n        elif jit:  # TorchScript\n            LOGGER.info(f\"Loading {w} for TorchScript inference...\")\n            extra_files = {\"config.txt\": \"\"}  # model metadata\n            model = torch.jit.load(w, _extra_files=extra_files, map_location=device)\n            model.half() if fp16 else model.float()\n            if extra_files[\"config.txt\"]:  # load metadata dict\n                d = json.loads(\n                    extra_files[\"config.txt\"],\n                    object_hook=lambda d: {int(k) if k.isdigit() else k: v for k, v in d.items()},\n                )\n                stride, names = int(d[\"stride\"]), d[\"names\"]\n        elif dnn:  # ONNX OpenCV DNN\n            LOGGER.info(f\"Loading {w} for ONNX OpenCV DNN inference...\")\n            check_requirements(\"opencv-python>=4.5.4\")\n            net = cv2.dnn.readNetFromONNX(w)\n        elif onnx:  # ONNX Runtime\n            LOGGER.info(f\"Loading {w} for ONNX Runtime inference...\")\n            check_requirements((\"onnx\", \"onnxruntime-gpu\" if cuda else \"onnxruntime\"))\n            import onnxruntime\n\n            providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if cuda else [\"CPUExecutionProvider\"]\n            session = onnxruntime.InferenceSession(w, providers=providers)\n            output_names = [x.name for x in session.get_outputs()]\n            meta = session.get_modelmeta().custom_metadata_map  # metadata\n            if \"stride\" in meta:\n                stride, names = int(meta[\"stride\"]), eval(meta[\"names\"])\n        elif xml:  # OpenVINO\n            LOGGER.info(f\"Loading {w} for OpenVINO inference...\")\n            check_requirements(\"openvino>=2023.0\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n            from openvino.runtime import Core, Layout, get_batch\n\n            core = Core()\n            if not Path(w).is_file():  # if not *.xml\n                w = next(Path(w).glob(\"*.xml\"))  # get *.xml file from *_openvino_model dir\n            ov_model = core.read_model(model=w, weights=Path(w).with_suffix(\".bin\"))\n            if ov_model.get_parameters()[0].get_layout().empty:\n                ov_model.get_parameters()[0].set_layout(Layout(\"NCHW\"))\n            batch_dim = get_batch(ov_model)\n            if batch_dim.is_static:\n                batch_size = batch_dim.get_length()\n            ov_compiled_model = core.compile_model(ov_model, device_name=\"AUTO\")  # AUTO selects best available device\n            stride, names = self._load_metadata(Path(w).with_suffix(\".yaml\"))  # load metadata\n        elif engine:  # TensorRT\n            LOGGER.info(f\"Loading {w} for TensorRT inference...\")\n            import tensorrt as trt  # https://developer.nvidia.com/nvidia-tensorrt-download\n\n            check_version(trt.__version__, \"7.0.0\", hard=True)  # require tensorrt>=7.0.0\n            if device.type == \"cpu\":\n                device = torch.device(\"cuda:0\")\n            Binding = namedtuple(\"Binding\", (\"name\", \"dtype\", \"shape\", \"data\", \"ptr\"))\n            logger = trt.Logger(trt.Logger.INFO)\n            with open(w, \"rb\") as f, trt.Runtime(logger) as runtime:\n                model = runtime.deserialize_cuda_engine(f.read())\n            context = model.create_execution_context()\n            bindings = OrderedDict()\n            output_names = []\n            fp16 = False  # default updated below\n            dynamic = False\n            is_trt10 = not hasattr(model, \"num_bindings\")\n            num = range(model.num_io_tensors) if is_trt10 else range(model.num_bindings)\n            for i in num:\n                if is_trt10:\n                    name = model.get_tensor_name(i)\n                    dtype = trt.nptype(model.get_tensor_dtype(name))\n                    is_input = model.get_tensor_mode(name) == trt.TensorIOMode.INPUT\n                    if is_input:\n                        if -1 in tuple(model.get_tensor_shape(name)):  # dynamic\n                            dynamic = True\n                            context.set_input_shape(name, tuple(model.get_profile_shape(name, 0)[2]))\n                        if dtype == np.float16:\n                            fp16 = True\n                    else:  # output\n                        output_names.append(name)\n                    shape = tuple(context.get_tensor_shape(name))\n                else:\n                    name = model.get_binding_name(i)\n                    dtype = trt.nptype(model.get_binding_dtype(i))\n                    if model.binding_is_input(i):\n                        if -1 in tuple(model.get_binding_shape(i)):  # dynamic\n                            dynamic = True\n                            context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[2]))\n                        if dtype == np.float16:\n                            fp16 = True\n                    else:  # output\n                        output_names.append(name)\n                    shape = tuple(context.get_binding_shape(i))\n                im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)\n                bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))\n            binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\n            batch_size = bindings[\"images\"].shape[0]  # if dynamic, this is instead max batch size\n        elif coreml:  # CoreML\n            LOGGER.info(f\"Loading {w} for CoreML inference...\")\n            import coremltools as ct\n\n            model = ct.models.MLModel(w)\n        elif saved_model:  # TF SavedModel\n            LOGGER.info(f\"Loading {w} for TensorFlow SavedModel inference...\")\n            import tensorflow as tf\n\n            keras = False  # assume TF1 saved_model\n            model = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)\n        elif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\n            LOGGER.info(f\"Loading {w} for TensorFlow GraphDef inference...\")\n            import tensorflow as tf\n\n            def wrap_frozen_graph(gd, inputs, outputs):\n                x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=\"\"), [])  # wrapped\n                ge = x.graph.as_graph_element\n                return x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))\n\n            def gd_outputs(gd):\n                name_list, input_list = [], []\n                for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef\n                    name_list.append(node.name)\n                    input_list.extend(node.input)\n                return sorted(f\"{x}:0\" for x in list(set(name_list) - set(input_list)) if not x.startswith(\"NoOp\"))\n\n            gd = tf.Graph().as_graph_def()  # TF GraphDef\n            with open(w, \"rb\") as f:\n                gd.ParseFromString(f.read())\n            frozen_func = wrap_frozen_graph(gd, inputs=\"x:0\", outputs=gd_outputs(gd))\n        elif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python\n            try:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu\n                from tflite_runtime.interpreter import Interpreter, load_delegate\n            except ImportError:\n                import tensorflow as tf\n\n                Interpreter, load_delegate = (\n                    tf.lite.Interpreter,\n                    tf.lite.experimental.load_delegate,\n                )\n            if edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime\n                LOGGER.info(f\"Loading {w} for TensorFlow Lite Edge TPU inference...\")\n                delegate = {\"Linux\": \"libedgetpu.so.1\", \"Darwin\": \"libedgetpu.1.dylib\", \"Windows\": \"edgetpu.dll\"}[\n                    platform.system()\n                ]\n                interpreter = Interpreter(model_path=w, experimental_delegates=[load_delegate(delegate)])\n            else:  # TFLite\n                LOGGER.info(f\"Loading {w} for TensorFlow Lite inference...\")\n                interpreter = Interpreter(model_path=w)  # load TFLite model\n            interpreter.allocate_tensors()  # allocate\n            input_details = interpreter.get_input_details()  # inputs\n            output_details = interpreter.get_output_details()  # outputs\n            # load metadata\n            with contextlib.suppress(zipfile.BadZipFile):\n                with zipfile.ZipFile(w, \"r\") as model:\n                    meta_file = model.namelist()[0]\n                    meta = ast.literal_eval(model.read(meta_file).decode(\"utf-8\"))\n                    stride, names = int(meta[\"stride\"]), meta[\"names\"]\n        elif tfjs:  # TF.js\n            raise NotImplementedError(\"ERROR: YOLOv5 TF.js inference is not supported\")\n        # PaddlePaddle\n        elif paddle:\n            LOGGER.info(f\"Loading {w} for PaddlePaddle inference...\")\n            check_requirements(\"paddlepaddle-gpu\" if cuda else \"paddlepaddle>=3.0.0\")\n            import paddle.inference as pdi\n\n            w = Path(w)\n            if w.is_dir():\n                model_file = next(w.rglob(\"*.json\"), None)\n                params_file = next(w.rglob(\"*.pdiparams\"), None)\n            elif w.suffix == \".pdiparams\":\n                model_file = w.with_name(\"model.json\")\n                params_file = w\n            else:\n                raise ValueError(f\"Invalid model path {w}. Provide model directory or a .pdiparams file.\")\n\n            if not (model_file and params_file and model_file.is_file() and params_file.is_file()):\n                raise FileNotFoundError(f\"Model files not found in {w}. Both .json and .pdiparams files are required.\")\n\n            config = pdi.Config(str(model_file), str(params_file))\n            if cuda:\n                config.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)\n            predictor = pdi.create_predictor(config)\n            input_handle = predictor.get_input_handle(predictor.get_input_names()[0])\n            output_names = predictor.get_output_names()\n\n        elif triton:  # NVIDIA Triton Inference Server\n            LOGGER.info(f\"Using {w} as Triton Inference Server...\")\n            check_requirements(\"tritonclient[all]\")\n            from utils.triton import TritonRemoteModel\n\n            model = TritonRemoteModel(url=w)\n            nhwc = model.runtime.startswith(\"tensorflow\")\n        else:\n            raise NotImplementedError(f\"ERROR: {w} is not a supported format\")\n\n        # class names\n        if \"names\" not in locals():\n            names = yaml_load(data)[\"names\"] if data else {i: f\"class{i}\" for i in range(999)}\n        if names[0] == \"n01440764\" and len(names) == 1000:  # ImageNet\n            names = yaml_load(ROOT / \"data/ImageNet.yaml\")[\"names\"]  # human-readable names\n\n        self.__dict__.update(locals())  # assign all variables to self\n\n    def forward(self, im, augment=False, visualize=False):\n        b, ch, h, w = im.shape  # batch, channel, height, width\n        if self.fp16 and im.dtype != torch.float16:\n            im = im.half()  # to FP16\n        if self.nhwc:\n            im = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\n\n        if self.pt:  # PyTorch\n            y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n        elif self.jit:  # TorchScript\n            y = self.model(im)\n        elif self.dnn:  # ONNX OpenCV DNN\n            im = im.cpu().numpy()  # torch to numpy\n            self.net.setInput(im)\n            y = self.net.forward()\n        elif self.onnx:  # ONNX Runtime\n            im = im.cpu().numpy()  # torch to numpy\n            y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\n        elif self.xml:  # OpenVINO\n            im = im.cpu().numpy()  # FP32\n            y = list(self.ov_compiled_model(im).values())\n        elif self.engine:  # TensorRT\n            if self.dynamic and im.shape != self.bindings[\"images\"].shape:\n                i = self.model.get_binding_index(\"images\")\n                self.context.set_binding_shape(i, im.shape)  # reshape if dynamic\n                self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=im.shape)\n                for name in self.output_names:\n                    i = self.model.get_binding_index(name)\n                    self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\n            s = self.bindings[\"images\"].shape\n            assert im.shape == s, f\"input size {im.shape} {'>' if self.dynamic else 'not equal to'} max model size {s}\"\n            self.binding_addrs[\"images\"] = int(im.data_ptr())\n            self.context.execute_v2(list(self.binding_addrs.values()))\n            y = [self.bindings[x].data for x in sorted(self.output_names)]\n        elif self.coreml:  # CoreML\n            im = im.cpu().numpy()\n            im = Image.fromarray((im[0] * 255).astype(\"uint8\"))\n            # im = im.resize((192, 320), Image.BILINEAR)\n            y = self.model.predict({\"image\": im})  # coordinates are xywh normalized\n            if \"confidence\" in y:\n                box = xywh2xyxy(y[\"coordinates\"] * [[w, h, w, h]])  # xyxy pixels\n                conf, cls = y[\"confidence\"].max(1), y[\"confidence\"].argmax(1).astype(np.float)\n                y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\n            else:\n                y = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\n        elif self.paddle:  # PaddlePaddle\n            im = im.cpu().numpy().astype(np.float32)\n            self.input_handle.copy_from_cpu(im)\n            self.predictor.run()\n            y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\n        elif self.triton:  # NVIDIA Triton Inference Server\n            y = self.model(im)\n        else:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\n            im = im.cpu().numpy()\n            if self.saved_model:  # SavedModel\n                y = self.model(im, training=False) if self.keras else self.model(im)\n            elif self.pb:  # GraphDef\n                y = self.frozen_func(x=self.tf.constant(im))\n            else:  # Lite or Edge TPU\n                input = self.input_details[0]\n                int8 = input[\"dtype\"] == np.uint8  # is TFLite quantized uint8 model\n                if int8:\n                    scale, zero_point = input[\"quantization\"]\n                    im = (im / scale + zero_point).astype(np.uint8)  # de-scale\n                self.interpreter.set_tensor(input[\"index\"], im)\n                self.interpreter.invoke()\n                y = []\n                for output in self.output_details:\n                    x = self.interpreter.get_tensor(output[\"index\"])\n                    if int8:\n                        scale, zero_point = output[\"quantization\"]\n                        x = (x.astype(np.float32) - zero_point) * scale  # re-scale\n                    y.append(x)\n            if len(y) == 2 and len(y[1].shape) != 4:\n                y = list(reversed(y))\n            y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\n            y[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels\n\n        if isinstance(y, (list, tuple)):\n            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\n        else:\n            return self.from_numpy(y)",
            "Examples": [
                "\n"
            ]
        }
    ]
}