{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/tortoise-tts",
    "API_Calls": [
        {
            "Name": "TextToSpeech_Call",
            "Description": "实现命令行文本到语音（TTS）生成工具,基于 TextToSpeech 类将文本转换为语音",
            "Code": "import argparse\nimport os\n\nimport torch\nimport torchaudio\n\nfrom api import TextToSpeech, MODELS_DIR\nfrom utils.audio import load_voices\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--text', type=str, help='Text to speak.', default=\"The expressiveness of autoregressive transformers is literally nuts! I absolutely adore them.\")\n    parser.add_argument('--voice', type=str, help='Selects the voice to use for generation. See options in voices/ directory (and add your own!) '\n                                                 'Use the & character to join two voices together. Use a comma to perform inference on multiple voices.', default='random')\n    parser.add_argument('--preset', type=str, help='Which voice preset to use.', default='fast')\n    parser.add_argument('--use_deepspeed', type=str, help='Use deepspeed for speed bump.', default=False)\n    parser.add_argument('--kv_cache', type=bool, help='If you disable this please wait for a long a time to get the output', default=True)\n    parser.add_argument('--half', type=bool, help=\"float16(half) precision inference if True it's faster and take less vram and ram\", default=True)\n    parser.add_argument('--output_path', type=str, help='Where to store outputs.', default='results/')\n    parser.add_argument('--model_dir', type=str, help='Where to find pretrained model checkpoints. Tortoise automatically downloads these to .models, so this'\n                                                      'should only be specified if you have custom checkpoints.', default=MODELS_DIR)\n    parser.add_argument('--candidates', type=int, help='How many output candidates to produce per-voice.', default=3)\n    parser.add_argument('--seed', type=int, help='Random seed which can be used to reproduce results.', default=None)\n    parser.add_argument('--produce_debug_state', type=bool, help='Whether or not to produce debug_state.pth, which can aid in reproducing problems. Defaults to true.', default=True)\n    parser.add_argument('--cvvp_amount', type=float, help='How much the CVVP model should influence the output.'\n                                                          'Increasing this can in some cases reduce the likelihood of multiple speakers. Defaults to 0 (disabled)', default=.0)\n    args = parser.parse_args()\n    if torch.backends.mps.is_available():\n        args.use_deepspeed = False\n    os.makedirs(args.output_path, exist_ok=True)\n    tts = TextToSpeech(models_dir=args.model_dir, use_deepspeed=args.use_deepspeed, kv_cache=args.kv_cache, half=args.half)\n\n    selected_voices = args.voice.split(',')\n    for k, selected_voice in enumerate(selected_voices):\n        if '&' in selected_voice:\n            voice_sel = selected_voice.split('&')\n        else:\n            voice_sel = [selected_voice]\n        voice_samples, conditioning_latents = load_voices(voice_sel)\n\n        gen, dbg_state = tts.tts_with_preset(args.text, k=args.candidates, voice_samples=voice_samples, conditioning_latents=conditioning_latents,\n                                  preset=args.preset, use_deterministic_seed=args.seed, return_deterministic_state=True, cvvp_amount=args.cvvp_amount)\n        if isinstance(gen, list):\n            for j, g in enumerate(gen):\n                torchaudio.save(os.path.join(args.output_path, f'{selected_voice}_{k}_{j}.wav'), g.squeeze(0).cpu(), 24000)\n        else:\n            torchaudio.save(os.path.join(args.output_path, f'{selected_voice}_{k}.wav'), gen.squeeze(0).cpu(), 24000)\n\n        if args.produce_debug_state:\n            os.makedirs('debug_states', exist_ok=True)\n            torch.save(dbg_state, f'debug_states/do_tts_debug_{selected_voice}.pth')\n\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/tortoise-tts/tortoise/do_tts.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class TextToSpeech",
            "Description": "实现了一个基于深度学习的 多阶段文本到语音（TTS）合成系统，结合了自回归模型、扩散模型和语音质量评估模型",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/tortoise-tts/tortoise/api.py",
            "Implementation": "class TextToSpeech:\n    \"\"\"\n    Main entry point into Tortoise.\n    \"\"\"\n\n    def __init__(self, autoregressive_batch_size=None, models_dir=MODELS_DIR, \n                 enable_redaction=True, kv_cache=False, use_deepspeed=False, half=False, device=None,\n                 tokenizer_vocab_file=None, tokenizer_basic=False):\n\n        \"\"\"\n        Constructor\n        :param autoregressive_batch_size: Specifies how many samples to generate per batch. Lower this if you are seeing\n                                          GPU OOM errors. Larger numbers generates slightly faster.\n        :param models_dir: Where model weights are stored. This should only be specified if you are providing your own\n                           models, otherwise use the defaults.\n        :param enable_redaction: When true, text enclosed in brackets are automatically redacted from the spoken output\n                                 (but are still rendered by the model). This can be used for prompt engineering.\n                                 Default is true.\n        :param device: Device to use when running the model. If omitted, the device will be automatically chosen.\n        \"\"\"\n        self.models_dir = models_dir\n        self.autoregressive_batch_size = pick_best_batch_size_for_gpu() if autoregressive_batch_size is None else autoregressive_batch_size\n        self.enable_redaction = enable_redaction\n        if device is None:\n            self.device = torch.device('cuda' if torch.cuda.is_available() else'cpu')\n        else:\n            self.device = torch.device(device)\n            \n        if torch.backends.mps.is_available():\n            self.device = torch.device('mps')\n        if self.enable_redaction:\n            self.aligner = Wav2VecAlignment()\n\n        self.tokenizer = VoiceBpeTokenizer(\n            vocab_file=tokenizer_vocab_file,\n            use_basic_cleaners=tokenizer_basic,\n        )\n        self.half = half\n        if os.path.exists(f'{models_dir}/autoregressive.ptt'):\n            # Assume this is a traced directory.\n            self.autoregressive = torch.jit.load(f'{models_dir}/autoregressive.ptt')\n            self.diffusion = torch.jit.load(f'{models_dir}/diffusion_decoder.ptt')\n        else:\n            self.autoregressive = UnifiedVoice(max_mel_tokens=604, max_text_tokens=402, max_conditioning_inputs=2, layers=30,\n                                          model_dim=1024,\n                                          heads=16, number_text_tokens=255, start_text_token=255, checkpointing=False,\n                                          train_solo_embeddings=False).cpu().eval()\n            self.autoregressive.load_state_dict(torch.load(get_model_path('autoregressive.pth', models_dir)), strict=False)\n            self.autoregressive.post_init_gpt2_config(use_deepspeed=use_deepspeed, kv_cache=kv_cache, half=self.half)\n            \n            self.diffusion = DiffusionTts(model_channels=1024, num_layers=10, in_channels=100, out_channels=200,\n                                          in_latent_channels=1024, in_tokens=8193, dropout=0, use_fp16=False, num_heads=16,\n                                          layer_drop=0, unconditioned_percentage=0).cpu().eval()\n            self.diffusion.load_state_dict(torch.load(get_model_path('diffusion_decoder.pth', models_dir)))\n\n        self.clvp = CLVP(dim_text=768, dim_speech=768, dim_latent=768, num_text_tokens=256, text_enc_depth=20,\n                         text_seq_len=350, text_heads=12,\n                         num_speech_tokens=8192, speech_enc_depth=20, speech_heads=12, speech_seq_len=430,\n                         use_xformers=True).cpu().eval()\n        self.clvp.load_state_dict(torch.load(get_model_path('clvp2.pth', models_dir)))\n        self.cvvp = None # CVVP model is only loaded if used.\n\n        self.vocoder = UnivNetGenerator().cpu()\n        self.vocoder.load_state_dict(torch.load(get_model_path('vocoder.pth', models_dir), map_location=torch.device('cpu'))['model_g'])\n        self.vocoder.eval(inference=True)\n        \n        self.stft = None # TacotronSTFT is only loaded if used.\n\n        # Random latent generators (RLGs) are loaded lazily.\n        self.rlg_auto = None\n        self.rlg_diffusion = None\n    @contextmanager\n    def temporary_cuda(self, model):\n        m = model.to(self.device)\n        yield m\n        m = model.cpu()\n\n    \n    def load_cvvp(self):\n        \"\"\"Load CVVP model.\"\"\"\n        self.cvvp = CVVP(model_dim=512, transformer_heads=8, dropout=0, mel_codes=8192, conditioning_enc_depth=8, cond_mask_percentage=0,\n                         speech_enc_depth=8, speech_mask_percentage=0, latent_multiplier=1).cpu().eval()\n        self.cvvp.load_state_dict(torch.load(get_model_path('cvvp.pth', self.models_dir)))\n\n    def get_conditioning_latents(self, voice_samples, return_mels=False):\n        \"\"\"\n        Transforms one or more voice_samples into a tuple (autoregressive_conditioning_latent, diffusion_conditioning_latent).\n        These are expressive learned latents that encode aspects of the provided clips like voice, intonation, and acoustic\n        properties.\n        :param voice_samples: List of 2 or more ~10 second reference clips, which should be torch tensors containing 22.05kHz waveform data.\n        \"\"\"\n        with torch.no_grad():\n            voice_samples = [v.to(self.device) for v in voice_samples]\n\n            auto_conds = []\n            if not isinstance(voice_samples, list):\n                voice_samples = [voice_samples]\n            for vs in voice_samples:\n                auto_conds.append(format_conditioning(vs, device=self.device))\n            auto_conds = torch.stack(auto_conds, dim=1)\n            self.autoregressive = self.autoregressive.to(self.device)\n            auto_latent = self.autoregressive.get_conditioning(auto_conds)\n            self.autoregressive = self.autoregressive.cpu()\n\n            if self.stft is None:\n                # Initialize STFT\n                self.stft = TacotronSTFT(1024, 256, 1024, 100, 24000, 0, 12000).to(self.device)\n\n            diffusion_conds = []\n            for sample in voice_samples:\n                # The diffuser operates at a sample rate of 24000 (except for the latent inputs)\n                sample = torchaudio.functional.resample(sample, 22050, 24000)\n                sample = pad_or_truncate(sample, 102400)\n                cond_mel = wav_to_univnet_mel(sample.to(self.device), do_normalization=False,\n                                              device=self.device, stft=self.stft)\n                diffusion_conds.append(cond_mel)\n            diffusion_conds = torch.stack(diffusion_conds, dim=1)\n\n            self.diffusion = self.diffusion.to(self.device)\n            diffusion_latent = self.diffusion.get_conditioning(diffusion_conds)\n            self.diffusion = self.diffusion.cpu()\n\n        if return_mels:\n            return auto_latent, diffusion_latent, auto_conds, diffusion_conds\n        else:\n            return auto_latent, diffusion_latent\n\n    def get_random_conditioning_latents(self):\n        # Lazy-load the RLG models.\n        if self.rlg_auto is None:\n            self.rlg_auto = RandomLatentConverter(1024).eval()\n            self.rlg_auto.load_state_dict(torch.load(get_model_path('rlg_auto.pth', self.models_dir), map_location=torch.device('cpu')))\n            self.rlg_diffusion = RandomLatentConverter(2048).eval()\n            self.rlg_diffusion.load_state_dict(torch.load(get_model_path('rlg_diffuser.pth', self.models_dir), map_location=torch.device('cpu')))\n        with torch.no_grad():\n            return self.rlg_auto(torch.tensor([0.0])), self.rlg_diffusion(torch.tensor([0.0]))\n\n    def tts_with_preset(self, text, preset='fast', **kwargs):\n        \"\"\"\n        Calls TTS with one of a set of preset generation parameters. Options:\n            'ultra_fast': Produces speech at a speed which belies the name of this repo. (Not really, but it's definitely fastest).\n            'fast': Decent quality speech at a decent inference rate. A good choice for mass inference.\n            'standard': Very good quality. This is generally about as good as you are going to get.\n            'high_quality': Use if you want the absolute best. This is not really worth the compute, though.\n        \"\"\"\n        # Use generally found best tuning knobs for generation.\n        settings = {'temperature': .8, 'length_penalty': 1.0, 'repetition_penalty': 2.0,\n                    'top_p': .8,\n                    'cond_free_k': 2.0, 'diffusion_temperature': 1.0}\n        # Presets are defined here.\n        presets = {\n            'ultra_fast': {'num_autoregressive_samples': 16, 'diffusion_iterations': 30, 'cond_free': False},\n            'fast': {'num_autoregressive_samples': 96, 'diffusion_iterations': 80},\n            'standard': {'num_autoregressive_samples': 256, 'diffusion_iterations': 200},\n            'high_quality': {'num_autoregressive_samples': 256, 'diffusion_iterations': 400},\n        }\n        settings.update(presets[preset])\n        settings.update(kwargs) # allow overriding of preset settings with kwargs\n        return self.tts(text, **settings)\n\n    def tts(self, text, voice_samples=None, conditioning_latents=None, k=1, verbose=True, use_deterministic_seed=None,\n            return_deterministic_state=False,\n            # autoregressive generation parameters follow\n            num_autoregressive_samples=512, temperature=.8, length_penalty=1, repetition_penalty=2.0, top_p=.8, max_mel_tokens=500,\n            # CVVP parameters follow\n            cvvp_amount=.0,\n            # diffusion generation parameters follow\n            diffusion_iterations=100, cond_free=True, cond_free_k=2, diffusion_temperature=1.0,\n            **hf_generate_kwargs):\n        \"\"\"\n        Produces an audio clip of the given text being spoken with the given reference voice.\n        :param text: Text to be spoken.\n        :param voice_samples: List of 2 or more ~10 second reference clips which should be torch tensors containing 22.05kHz waveform data.\n        :param conditioning_latents: A tuple of (autoregressive_conditioning_latent, diffusion_conditioning_latent), which\n                                     can be provided in lieu of voice_samples. This is ignored unless voice_samples=None.\n                                     Conditioning latents can be retrieved via get_conditioning_latents().\n        :param k: The number of returned clips. The most likely (as determined by Tortoises' CLVP model) clips are returned.\n        :param verbose: Whether or not to print log messages indicating the progress of creating a clip. Default=true.\n        ~~AUTOREGRESSIVE KNOBS~~\n        :param num_autoregressive_samples: Number of samples taken from the autoregressive model, all of which are filtered using CLVP.\n               As Tortoise is a probabilistic model, more samples means a higher probability of creating something \"great\".\n        :param temperature: The softmax temperature of the autoregressive model.\n        :param length_penalty: A length penalty applied to the autoregressive decoder. Higher settings causes the model to produce more terse outputs.\n        :param repetition_penalty: A penalty that prevents the autoregressive decoder from repeating itself during decoding. Can be used to reduce the incidence\n                                   of long silences or \"uhhhhhhs\", etc.\n        :param top_p: P value used in nucleus sampling. (0,1]. Lower values mean the decoder produces more \"likely\" (aka boring) outputs.\n        :param max_mel_tokens: Restricts the output length. (0,600] integer. Each unit is 1/20 of a second.\n        :param typical_sampling: Turns typical sampling on or off. This sampling mode is discussed in this paper: https://arxiv.org/abs/2202.00666\n                                 I was interested in the premise, but the results were not as good as I was hoping. This is off by default, but\n                                 could use some tuning.\n        :param typical_mass: The typical_mass parameter from the typical_sampling algorithm.\n        ~~CLVP-CVVP KNOBS~~\n        :param cvvp_amount: Controls the influence of the CVVP model in selecting the best output from the autoregressive model.\n                            [0,1]. Values closer to 1 mean the CVVP model is more important, 0 disables the CVVP model.\n        ~~DIFFUSION KNOBS~~\n        :param diffusion_iterations: Number of diffusion steps to perform. [0,4000]. More steps means the network has more chances to iteratively refine\n                                     the output, which should theoretically mean a higher quality output. Generally a value above 250 is not noticeably better,\n                                     however.\n        :param cond_free: Whether or not to perform conditioning-free diffusion. Conditioning-free diffusion performs two forward passes for\n                          each diffusion step: one with the outputs of the autoregressive model and one with no conditioning priors. The output\n                          of the two is blended according to the cond_free_k value below. Conditioning-free diffusion is the real deal, and\n                          dramatically improves realism.\n        :param cond_free_k: Knob that determines how to balance the conditioning free signal with the conditioning-present signal. [0,inf].\n                            As cond_free_k increases, the output becomes dominated by the conditioning-free signal.\n                            Formula is: output=cond_present_output*(cond_free_k+1)-cond_absenct_output*cond_free_k\n        :param diffusion_temperature: Controls the variance of the noise fed into the diffusion model. [0,1]. Values at 0\n                                      are the \"mean\" prediction of the diffusion network and will sound bland and smeared.\n        ~~OTHER STUFF~~\n        :param hf_generate_kwargs: The huggingface Transformers generate API is used for the autoregressive transformer.\n                                   Extra keyword args fed to this function get forwarded directly to that API. Documentation\n                                   here: https://huggingface.co/docs/transformers/internal/generation_utils\n        :return: Generated audio clip(s) as a torch tensor. Shape 1,S if k=1 else, (k,1,S) where S is the sample length.\n                 Sample rate is 24kHz.\n        \"\"\"\n        deterministic_seed = self.deterministic_state(seed=use_deterministic_seed)\n\n        text_tokens = torch.IntTensor(self.tokenizer.encode(text)).unsqueeze(0).to(self.device)\n        text_tokens = F.pad(text_tokens, (0, 1))  # This may not be necessary.\n        assert text_tokens.shape[-1] < 400, 'Too much text provided. Break the text up into separate segments and re-try inference.'\n        auto_conds = None\n        if voice_samples is not None:\n            auto_conditioning, diffusion_conditioning, auto_conds, _ = self.get_conditioning_latents(voice_samples, return_mels=True)\n        elif conditioning_latents is not None:\n            auto_conditioning, diffusion_conditioning = conditioning_latents\n        else:\n            auto_conditioning, diffusion_conditioning = self.get_random_conditioning_latents()\n        auto_conditioning = auto_conditioning.to(self.device)\n        diffusion_conditioning = diffusion_conditioning.to(self.device)\n\n        diffuser = load_discrete_vocoder_diffuser(desired_diffusion_steps=diffusion_iterations, cond_free=cond_free, cond_free_k=cond_free_k)\n\n        with torch.no_grad():\n            samples = []\n            num_batches = num_autoregressive_samples // self.autoregressive_batch_size\n            stop_mel_token = self.autoregressive.stop_mel_token\n            calm_token = 83  # This is the token for coding silence, which is fixed in place with \"fix_autoregressive_output\"\n            if verbose:\n                print(\"Generating autoregressive samples..\")\n            if not torch.backends.mps.is_available():\n                with self.temporary_cuda(self.autoregressive\n                ) as autoregressive, torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=self.half):\n                    for b in tqdm(range(num_batches), disable=not verbose):\n                        codes = autoregressive.inference_speech(auto_conditioning, text_tokens,\n                                                                    do_sample=True,\n                                                                    top_p=top_p,\n                                                                    temperature=temperature,\n                                                                    num_return_sequences=self.autoregressive_batch_size,\n                                                                    length_penalty=length_penalty,\n                                                                    repetition_penalty=repetition_penalty,\n                                                                    max_generate_length=max_mel_tokens,\n                                                                    **hf_generate_kwargs)\n                        padding_needed = max_mel_tokens - codes.shape[1]\n                        codes = F.pad(codes, (0, padding_needed), value=stop_mel_token)\n                        samples.append(codes)\n            else:\n                with self.temporary_cuda(self.autoregressive) as autoregressive:\n                    for b in tqdm(range(num_batches), disable=not verbose):\n                        codes = autoregressive.inference_speech(auto_conditioning, text_tokens,\n                                                                    do_sample=True,\n                                                                    top_p=top_p,\n                                                                    temperature=temperature,\n                                                                    num_return_sequences=self.autoregressive_batch_size,\n                                                                    length_penalty=length_penalty,\n                                                                    repetition_penalty=repetition_penalty,\n                                                                    max_generate_length=max_mel_tokens,\n                                                                    **hf_generate_kwargs)\n                        padding_needed = max_mel_tokens - codes.shape[1]\n                        codes = F.pad(codes, (0, padding_needed), value=stop_mel_token)\n                        samples.append(codes)\n\n            clip_results = []\n            \n            if not torch.backends.mps.is_available():\n                with self.temporary_cuda(self.clvp) as clvp, torch.autocast(\n                    device_type=\"cuda\" if not torch.backends.mps.is_available() else 'mps', dtype=torch.float16, enabled=self.half\n                ):\n                    if cvvp_amount > 0:\n                        if self.cvvp is None:\n                            self.load_cvvp()\n                        self.cvvp = self.cvvp.to(self.device)\n                    if verbose:\n                        if self.cvvp is None:\n                            print(\"Computing best candidates using CLVP\")\n                        else:\n                            print(f\"Computing best candidates using CLVP {((1-cvvp_amount) * 100):2.0f}% and CVVP {(cvvp_amount * 100):2.0f}%\")\n                    for batch in tqdm(samples, disable=not verbose):\n                        for i in range(batch.shape[0]):\n                            batch[i] = fix_autoregressive_output(batch[i], stop_mel_token)\n                        if cvvp_amount != 1:\n                            clvp_out = clvp(text_tokens.repeat(batch.shape[0], 1), batch, return_loss=False)\n                        if auto_conds is not None and cvvp_amount > 0:\n                            cvvp_accumulator = 0\n                            for cl in range(auto_conds.shape[1]):\n                                cvvp_accumulator = cvvp_accumulator + self.cvvp(auto_conds[:, cl].repeat(batch.shape[0], 1, 1), batch, return_loss=False)\n                            cvvp = cvvp_accumulator / auto_conds.shape[1]\n                            if cvvp_amount == 1:\n                                clip_results.append(cvvp)\n                            else:\n                                clip_results.append(cvvp * cvvp_amount + clvp_out * (1-cvvp_amount))\n                        else:\n                            clip_results.append(clvp_out)\n                    clip_results = torch.cat(clip_results, dim=0)\n                    samples = torch.cat(samples, dim=0)\n                    best_results = samples[torch.topk(clip_results, k=k).indices]\n            else:\n                with self.temporary_cuda(self.clvp) as clvp:\n                    if cvvp_amount > 0:\n                        if self.cvvp is None:\n                            self.load_cvvp()\n                        self.cvvp = self.cvvp.to(self.device)\n                    if verbose:\n                        if self.cvvp is None:\n                            print(\"Computing best candidates using CLVP\")\n                        else:\n                            print(f\"Computing best candidates using CLVP {((1-cvvp_amount) * 100):2.0f}% and CVVP {(cvvp_amount * 100):2.0f}%\")\n                    for batch in tqdm(samples, disable=not verbose):\n                        for i in range(batch.shape[0]):\n                            batch[i] = fix_autoregressive_output(batch[i], stop_mel_token)\n                        if cvvp_amount != 1:\n                            clvp_out = clvp(text_tokens.repeat(batch.shape[0], 1), batch, return_loss=False)\n                        if auto_conds is not None and cvvp_amount > 0:\n                            cvvp_accumulator = 0\n                            for cl in range(auto_conds.shape[1]):\n                                cvvp_accumulator = cvvp_accumulator + self.cvvp(auto_conds[:, cl].repeat(batch.shape[0], 1, 1), batch, return_loss=False)\n                            cvvp = cvvp_accumulator / auto_conds.shape[1]\n                            if cvvp_amount == 1:\n                                clip_results.append(cvvp)\n                            else:\n                                clip_results.append(cvvp * cvvp_amount + clvp_out * (1-cvvp_amount))\n                        else:\n                            clip_results.append(clvp_out)\n                    clip_results = torch.cat(clip_results, dim=0)\n                    samples = torch.cat(samples, dim=0)\n                    best_results = samples[torch.topk(clip_results, k=k).indices]\n            if self.cvvp is not None:\n                self.cvvp = self.cvvp.cpu()\n            del samples\n\n            # The diffusion model actually wants the last hidden layer from the autoregressive model as conditioning\n            # inputs. Re-produce those for the top results. This could be made more efficient by storing all of these\n            # results, but will increase memory usage.\n            if not torch.backends.mps.is_available():\n                with self.temporary_cuda(\n                    self.autoregressive\n                ) as autoregressive, torch.autocast(\n                    device_type=\"cuda\" if not torch.backends.mps.is_available() else 'mps', dtype=torch.float16, enabled=self.half\n                ):\n                    best_latents = autoregressive(auto_conditioning.repeat(k, 1), text_tokens.repeat(k, 1),\n                                                    torch.tensor([text_tokens.shape[-1]], device=text_tokens.device), best_results,\n                                                    torch.tensor([best_results.shape[-1]*self.autoregressive.mel_length_compression], device=text_tokens.device),\n                                                    return_latent=True, clip_inputs=False)\n                    del auto_conditioning\n            else:\n                with self.temporary_cuda(\n                    self.autoregressive\n                ) as autoregressive:\n                    best_latents = autoregressive(auto_conditioning.repeat(k, 1), text_tokens.repeat(k, 1),\n                                                    torch.tensor([text_tokens.shape[-1]], device=text_tokens.device), best_results,\n                                                    torch.tensor([best_results.shape[-1]*self.autoregressive.mel_length_compression], device=text_tokens.device),\n                                                    return_latent=True, clip_inputs=False)\n                    del auto_conditioning\n\n            if verbose:\n                print(\"Transforming autoregressive outputs into audio..\")\n            wav_candidates = []\n            if not torch.backends.mps.is_available():\n                with self.temporary_cuda(self.diffusion) as diffusion, self.temporary_cuda(\n                    self.vocoder\n                ) as vocoder:\n                    for b in range(best_results.shape[0]):\n                        codes = best_results[b].unsqueeze(0)\n                        latents = best_latents[b].unsqueeze(0)\n\n                        # Find the first occurrence of the \"calm\" token and trim the codes to that.\n                        ctokens = 0\n                        for k in range(codes.shape[-1]):\n                            if codes[0, k] == calm_token:\n                                ctokens += 1\n                            else:\n                                ctokens = 0\n                            if ctokens > 8:  # 8 tokens gives the diffusion model some \"breathing room\" to terminate speech.\n                                latents = latents[:, :k]\n                                break\n                        mel = do_spectrogram_diffusion(diffusion, diffuser, latents, diffusion_conditioning, temperature=diffusion_temperature, \n                                                    verbose=verbose)\n                        wav = vocoder.inference(mel)\n                        wav_candidates.append(wav.cpu())\n            else:\n                diffusion, vocoder = self.diffusion, self.vocoder\n                diffusion_conditioning = diffusion_conditioning.cpu()\n                for b in range(best_results.shape[0]):\n                    codes = best_results[b].unsqueeze(0).cpu()\n                    latents = best_latents[b].unsqueeze(0).cpu()\n\n                    # Find the first occurrence of the \"calm\" token and trim the codes to that.\n                    ctokens = 0\n                    for k in range(codes.shape[-1]):\n                        if codes[0, k] == calm_token:\n                            ctokens += 1\n                        else:\n                            ctokens = 0\n                        if ctokens > 8:  # 8 tokens gives the diffusion model some \"breathing room\" to terminate speech.\n                            latents = latents[:, :k]\n                            break\n                    mel = do_spectrogram_diffusion(diffusion, diffuser, latents, diffusion_conditioning, temperature=diffusion_temperature, \n                                                verbose=verbose)\n                    wav = vocoder.inference(mel)\n                    wav_candidates.append(wav.cpu())\n\n            def potentially_redact(clip, text):\n                if self.enable_redaction:\n                    return self.aligner.redact(clip.squeeze(1), text).unsqueeze(1)\n                return clip\n            wav_candidates = [potentially_redact(wav_candidate, text) for wav_candidate in wav_candidates]\n\n            if len(wav_candidates) > 1:\n                res = wav_candidates\n            else:\n                res = wav_candidates[0]\n\n            if return_deterministic_state:\n                return res, (deterministic_seed, text, voice_samples, conditioning_latents)\n            else:\n                return res\n    def deterministic_state(self, seed=None):\n        \"\"\"\n        Sets the random seeds that tortoise uses to the current time() and returns that seed so results can be\n        reproduced.\n        \"\"\"\n        seed = int(time()) if seed is None else seed\n        torch.manual_seed(seed)\n        random.seed(seed)\n        # Can't currently set this because of CUBLAS. TODO: potentially enable it if necessary.\n        # torch.use_deterministic_algorithms(True)\n\n        return seed\n",
            "Examples": []
        }
    ]
}