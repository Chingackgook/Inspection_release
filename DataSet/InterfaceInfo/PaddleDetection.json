{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/PaddleDetection",
    "API_Calls": [
        {
            "Name": "infer",
            "Description": "主要功能是进行目标检测的推理,实现对图像/视频进行目标检测预测",
            "Code": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n# add python path of PaddleDetection to sys.path\nparent_path = os.path.abspath(os.path.join(__file__, *(['..'] * 2)))\nsys.path.insert(0, parent_path)\n\n# ignore warning log\nimport warnings\nwarnings.filterwarnings('ignore')\nimport glob\nimport ast\n\nimport paddle\nfrom ppdet.core.workspace import create, load_config, merge_config\nfrom ppdet.engine import Trainer, Trainer_ARSL\nfrom ppdet.utils.check import check_gpu, check_npu, check_xpu, check_mlu, check_gcu, check_version, check_config\nfrom ppdet.utils.cli import ArgsParser, merge_args\nfrom ppdet.slim import build_slim_model\n\nfrom ppdet.utils.logger import setup_logger\nlogger = setup_logger('train')\n\n\ndef parse_args():\n    parser = ArgsParser()\n    parser.add_argument(\n        \"--infer_dir\",\n        type=str,\n        default=None,\n        help=\"Directory for images to perform inference on.\")\n    parser.add_argument(\n        \"--infer_list\",\n        type=str,\n        default=None,\n        help=\"The file path containing path of image to be infered. Valid only when --infer_dir is given.\"\n    )\n    parser.add_argument(\n        \"--infer_img\",\n        type=str,\n        default=None,\n        help=\"Image path, has higher priority over --infer_dir\")\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"output\",\n        help=\"Directory for storing the output visualization files.\")\n    parser.add_argument(\n        \"--draw_threshold\",\n        type=float,\n        default=0.5,\n        help=\"Threshold to reserve the result for visualization.\")\n    parser.add_argument(\n        \"--save_threshold\",\n        type=float,\n        default=0.5,\n        help=\"Threshold to reserve the result for saving.\")\n    parser.add_argument(\n        \"--slim_config\",\n        default=None,\n        type=str,\n        help=\"Configuration file of slim method.\")\n    parser.add_argument(\n        \"--use_vdl\",\n        type=bool,\n        default=False,\n        help=\"Whether to record the data to VisualDL.\")\n    parser.add_argument(\n        \"--do_eval\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"Whether to eval after infer.\")\n    parser.add_argument(\n        '--vdl_log_dir',\n        type=str,\n        default=\"vdl_log_dir/image\",\n        help='VisualDL logging directory for image.')\n    parser.add_argument(\n        \"--save_results\",\n        type=bool,\n        default=False,\n        help=\"Whether to save inference results to output_dir.\")\n    parser.add_argument(\n        \"--slice_infer\",\n        action='store_true',\n        help=\"Whether to slice the image and merge the inference results for small object detection.\"\n    )\n    parser.add_argument(\n        '--slice_size',\n        nargs='+',\n        type=int,\n        default=[640, 640],\n        help=\"Height of the sliced image.\")\n    parser.add_argument(\n        \"--overlap_ratio\",\n        nargs='+',\n        type=float,\n        default=[0.25, 0.25],\n        help=\"Overlap height ratio of the sliced image.\")\n    parser.add_argument(\n        \"--combine_method\",\n        type=str,\n        default='nms',\n        help=\"Combine method of the sliced images' detection results, choose in ['nms', 'nmm', 'concat'].\"\n    )\n    parser.add_argument(\n        \"--match_threshold\",\n        type=float,\n        default=0.6,\n        help=\"Combine method matching threshold.\")\n    parser.add_argument(\n        \"--match_metric\",\n        type=str,\n        default='ios',\n        help=\"Combine method matching metric, choose in ['iou', 'ios'].\")\n    parser.add_argument(\n        \"--visualize\",\n        type=ast.literal_eval,\n        default=True,\n        help=\"Whether to save visualize results to output_dir.\")\n    parser.add_argument(\n        \"--rtn_im_file\",\n        type=bool,\n        default=False,\n        help=\"Whether to return image file path in Dataloader.\")\n    args = parser.parse_args()\n    return args\n\n\ndef get_test_images(infer_dir, infer_img, infer_list=None):\n    \"\"\"\n    Get image path list in TEST mode\n    \"\"\"\n    assert infer_img is not None or infer_dir is not None, \\\n        \"--infer_img or --infer_dir should be set\"\n    assert infer_img is None or os.path.isfile(infer_img), \\\n            \"{} is not a file\".format(infer_img)\n    assert infer_dir is None or os.path.isdir(infer_dir), \\\n            \"{} is not a directory\".format(infer_dir)\n\n    # infer_img has a higher priority\n    if infer_img and os.path.isfile(infer_img):\n        return [infer_img]\n\n    images = set()\n    infer_dir = os.path.abspath(infer_dir)\n    assert os.path.isdir(infer_dir), \\\n        \"infer_dir {} is not a directory\".format(infer_dir)\n    if infer_list:\n        assert os.path.isfile(\n            infer_list), f\"infer_list {infer_list} is not a valid file path.\"\n        with open(infer_list, 'r') as f:\n            lines = f.readlines()\n        for line in lines:\n            images.update([os.path.join(infer_dir, line.strip())])\n    else:\n        exts = ['jpg', 'jpeg', 'png', 'bmp']\n        exts += [ext.upper() for ext in exts]\n        for ext in exts:\n            images.update(glob.glob('{}/*.{}'.format(infer_dir, ext)))\n    images = list(images)\n    assert len(images) > 0, \"no image found in {}\".format(infer_dir)\n    logger.info(\"Found {} inference images in total.\".format(len(images)))\n\n    return images\n\n\ndef run(FLAGS, cfg):\n    if FLAGS.rtn_im_file:\n        cfg['TestReader']['sample_transforms'][0]['Decode'][\n            'rtn_im_file'] = FLAGS.rtn_im_file\n    ssod_method = cfg.get('ssod_method', None)\n    if ssod_method == 'ARSL':\n        trainer = Trainer_ARSL(cfg, mode='test')\n        trainer.load_weights(cfg.weights, ARSL_eval=True)\n    else:\n        trainer = Trainer(cfg, mode='test')\n        trainer.load_weights(cfg.weights)\n    # get inference images\n    if FLAGS.do_eval:\n        dataset = create('TestDataset')()\n        images = dataset.get_images()\n    else:\n        images = get_test_images(FLAGS.infer_dir, FLAGS.infer_img, FLAGS.infer_list)\n\n    # inference\n    if FLAGS.slice_infer:\n        trainer.slice_predict(\n            images,\n            slice_size=FLAGS.slice_size,\n            overlap_ratio=FLAGS.overlap_ratio,\n            combine_method=FLAGS.combine_method,\n            match_threshold=FLAGS.match_threshold,\n            match_metric=FLAGS.match_metric,\n            draw_threshold=FLAGS.draw_threshold,\n            output_dir=FLAGS.output_dir,\n            save_results=FLAGS.save_results,\n            visualize=FLAGS.visualize)\n    else:\n        trainer.predict(\n            images,\n            draw_threshold=FLAGS.draw_threshold,\n            output_dir=FLAGS.output_dir,\n            save_results=FLAGS.save_results,\n            visualize=FLAGS.visualize,\n            save_threshold=FLAGS.save_threshold,\n            do_eval=FLAGS.do_eval)\n\n\ndef main():\n    FLAGS = parse_args()\n    cfg = load_config(FLAGS.config)\n    merge_args(cfg, FLAGS)\n    merge_config(FLAGS.opt)\n\n    # disable npu in config by default\n    if 'use_npu' not in cfg:\n        cfg.use_npu = False\n\n    # disable xpu in config by default\n    if 'use_xpu' not in cfg:\n        cfg.use_xpu = False\n\n    if 'use_gpu' not in cfg:\n        cfg.use_gpu = False\n\n    # disable mlu in config by default\n    if 'use_mlu' not in cfg:\n        cfg.use_mlu = False\n\n    # disable gcu in config by default\n    if 'use_gcu' not in cfg:\n        cfg.use_gcu = False\n\n    if cfg.use_gpu:\n        place = paddle.set_device('gpu')\n    elif cfg.use_npu:\n        place = paddle.set_device('npu')\n    elif cfg.use_xpu:\n        place = paddle.set_device('xpu')\n    elif cfg.use_mlu:\n        place = paddle.set_device('mlu')\n    elif cfg.use_gcu:\n        place = paddle.set_device('gcu')\n    else:\n        place = paddle.set_device('cpu')\n\n    if FLAGS.slim_config:\n        cfg = build_slim_model(cfg, FLAGS.slim_config, mode='test')\n\n    check_config(cfg)\n    check_gpu(cfg.use_gpu)\n    check_npu(cfg.use_npu)\n    check_xpu(cfg.use_xpu)\n    check_mlu(cfg.use_mlu)\n    check_gcu(cfg.use_gcu)\n    check_version()\n    run(FLAGS, cfg)\n\n\nif __name__ == '__main__':\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/PaddleDetection/tools/infer.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class Trainer",
            "Description": "PaddlePaddle框架中用于深度学习模型训练、评估和推理的核心组件,核心方法为Trainer.predict()和Trainer.slice_predict()",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/PaddleDetection/ppdet/engine/trainer.py",
            "Implementation": "class Trainer(object):\n    def __init__(self, cfg, mode='train'):\n        self.cfg = cfg.copy()\n        assert mode.lower() in ['train', 'eval', 'test'], \\\n                \"mode should be 'train', 'eval' or 'test'\"\n        self.mode = mode.lower()\n        self.optimizer = None\n        self.is_loaded_weights = False\n        self.use_amp = self.cfg.get('amp', False)\n        self.amp_level = self.cfg.get('amp_level', 'O1')\n        self.custom_white_list = self.cfg.get('custom_white_list', None)\n        self.custom_black_list = self.cfg.get('custom_black_list', None)\n        self.use_master_grad = self.cfg.get('master_grad', False)\n        self.uniform_output_enabled = self.cfg.get('uniform_output_enabled', False)\n        if ('slim' in cfg and cfg['slim_type'] == 'PTQ') or self.uniform_output_enabled:\n            self.cfg['TestDataset'] = create('TestDataset')()\n        log_ranks = cfg.get('log_ranks', '0')\n        if isinstance(log_ranks, str):\n            self.log_ranks = [int(i) for i in log_ranks.split(',')]\n        elif isinstance(log_ranks, int):\n            self.log_ranks = [log_ranks]\n        train_results_path = os.path.abspath(os.path.join(self.cfg.save_dir, \"train_result.json\"))\n        if self.uniform_output_enabled:\n            if os.path.exists(train_results_path) and self.mode == 'train':\n                try:\n                    os.remove(train_results_path)\n                except:\n                    pass\n            if not os.path.exists(self.cfg.save_dir):\n                os.makedirs(self.cfg.save_dir)\n            with open(os.path.join(self.cfg.save_dir, \"config.yaml\"), \"w\") as f:\n                config_dict = convert_to_dict(self.cfg)\n                config_dict = {k: v for k, v in config_dict.items() if v != {}}\n                yaml.dump(config_dict, f)\n\n        # build data loader\n        capital_mode = self.mode.capitalize()\n        if cfg.architecture in MOT_ARCH and self.mode in [\n                'eval', 'test'\n        ] and cfg.metric not in ['COCO', 'VOC']:\n            self.dataset = self.cfg['{}MOTDataset'.format(\n                capital_mode)] = create('{}MOTDataset'.format(capital_mode))()\n        else:\n            self.dataset = self.cfg['{}Dataset'.format(capital_mode)] = create(\n                '{}Dataset'.format(capital_mode))()\n\n        if cfg.architecture == 'DeepSORT' and self.mode == 'train':\n            logger.error('DeepSORT has no need of training on mot dataset.')\n            sys.exit(1)\n\n        if cfg.architecture == 'FairMOT' and self.mode == 'eval':\n            images = self.parse_mot_images(cfg)\n            self.dataset.set_images(images)\n\n        if self.mode == 'train':\n            self.loader = create('{}Reader'.format(capital_mode))(\n                self.dataset, cfg.worker_num)\n\n        if cfg.architecture == 'JDE' and self.mode == 'train':\n            self.cfg['JDEEmbeddingHead'][\n                'num_identities'] = self.dataset.num_identities_dict[0]\n            # JDE only support single class MOT now.\n\n        if cfg.architecture == 'FairMOT' and self.mode == 'train':\n            self.cfg['FairMOTEmbeddingHead'][\n                'num_identities_dict'] = self.dataset.num_identities_dict\n            # FairMOT support single class and multi-class MOT now.\n\n        # build model\n        if 'model' not in self.cfg:\n            self.model = create(cfg.architecture)\n        else:\n            self.model = self.cfg.model\n            self.is_loaded_weights = True\n\n        if cfg.architecture == 'YOLOX':\n            for k, m in self.model.named_sublayers():\n                if isinstance(m, nn.BatchNorm2D):\n                    m._epsilon = 1e-3  # for amp(fp16)\n                    m._momentum = 0.97  # 0.03 in pytorch\n\n        # reset norm param attr for setting them in optimizer\n        if 'reset_norm_param_attr' in cfg and cfg['reset_norm_param_attr']:\n            self.model = self.reset_norm_param_attr(\n                self.model, weight_attr=None, bias_attr=None)\n\n        # normalize params for deploy\n        if 'slim' in cfg and cfg['slim_type'] == 'OFA':\n            self.model.model.load_meanstd(cfg['TestReader'][\n                'sample_transforms'])\n        elif 'slim' in cfg and cfg['slim_type'] == 'Distill':\n            self.model.student_model.load_meanstd(cfg['TestReader'][\n                'sample_transforms'])\n        elif 'slim' in cfg and cfg[\n                'slim_type'] == 'DistillPrune' and self.mode == 'train':\n            self.model.student_model.load_meanstd(cfg['TestReader'][\n                'sample_transforms'])\n        else:\n            self.model.load_meanstd(cfg['TestReader']['sample_transforms'])\n\n        # EvalDataset build with BatchSampler to evaluate in single device\n        # TODO: multi-device evaluate\n        if self.mode == 'eval':\n            if cfg.architecture == 'FairMOT':\n                self.loader = create('EvalMOTReader')(self.dataset, 0)\n            elif cfg.architecture == \"METRO_Body\":\n                reader_name = '{}Reader'.format(self.mode.capitalize())\n                self.loader = create(reader_name)(self.dataset, cfg.worker_num)\n            else:\n                self._eval_batch_sampler = paddle.io.BatchSampler(\n                    self.dataset, batch_size=self.cfg.EvalReader['batch_size'])\n                reader_name = '{}Reader'.format(self.mode.capitalize())\n                # If metric is VOC, need to be set collate_batch=False.\n                if cfg.metric == 'VOC':\n                    self.cfg[reader_name]['collate_batch'] = False\n                self.loader = create(reader_name)(self.dataset, cfg.worker_num,\n                                                  self._eval_batch_sampler)\n        # TestDataset build after user set images, skip loader creation here\n\n        # get Params\n        print_params = self.cfg.get('print_params', False)\n        if print_params:\n            params = sum([\n                p.numel() for n, p in self.model.named_parameters()\n                if all([x not in n for x in ['_mean', '_variance', 'aux_']])\n            ])  # exclude BatchNorm running status\n            logger.info('Model Params : {} M.'.format((params / 1e6).numpy()[\n                0]))\n\n        # build optimizer in train mode\n        if self.mode == 'train':\n            steps_per_epoch = len(self.loader)\n            if steps_per_epoch < 1:\n                logger.warning(\n                    \"Samples in dataset are less than batch_size, please set smaller batch_size in TrainReader.\"\n                )\n            self.lr = create('LearningRate')(steps_per_epoch)\n            self.optimizer = create('OptimizerBuilder')(self.lr, self.model)\n\n            # Unstructured pruner is only enabled in the train mode.\n            if self.cfg.get('unstructured_prune'):\n                self.pruner = create('UnstructuredPruner')(self.model,\n                                                           steps_per_epoch)\n        if self.use_amp and self.amp_level == 'O2':\n            paddle_version = paddle.__version__[:3]\n            # paddle version >= 2.5.0 or develop\n            if paddle_version in [\"2.5\", \"0.0\"]:\n                self.model, self.optimizer = paddle.amp.decorate(\n                    models=self.model,\n                    optimizers=self.optimizer,\n                    level=self.amp_level,\n                    master_grad=self.use_master_grad)\n            else:\n                self.model, self.optimizer = paddle.amp.decorate(\n                    models=self.model,\n                    optimizers=self.optimizer,\n                    level=self.amp_level)\n\n        # support sync_bn for npu/xpu\n        if (paddle.get_device()[:3]=='npu' or paddle.get_device()[:3]=='xpu'):\n            use_npu = ('use_npu' in cfg and cfg['use_npu'])\n            use_xpu = ('use_xpu' in cfg and cfg['use_xpu'])\n            use_mlu = ('use_mlu' in cfg and cfg['use_mlu'])\n            norm_type = ('norm_type' in cfg and cfg['norm_type'])\n            if norm_type == 'sync_bn' and (use_npu or use_xpu or use_mlu) and dist.get_world_size() > 1:\n                convert_syncbn(self.model)\n\n        self.use_ema = ('use_ema' in cfg and cfg['use_ema'])\n        if self.use_ema:\n            ema_decay = self.cfg.get('ema_decay', 0.9998)\n            ema_decay_type = self.cfg.get('ema_decay_type', 'threshold')\n            cycle_epoch = self.cfg.get('cycle_epoch', -1)\n            ema_black_list = self.cfg.get('ema_black_list', None)\n            ema_filter_no_grad = self.cfg.get('ema_filter_no_grad', False)\n            self.ema = ModelEMA(\n                self.model,\n                decay=ema_decay,\n                ema_decay_type=ema_decay_type,\n                cycle_epoch=cycle_epoch,\n                ema_black_list=ema_black_list,\n                ema_filter_no_grad=ema_filter_no_grad)\n\n        self._nranks = dist.get_world_size()\n        self._local_rank = dist.get_rank()\n\n        self.status = {}\n\n        self.start_epoch = 0\n        self.end_epoch = 0 if 'epoch' not in cfg else cfg.epoch\n\n        # initial default callbacks\n        self._init_callbacks()\n\n        # initial default metrics\n        self._init_metrics()\n        self._reset_metrics()\n\n    def _init_callbacks(self):\n        if self.mode == 'train':\n            if self.cfg.get('ssod_method',\n                            False) and self.cfg['ssod_method'] == 'Semi_RTDETR':\n                self._callbacks = [SemiLogPrinter(self), SemiCheckpointer(self)]\n            else:\n                self._callbacks = [LogPrinter(self), Checkpointer(self)]\n            if self.cfg.get('use_vdl', False):\n                self._callbacks.append(VisualDLWriter(self))\n            if self.cfg.get('save_proposals', False):\n                self._callbacks.append(SniperProposalsGenerator(self))\n            if self.cfg.get('use_wandb', False) or 'wandb' in self.cfg:\n                self._callbacks.append(WandbCallback(self))\n            self._compose_callback = ComposeCallback(self._callbacks)\n        elif self.mode == 'eval':\n            self._callbacks = [LogPrinter(self)]\n            # if self.cfg.metric == 'WiderFace':\n            #     self._callbacks.append(WiferFaceEval(self))\n            self._compose_callback = ComposeCallback(self._callbacks)\n        elif self.mode == 'test' and self.cfg.get('use_vdl', False):\n            self._callbacks = [VisualDLWriter(self)]\n            self._compose_callback = ComposeCallback(self._callbacks)\n        else:\n            self._callbacks = []\n            self._compose_callback = None\n\n    def _init_metrics(self, validate=False):\n        if self.mode == 'test' or (self.mode == 'train' and not validate):\n            self._metrics = []\n            return\n        classwise = self.cfg['classwise'] if 'classwise' in self.cfg else False\n        if self.cfg.metric == 'COCO' or self.cfg.metric == \"SNIPERCOCO\":\n            # TODO: bias should be unified\n            bias = 1 if self.cfg.get('bias', False) else 0\n            output_eval = self.cfg['output_eval'] \\\n                if 'output_eval' in self.cfg else None\n            save_prediction_only = self.cfg.get('save_prediction_only', False)\n\n            # pass clsid2catid info to metric instance to avoid multiple loading\n            # annotation file\n            clsid2catid = {v: k for k, v in self.dataset.catid2clsid.items()} \\\n                                if self.mode == 'eval' else None\n\n            save_threshold = self.cfg.get('save_threshold', 0)\n\n            # when do validation in train, annotation file should be get from\n            # EvalReader instead of self.dataset(which is TrainReader)\n            if self.mode == 'train' and validate:\n                eval_dataset = self.cfg['EvalDataset']\n                eval_dataset.check_or_download_dataset()\n                anno_file = eval_dataset.get_anno()\n                dataset = eval_dataset\n            else:\n                dataset = self.dataset\n                anno_file = dataset.get_anno()\n\n            IouType = self.cfg['IouType'] if 'IouType' in self.cfg else 'bbox'\n            if self.cfg.metric == \"COCO\":\n                self._metrics = [\n                    COCOMetric(\n                        anno_file=anno_file,\n                        clsid2catid=clsid2catid,\n                        classwise=classwise,\n                        output_eval=output_eval,\n                        bias=bias,\n                        IouType=IouType,\n                        save_prediction_only=save_prediction_only,\n                        save_threshold=save_threshold)\n                ]\n            elif self.cfg.metric == \"SNIPERCOCO\":  # sniper\n                self._metrics = [\n                    SNIPERCOCOMetric(\n                        anno_file=anno_file,\n                        dataset=dataset,\n                        clsid2catid=clsid2catid,\n                        classwise=classwise,\n                        output_eval=output_eval,\n                        bias=bias,\n                        IouType=IouType,\n                        save_prediction_only=save_prediction_only)\n                ]\n        elif self.cfg.metric == 'RBOX':\n            # TODO: bias should be unified\n            bias = self.cfg['bias'] if 'bias' in self.cfg else 0\n            output_eval = self.cfg['output_eval'] \\\n                if 'output_eval' in self.cfg else None\n            save_prediction_only = self.cfg.get('save_prediction_only', False)\n            imid2path = self.cfg.get('imid2path', None)\n\n            # when do validation in train, annotation file should be get from\n            # EvalReader instead of self.dataset(which is TrainReader)\n            anno_file = self.dataset.get_anno()\n            if self.mode == 'train' and validate:\n                eval_dataset = self.cfg['EvalDataset']\n                eval_dataset.check_or_download_dataset()\n                anno_file = eval_dataset.get_anno()\n\n            self._metrics = [\n                RBoxMetric(\n                    anno_file=anno_file,\n                    classwise=classwise,\n                    output_eval=output_eval,\n                    bias=bias,\n                    save_prediction_only=save_prediction_only,\n                    imid2path=imid2path)\n            ]\n        elif self.cfg.metric == 'VOC':\n            output_eval = self.cfg['output_eval'] \\\n                if 'output_eval' in self.cfg else None\n            save_prediction_only = self.cfg.get('save_prediction_only', False)\n\n            self._metrics = [\n                VOCMetric(\n                    label_list=self.dataset.get_label_list(),\n                    class_num=self.cfg.num_classes,\n                    map_type=self.cfg.map_type,\n                    classwise=classwise,\n                    output_eval=output_eval,\n                    save_prediction_only=save_prediction_only)\n            ]\n        elif self.cfg.metric == 'WiderFace':\n            self._metrics = [\n                WiderFaceMetric()\n            ]\n        elif self.cfg.metric == 'KeyPointTopDownCOCOEval':\n            eval_dataset = self.cfg['EvalDataset']\n            eval_dataset.check_or_download_dataset()\n            anno_file = eval_dataset.get_anno()\n            save_prediction_only = self.cfg.get('save_prediction_only', False)\n            self._metrics = [\n                KeyPointTopDownCOCOEval(\n                    anno_file,\n                    len(eval_dataset),\n                    self.cfg.num_joints,\n                    self.cfg.save_dir,\n                    save_prediction_only=save_prediction_only)\n            ]\n        elif self.cfg.metric == 'KeyPointTopDownCOCOWholeBadyHandEval':\n            eval_dataset = self.cfg['EvalDataset']\n            eval_dataset.check_or_download_dataset()\n            anno_file = eval_dataset.get_anno()\n            save_prediction_only = self.cfg.get('save_prediction_only', False)\n            self._metrics = [\n                KeyPointTopDownCOCOWholeBadyHandEval(\n                    anno_file,\n                    len(eval_dataset),\n                    self.cfg.num_joints,\n                    self.cfg.save_dir,\n                    save_prediction_only=save_prediction_only)\n            ]\n        elif self.cfg.metric == 'KeyPointTopDownMPIIEval':\n            eval_dataset = self.cfg['EvalDataset']\n            eval_dataset.check_or_download_dataset()\n            anno_file = eval_dataset.get_anno()\n            save_prediction_only = self.cfg.get('save_prediction_only', False)\n            self._metrics = [\n                KeyPointTopDownMPIIEval(\n                    anno_file,\n                    len(eval_dataset),\n                    self.cfg.num_joints,\n                    self.cfg.save_dir,\n                    save_prediction_only=save_prediction_only)\n            ]\n        elif self.cfg.metric == 'Pose3DEval':\n            save_prediction_only = self.cfg.get('save_prediction_only', False)\n            self._metrics = [\n                Pose3DEval(\n                    self.cfg.save_dir,\n                    save_prediction_only=save_prediction_only)\n            ]\n        elif self.cfg.metric == 'MOTDet':\n            self._metrics = [JDEDetMetric(), ]\n        elif self.cfg.metric == 'CULaneMetric':\n            output_eval = self.cfg.get('output_eval', None)\n            self._metrics = [\n                CULaneMetric(\n                    cfg=self.cfg,\n                    output_eval=output_eval,\n                    split=self.dataset.split,\n                    dataset_dir=self.cfg.dataset_dir)\n            ]\n        else:\n            logger.warning(\"Metric not support for metric type {}\".format(\n                self.cfg.metric))\n            self._metrics = []\n\n    def _reset_metrics(self):\n        for metric in self._metrics:\n            metric.reset()\n\n    def register_callbacks(self, callbacks):\n        callbacks = [c for c in list(callbacks) if c is not None]\n        for c in callbacks:\n            assert isinstance(c, Callback), \\\n                    \"metrics shoule be instances of subclass of Metric\"\n        self._callbacks.extend(callbacks)\n        self._compose_callback = ComposeCallback(self._callbacks)\n\n    def register_metrics(self, metrics):\n        metrics = [m for m in list(metrics) if m is not None]\n        for m in metrics:\n            assert isinstance(m, Metric), \\\n                    \"metrics shoule be instances of subclass of Metric\"\n        self._metrics.extend(metrics)\n\n    def load_weights(self, weights, ARSL_eval=False):\n        if self.is_loaded_weights:\n            return\n        self.start_epoch = 0\n        load_pretrain_weight(self.model, weights, ARSL_eval)\n        logger.debug(\"Load weights {} to start training\".format(weights))\n\n    def load_weights_sde(self, det_weights, reid_weights):\n        if self.model.detector:\n            load_weight(self.model.detector, det_weights)\n            if self.model.reid:\n                load_weight(self.model.reid, reid_weights)\n        else:\n            load_weight(self.model.reid, reid_weights)\n\n    def resume_weights(self, weights):\n        # support Distill resume weights\n        if hasattr(self.model, 'student_model'):\n            self.start_epoch = load_weight(self.model.student_model, weights,\n                                           self.optimizer)\n        else:\n            self.start_epoch = load_weight(self.model, weights, self.optimizer,\n                                           self.ema if self.use_ema else None)\n        logger.debug(\"Resume weights of epoch {}\".format(self.start_epoch))\n\n    def train(self, validate=False):\n        assert self.mode == 'train', \"Model not in 'train' mode\"\n        Init_mark = False\n        if validate:\n            self.cfg['EvalDataset'] = self.cfg.EvalDataset = create(\n                \"EvalDataset\")()\n\n        model = self.model\n        if self.cfg.get('to_static', False):\n            model = apply_to_static(self.cfg, model)\n        sync_bn = (getattr(self.cfg, 'norm_type', None) == 'sync_bn' and\n                   self.cfg.use_gpu and self._nranks > 1)\n        if sync_bn:\n            model = paddle.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n\n        # enabel auto mixed precision mode\n        if self.use_amp:\n            scaler = paddle.amp.GradScaler(\n                enable=self.cfg.use_gpu or self.cfg.use_npu or self.cfg.use_mlu,\n                init_loss_scaling=self.cfg.get('init_loss_scaling', 1024))\n        # get distributed model\n        if self.cfg.get('fleet', False):\n            model = fleet.distributed_model(model)\n            self.optimizer = fleet.distributed_optimizer(self.optimizer)\n        elif self._nranks > 1:\n            find_unused_parameters = self.cfg[\n                'find_unused_parameters'] if 'find_unused_parameters' in self.cfg else False\n            model = paddle.DataParallel(\n                model, find_unused_parameters=find_unused_parameters)\n\n        self.status.update({\n            'epoch_id': self.start_epoch,\n            'step_id': 0,\n            'steps_per_epoch': len(self.loader)\n        })\n\n        self.status['batch_time'] = stats.SmoothedValue(\n            self.cfg.log_iter, fmt='{avg:.4f}')\n        self.status['data_time'] = stats.SmoothedValue(\n            self.cfg.log_iter, fmt='{avg:.4f}')\n        self.status['training_staus'] = stats.TrainingStats(self.cfg.log_iter)\n\n        if self.cfg.get('print_flops', False):\n            flops_loader = create('{}Reader'.format(self.mode.capitalize()))(\n                self.dataset, self.cfg.worker_num)\n            self._flops(flops_loader)\n        profiler_options = self.cfg.get('profiler_options', None)\n\n        self._compose_callback.on_train_begin(self.status)\n\n        use_fused_allreduce_gradients = self.cfg[\n            'use_fused_allreduce_gradients'] if 'use_fused_allreduce_gradients' in self.cfg else False\n\n        for epoch_id in range(self.start_epoch, self.cfg.epoch):\n            self.status['mode'] = 'train'\n            self.status['epoch_id'] = epoch_id\n            self._compose_callback.on_epoch_begin(self.status)\n            self.loader.dataset.set_epoch(epoch_id)\n            model.train()\n            iter_tic = time.time()\n            for step_id, data in enumerate(self.loader):\n                def deep_pin(blob, blocking):\n                    if isinstance(blob, paddle.Tensor):\n                        return blob.cuda(blocking=blocking)\n                    elif isinstance(blob, dict):\n                        return {k: deep_pin(v, blocking) for k, v in blob.items()}\n                    elif isinstance(blob, (list, tuple)):\n                        return type(blob)([deep_pin(x, blocking) for x in blob])\n                    else:\n                        return blob\n                if paddle.base.core.is_compiled_with_cuda():\n                    data = deep_pin(data, False)\n\n                self.status['data_time'].update(time.time() - iter_tic)\n                self.status['step_id'] = step_id\n                profiler.add_profiler_step(profiler_options)\n                self._compose_callback.on_step_begin(self.status)\n                data['epoch_id'] = epoch_id\n                if self.cfg.get('to_static',\n                                False) and 'image_file' in data.keys():\n                    data.pop('image_file')\n\n                if self.use_amp:\n                    if isinstance(\n                            model, paddle.\n                            DataParallel) and use_fused_allreduce_gradients:\n                        with model.no_sync():\n                            with paddle.amp.auto_cast(\n                                    enable=self.cfg.use_gpu or\n                                    self.cfg.use_npu or self.cfg.use_mlu,\n                                    custom_white_list=self.custom_white_list,\n                                    custom_black_list=self.custom_black_list,\n                                    level=self.amp_level):\n                                # model forward\n                                outputs = model(data)\n                                loss = outputs['loss']\n                            # model backward\n                            scaled_loss = scaler.scale(loss)\n                            scaled_loss.backward()\n                        fused_allreduce_gradients(\n                            list(model.parameters()), None)\n                    else:\n                        with paddle.amp.auto_cast(\n                                enable=self.cfg.use_gpu or self.cfg.use_npu or\n                                self.cfg.use_mlu,\n                                custom_white_list=self.custom_white_list,\n                                custom_black_list=self.custom_black_list,\n                                level=self.amp_level):\n                            # model forward\n                            outputs = model(data)\n                            loss = outputs['loss']\n                        # model backward\n                        scaled_loss = scaler.scale(loss)\n                        scaled_loss.backward()\n                    # in dygraph mode, optimizer.minimize is equal to optimizer.step\n                    scaler.minimize(self.optimizer, scaled_loss)\n                else:\n                    if isinstance(\n                            model, paddle.\n                            DataParallel) and use_fused_allreduce_gradients:\n                        with model.no_sync():\n                            # model forward\n                            outputs = model(data)\n                            loss = outputs['loss']\n                            # model backward\n                            loss.backward()\n                        fused_allreduce_gradients(\n                            list(model.parameters()), None)\n                    else:\n                        # model forward\n                        outputs = model(data)\n                        loss = outputs['loss']\n                        # model backward\n                        loss.backward()\n                    self.optimizer.step()\n                curr_lr = self.optimizer.get_lr()\n                self.lr.step()\n                if self.cfg.get('unstructured_prune'):\n                    self.pruner.step()\n                self.optimizer.clear_grad()\n                self.status['learning_rate'] = curr_lr\n\n                if self._nranks < 2 or self._local_rank in self.log_ranks:\n                    self.status['training_staus'].update(outputs)\n\n                self.status['batch_time'].update(time.time() - iter_tic)\n                self._compose_callback.on_step_end(self.status)\n                if self.use_ema:\n                    self.ema.update()\n                iter_tic = time.time()\n\n            if self.cfg.get('unstructured_prune'):\n                self.pruner.update_params()\n\n            is_snapshot = (self._nranks < 2 or (self._local_rank == 0 or self.cfg.metric == \"Pose3DEval\")) \\\n                       and ((epoch_id + 1) % self.cfg.snapshot_epoch == 0 or epoch_id == self.end_epoch - 1)\n            if is_snapshot and self.use_ema:\n                # apply ema weight on model\n                weight = copy.deepcopy(self.model.state_dict())\n                self.model.set_dict(self.ema.apply())\n                self.status['weight'] = weight\n\n            self._compose_callback.on_epoch_end(self.status)\n\n            if validate and is_snapshot:\n                if not hasattr(self, '_eval_loader'):\n                    # build evaluation dataset and loader\n                    self._eval_dataset = self.cfg.EvalDataset\n                    self._eval_batch_sampler = \\\n                        paddle.io.BatchSampler(\n                            self._eval_dataset,\n                            batch_size=self.cfg.EvalReader['batch_size'])\n                    # If metric is VOC, need to be set collate_batch=False.\n                    if self.cfg.metric == 'VOC':\n                        self.cfg['EvalReader']['collate_batch'] = False\n                    if self.cfg.metric == \"Pose3DEval\":\n                        self._eval_loader = create('EvalReader')(\n                            self._eval_dataset, self.cfg.worker_num)\n                    else:\n                        self._eval_loader = create('EvalReader')(\n                            self._eval_dataset,\n                            self.cfg.worker_num,\n                            batch_sampler=self._eval_batch_sampler)\n                # if validation in training is enabled, metrics should be re-init\n                # Init_mark makes sure this code will only execute once\n                if validate and Init_mark == False:\n                    Init_mark = True\n                    self._init_metrics(validate=validate)\n                    self._reset_metrics()\n\n                with paddle.no_grad():\n                    self.status['save_best_model'] = True\n                    self._eval_with_loader(self._eval_loader)\n\n            if is_snapshot and self.use_ema:\n                # reset original weight\n                self.model.set_dict(weight)\n                self.status.pop('weight')\n\n        self._compose_callback.on_train_end(self.status)\n\n    def _eval_with_loader(self, loader):\n        sample_num = 0\n        tic = time.time()\n        self._compose_callback.on_epoch_begin(self.status)\n        self.status['mode'] = 'eval'\n\n        self.model.eval()\n        if self.cfg.get('print_flops', False):\n            flops_loader = create('{}Reader'.format(self.mode.capitalize()))(\n                self.dataset, self.cfg.worker_num, self._eval_batch_sampler)\n            self._flops(flops_loader)\n        for step_id, data in enumerate(loader):\n            self.status['step_id'] = step_id\n            self._compose_callback.on_step_begin(self.status)\n            # forward\n            if self.use_amp:\n                with paddle.amp.auto_cast(\n                        enable=self.cfg.use_gpu or self.cfg.use_npu or\n                        self.cfg.use_mlu,\n                        custom_white_list=self.custom_white_list,\n                        custom_black_list=self.custom_black_list,\n                        level=self.amp_level):\n                    outs = self.model(data)\n            else:\n                outs = self.model(data)\n\n            # update metrics\n            for metric in self._metrics:\n                metric.update(data, outs)\n\n            # multi-scale inputs: all inputs have same im_id\n            if isinstance(data, typing.Sequence):\n                sample_num += data[0]['im_id'].numpy().shape[0]\n            else:\n                sample_num += data['im_id'].numpy().shape[0]\n            self._compose_callback.on_step_end(self.status)\n\n        self.status['sample_num'] = sample_num\n        self.status['cost_time'] = time.time() - tic\n\n        # accumulate metric to log out\n        for metric in self._metrics:\n            metric.accumulate()\n            metric.log()\n        self._compose_callback.on_epoch_end(self.status)\n        # reset metric states for metric may performed multiple times\n        self._reset_metrics()\n\n    def evaluate(self):\n        # get distributed model\n        if self.cfg.get('fleet', False):\n            self.model = fleet.distributed_model(self.model)\n            self.optimizer = fleet.distributed_optimizer(self.optimizer)\n        elif self._nranks > 1:\n            find_unused_parameters = self.cfg[\n                'find_unused_parameters'] if 'find_unused_parameters' in self.cfg else False\n            self.model = paddle.DataParallel(\n                self.model, find_unused_parameters=find_unused_parameters)\n        with paddle.no_grad():\n            self._eval_with_loader(self.loader)\n\n    def _eval_with_loader_slice(self,\n                                loader,\n                                slice_size=[640, 640],\n                                overlap_ratio=[0.25, 0.25],\n                                combine_method='nms',\n                                match_threshold=0.6,\n                                match_metric='iou'):\n        sample_num = 0\n        tic = time.time()\n        self._compose_callback.on_epoch_begin(self.status)\n        self.status['mode'] = 'eval'\n        self.model.eval()\n        if self.cfg.get('print_flops', False):\n            flops_loader = create('{}Reader'.format(self.mode.capitalize()))(\n                self.dataset, self.cfg.worker_num, self._eval_batch_sampler)\n            self._flops(flops_loader)\n\n        merged_bboxs = []\n        for step_id, data in enumerate(loader):\n            self.status['step_id'] = step_id\n            self._compose_callback.on_step_begin(self.status)\n            # forward\n            if self.use_amp:\n                with paddle.amp.auto_cast(\n                        enable=self.cfg.use_gpu or self.cfg.use_npu or\n                        self.cfg.use_mlu,\n                        custom_white_list=self.custom_white_list,\n                        custom_black_list=self.custom_black_list,\n                        level=self.amp_level):\n                    outs = self.model(data)\n            else:\n                outs = self.model(data)\n\n            shift_amount = data['st_pix']\n            outs['bbox'][:, 2:4] = outs['bbox'][:, 2:4] + shift_amount\n            outs['bbox'][:, 4:6] = outs['bbox'][:, 4:6] + shift_amount\n            merged_bboxs.append(outs['bbox'])\n\n            if data['is_last'] > 0:\n                # merge matching predictions\n                merged_results = {'bbox': []}\n                if combine_method == 'nms':\n                    final_boxes = multiclass_nms(\n                        np.concatenate(merged_bboxs), self.cfg.num_classes,\n                        match_threshold, match_metric)\n                    merged_results['bbox'] = np.concatenate(final_boxes)\n                elif combine_method == 'concat':\n                    merged_results['bbox'] = np.concatenate(merged_bboxs)\n                else:\n                    raise ValueError(\n                        \"Now only support 'nms' or 'concat' to fuse detection results.\"\n                    )\n                merged_results['im_id'] = np.array([[0]])\n                merged_results['bbox_num'] = np.array(\n                    [len(merged_results['bbox'])])\n\n                merged_bboxs = []\n                data['im_id'] = data['ori_im_id']\n                # update metrics\n                for metric in self._metrics:\n                    metric.update(data, merged_results)\n\n                # multi-scale inputs: all inputs have same im_id\n                if isinstance(data, typing.Sequence):\n                    sample_num += data[0]['im_id'].numpy().shape[0]\n                else:\n                    sample_num += data['im_id'].numpy().shape[0]\n\n            self._compose_callback.on_step_end(self.status)\n\n        self.status['sample_num'] = sample_num\n        self.status['cost_time'] = time.time() - tic\n\n        # accumulate metric to log out\n        for metric in self._metrics:\n            metric.accumulate()\n            metric.log()\n        self._compose_callback.on_epoch_end(self.status)\n        # reset metric states for metric may performed multiple times\n        self._reset_metrics()\n\n    def evaluate_slice(self,\n                       slice_size=[640, 640],\n                       overlap_ratio=[0.25, 0.25],\n                       combine_method='nms',\n                       match_threshold=0.6,\n                       match_metric='iou'):\n        with paddle.no_grad():\n            self._eval_with_loader_slice(self.loader, slice_size, overlap_ratio,\n                                         combine_method, match_threshold,\n                                         match_metric)\n\n    def slice_predict(self,\n                      images,\n                      slice_size=[640, 640],\n                      overlap_ratio=[0.25, 0.25],\n                      combine_method='nms',\n                      match_threshold=0.6,\n                      match_metric='iou',\n                      draw_threshold=0.5,\n                      output_dir='output',\n                      save_results=False,\n                      visualize=True):\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        self.dataset.set_slice_images(images, slice_size, overlap_ratio)\n        loader = create('TestReader')(self.dataset, 0)\n        imid2path = self.dataset.get_imid2path()\n\n        def setup_metrics_for_loader():\n            # mem\n            metrics = copy.deepcopy(self._metrics)\n            mode = self.mode\n            save_prediction_only = self.cfg[\n                'save_prediction_only'] if 'save_prediction_only' in self.cfg else None\n            output_eval = self.cfg[\n                'output_eval'] if 'output_eval' in self.cfg else None\n\n            # modify\n            self.mode = '_test'\n            self.cfg['save_prediction_only'] = True\n            self.cfg['output_eval'] = output_dir\n            self.cfg['imid2path'] = imid2path\n            self._init_metrics()\n\n            # restore\n            self.mode = mode\n            self.cfg.pop('save_prediction_only')\n            if save_prediction_only is not None:\n                self.cfg['save_prediction_only'] = save_prediction_only\n\n            self.cfg.pop('output_eval')\n            if output_eval is not None:\n                self.cfg['output_eval'] = output_eval\n\n            self.cfg.pop('imid2path')\n\n            _metrics = copy.deepcopy(self._metrics)\n            self._metrics = metrics\n\n            return _metrics\n\n        if save_results:\n            metrics = setup_metrics_for_loader()\n        else:\n            metrics = []\n\n        anno_file = self.dataset.get_anno()\n        clsid2catid, catid2name = get_categories(\n            self.cfg.metric, anno_file=anno_file)\n\n        # Run Infer\n        self.status['mode'] = 'test'\n        self.model.eval()\n        if self.cfg.get('print_flops', False):\n            flops_loader = create('TestReader')(self.dataset, 0)\n            self._flops(flops_loader)\n\n        results = []  # all images\n        merged_bboxs = []  # single image\n        for step_id, data in enumerate(tqdm(loader)):\n            self.status['step_id'] = step_id\n            # forward\n            outs = self.model(data)\n\n            outs['bbox'] = outs['bbox'].numpy()  # only in test mode\n            shift_amount = data['st_pix']\n            outs['bbox'][:, 2:4] = outs['bbox'][:, 2:4] + shift_amount.numpy()\n            outs['bbox'][:, 4:6] = outs['bbox'][:, 4:6] + shift_amount.numpy()\n            merged_bboxs.append(outs['bbox'])\n\n            if data['is_last'] > 0:\n                # merge matching predictions\n                merged_results = {'bbox': []}\n                if combine_method == 'nms':\n                    final_boxes = multiclass_nms(\n                        np.concatenate(merged_bboxs), self.cfg.num_classes,\n                        match_threshold, match_metric)\n                    merged_results['bbox'] = np.concatenate(final_boxes)\n                elif combine_method == 'concat':\n                    merged_results['bbox'] = np.concatenate(merged_bboxs)\n                else:\n                    raise ValueError(\n                        \"Now only support 'nms' or 'concat' to fuse detection results.\"\n                    )\n                merged_results['im_id'] = np.array([[0]])\n                merged_results['bbox_num'] = np.array(\n                    [len(merged_results['bbox'])])\n\n                merged_bboxs = []\n                data['im_id'] = data['ori_im_id']\n\n                for _m in metrics:\n                    _m.update(data, merged_results)\n\n                for key in ['im_shape', 'scale_factor', 'im_id']:\n                    if isinstance(data, typing.Sequence):\n                        merged_results[key] = data[0][key]\n                    else:\n                        merged_results[key] = data[key]\n                for key, value in merged_results.items():\n                    if hasattr(value, 'numpy'):\n                        merged_results[key] = value.numpy()\n                results.append(merged_results)\n\n        for _m in metrics:\n            _m.accumulate()\n            _m.reset()\n\n        if visualize:\n            for outs in results:\n                batch_res = get_infer_results(outs, clsid2catid)\n                bbox_num = outs['bbox_num']\n\n                start = 0\n                for i, im_id in enumerate(outs['im_id']):\n                    image_path = imid2path[int(im_id)]\n                    image = Image.open(image_path).convert('RGB')\n                    image = ImageOps.exif_transpose(image)\n                    self.status['original_image'] = np.array(image.copy())\n\n                    end = start + bbox_num[i]\n                    bbox_res = batch_res['bbox'][start:end] \\\n                            if 'bbox' in batch_res else None\n                    mask_res = batch_res['mask'][start:end] \\\n                            if 'mask' in batch_res else None\n                    segm_res = batch_res['segm'][start:end] \\\n                            if 'segm' in batch_res else None\n                    keypoint_res = batch_res['keypoint'][start:end] \\\n                            if 'keypoint' in batch_res else None\n                    pose3d_res = batch_res['pose3d'][start:end] \\\n                            if 'pose3d' in batch_res else None\n                    image = visualize_results(\n                        image, bbox_res, mask_res, segm_res, keypoint_res,\n                        pose3d_res, int(im_id), catid2name, draw_threshold)\n                    self.status['result_image'] = np.array(image.copy())\n                    if self._compose_callback:\n                        self._compose_callback.on_step_end(self.status)\n                    # save image with detection\n                    save_name = self._get_save_image_name(output_dir,\n                                                          image_path)\n                    logger.info(\"Detection bbox results save in {}\".format(\n                        save_name))\n                    image.save(save_name, quality=95)\n\n                    start = end\n\n    def predict(self,\n                images,\n                draw_threshold=0.5,\n                output_dir='output',\n                save_results=False,\n                visualize=True,\n                save_threshold=0,\n                do_eval=False):\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        if do_eval:\n            save_threshold = 0.0\n        self.dataset.set_images(images, do_eval=do_eval)\n        loader = create('TestReader')(self.dataset, 0)\n\n        imid2path = self.dataset.get_imid2path()\n\n        def setup_metrics_for_loader():\n            # mem\n            metrics = copy.deepcopy(self._metrics)\n            mode = self.mode\n            save_prediction_only = self.cfg[\n                'save_prediction_only'] if 'save_prediction_only' in self.cfg else None\n            output_eval = self.cfg[\n                'output_eval'] if 'output_eval' in self.cfg else None\n\n            # modify\n            self.mode = '_test'\n            self.cfg['save_prediction_only'] = True\n            self.cfg['output_eval'] = output_dir\n            self.cfg['imid2path'] = imid2path\n            self.cfg['save_threshold'] = save_threshold\n            self._init_metrics()\n\n            # restore\n            self.mode = mode\n            self.cfg.pop('save_prediction_only')\n            if save_prediction_only is not None:\n                self.cfg['save_prediction_only'] = save_prediction_only            \n\n            self.cfg.pop('output_eval')\n            if output_eval is not None:\n                self.cfg['output_eval'] = output_eval\n\n            self.cfg.pop('imid2path')\n\n            _metrics = copy.deepcopy(self._metrics)\n            self._metrics = metrics\n\n            return _metrics\n\n        if save_results:\n            metrics = setup_metrics_for_loader()\n        else:\n            metrics = []\n\n        anno_file = self.dataset.get_anno()\n        clsid2catid, catid2name = get_categories(\n            self.cfg.metric, anno_file=anno_file)\n\n        # Run Infer\n        self.status['mode'] = 'test'\n        self.model.eval()\n        if self.cfg.get('print_flops', False):\n            flops_loader = create('TestReader')(self.dataset, 0)\n            self._flops(flops_loader)\n        results = []\n        for step_id, data in enumerate(tqdm(loader)):\n            self.status['step_id'] = step_id\n            # forward\n            if hasattr(self.model, 'modelTeacher'):\n                outs = self.model.modelTeacher(data)\n            else:\n                outs = self.model(data)\n            for _m in metrics:\n                _m.update(data, outs)\n\n            for key in ['im_shape', 'scale_factor', 'im_id']:\n                if isinstance(data, typing.Sequence):\n                    outs[key] = data[0][key]\n                else:\n                    outs[key] = data[key]\n            for key, value in outs.items():\n                if hasattr(value, 'numpy'):\n                    outs[key] = value.numpy()\n            results.append(outs)\n\n        # sniper\n        if type(self.dataset) == SniperCOCODataSet:\n            results = self.dataset.anno_cropper.aggregate_chips_detections(\n                results)\n\n        for _m in metrics:\n            _m.accumulate()\n            _m.reset()\n\n        if visualize:\n            for outs in results:\n                batch_res = get_infer_results(outs, clsid2catid)\n                bbox_num = outs['bbox_num']\n\n                start = 0\n                for i, im_id in enumerate(outs['im_id']):\n                    image_path = imid2path[int(im_id)]\n                    image = Image.open(image_path).convert('RGB')\n                    image = ImageOps.exif_transpose(image)\n                    self.status['original_image'] = np.array(image.copy())\n\n                    end = start + bbox_num[i]\n                    bbox_res = batch_res['bbox'][start:end] \\\n                            if 'bbox' in batch_res else None\n                    mask_res = batch_res['mask'][start:end] \\\n                            if 'mask' in batch_res else None\n                    segm_res = batch_res['segm'][start:end] \\\n                            if 'segm' in batch_res else None\n                    keypoint_res = batch_res['keypoint'][start:end] \\\n                            if 'keypoint' in batch_res else None\n                    pose3d_res = batch_res['pose3d'][start:end] \\\n                            if 'pose3d' in batch_res else None\n                    image = visualize_results(\n                        image, bbox_res, mask_res, segm_res, keypoint_res,\n                        pose3d_res, int(im_id), catid2name, draw_threshold)\n                    self.status['result_image'] = np.array(image.copy())\n                    if self._compose_callback:\n                        self._compose_callback.on_step_end(self.status)\n                    # save image with detection\n                    save_name = self._get_save_image_name(output_dir,\n                                                          image_path)\n                    logger.info(\"Detection bbox results save in {}\".format(\n                        save_name))\n                    image.save(save_name, quality=95)\n\n                    start = end\n        return results\n\n    def _get_save_image_name(self, output_dir, image_path):\n        \"\"\"\n        Get save image name from source image path.\n        \"\"\"\n        image_name = os.path.split(image_path)[-1]\n        name, ext = os.path.splitext(image_name)\n        return os.path.join(output_dir, \"{}\".format(name)) + ext\n\n    def _model_to_static(self, model, input_spec, prune_input=True):\n        if prune_input:\n            static_model = paddle.jit.to_static(\n                model, input_spec=input_spec, full_graph=True)\n            # NOTE: dy2st do not pruned program, but jit.save will prune program\n            # input spec, prune input spec here and save with pruned input spec\n            pruned_input_spec = _prune_input_spec(\n                input_spec, static_model.forward.main_program,\n                static_model.forward.outputs)\n        else:\n            static_model = None\n            pruned_input_spec = input_spec\n        \n        return static_model, pruned_input_spec\n\n    def _get_infer_cfg_and_input_spec(self,\n                                      save_dir,\n                                      prune_input=True,\n                                      kl_quant=False,\n                                      yaml_name=None,\n                                      model=None):\n        if yaml_name is None:\n            yaml_name = 'infer_cfg.yml'\n        if model is None:\n            model = self.model\n        image_shape = None\n        im_shape = [None, 2]\n        scale_factor = [None, 2]\n        if self.cfg.architecture in MOT_ARCH:\n            test_reader_name = 'TestMOTReader'\n        else:\n            test_reader_name = 'TestReader'\n        if 'inputs_def' in self.cfg[test_reader_name]:\n            inputs_def = self.cfg[test_reader_name]['inputs_def']\n            image_shape = inputs_def.get('image_shape', None)\n        # set image_shape=[None, 3, -1, -1] as default\n        if image_shape is None:\n            image_shape = [None, 3, -1, -1]\n\n        if len(image_shape) == 3:\n            image_shape = [None] + image_shape\n        else:\n            im_shape = [image_shape[0], 2]\n            scale_factor = [image_shape[0], 2]\n\n        if hasattr(model, 'deploy'):\n            model.deploy = True\n        if 'slim' not in self.cfg:\n            for layer in model.sublayers():\n                if hasattr(layer, 'convert_to_deploy'):\n                    layer.convert_to_deploy()\n\n        if hasattr(self.cfg, 'export') and 'fuse_conv_bn' in self.cfg[\n                'export'] and self.cfg['export']['fuse_conv_bn']:\n            model = fuse_conv_bn(model)\n\n        export_post_process = self.cfg['export'].get(\n            'post_process', False) if hasattr(self.cfg, 'export') else True\n        export_nms = self.cfg['export'].get('nms', False) if hasattr(\n            self.cfg, 'export') else True\n        export_benchmark = self.cfg['export'].get(\n            'benchmark', False) if hasattr(self.cfg, 'export') else False\n        if hasattr(model, 'fuse_norm'):\n            model.fuse_norm = self.cfg['TestReader'].get('fuse_normalize',\n                                                              False)\n        if hasattr(model, 'export_post_process'):\n            model.export_post_process = export_post_process if not export_benchmark else False\n        if hasattr(model, 'export_nms'):\n            model.export_nms = export_nms if not export_benchmark else False\n        if export_post_process and not export_benchmark:\n            image_shape = [None] + image_shape[1:]\n\n        input_spec = [{\n            \"image\": InputSpec(\n                shape=image_shape, name='image'),\n            \"im_shape\": InputSpec(\n                shape=im_shape, name='im_shape'),\n            \"scale_factor\": InputSpec(\n                shape=scale_factor, name='scale_factor')\n        }]\n        if self.cfg.architecture == 'DeepSORT':\n            input_spec[0].update({\n                \"crops\": InputSpec(\n                    shape=[None, 3, 192, 64], name='crops')\n            })\n\n        if self.cfg.architecture == 'CLRNet':\n            input_spec[0].update({\n                \"full_img_path\": str,\n                \"img_name\": str,\n            })\n\n        static_model, pruned_input_spec = self._model_to_static(model, input_spec, prune_input)\n\n        # TODO: Hard code, delete it when support prune input_spec.\n        if self.cfg.architecture == 'PicoDet' and not export_post_process:\n            pruned_input_spec = [{\n                \"image\": InputSpec(\n                    shape=image_shape, name='image')\n            }]\n        if kl_quant:\n            if self.cfg.architecture == 'PicoDet' or 'ppyoloe' in self.cfg.weights:\n                pruned_input_spec = [{\n                    \"image\": InputSpec(\n                        shape=image_shape, name='image'),\n                    \"scale_factor\": InputSpec(\n                        shape=scale_factor, name='scale_factor')\n                }]\n            elif 'tinypose' in self.cfg.weights:\n                pruned_input_spec = [{\n                    \"image\": InputSpec(\n                        shape=image_shape, name='image')\n                }]\n        \n        # Save infer cfg\n        _dump_infer_config(self.cfg,\n                           os.path.join(save_dir, yaml_name), image_shape,\n                           model, input_spec)\n\n        return static_model, pruned_input_spec, input_spec\n\n    def export(self, output_dir='output_inference', for_fd=False):\n        if hasattr(self.model, 'aux_neck'):\n            self.model.__delattr__('aux_neck')\n        if hasattr(self.model, 'aux_head'):\n            self.model.__delattr__('aux_head')\n        self.model.eval()\n        model = copy.deepcopy(self.model)\n        convert_bn(model)\n\n        # add `export_mode` attr for all layers\n        for layer in model.sublayers(include_self=True):\n            layer.export_mode = True\n\n        model_name = os.path.splitext(os.path.split(self.cfg.filename)[-1])[0]\n        if for_fd:\n            save_dir = output_dir\n            save_name = 'inference'\n            yaml_name = 'inference.yml'\n        else:\n            save_dir = os.path.join(output_dir, model_name)\n            save_name = 'model'\n            yaml_name = None\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        static_model, pruned_input_spec, input_spec = self._get_infer_cfg_and_input_spec(\n            save_dir, yaml_name=yaml_name, model=model)\n\n        # dy2st and save model\n        if 'slim' not in self.cfg or 'QAT' not in self.cfg['slim_type']:\n            try:\n                import encryption\n            except ModuleNotFoundError:\n                logger.info(\"Skipping import of the encryption module.\")\n            paddle_version = version.parse(paddle.__version__)\n            if self.cfg.get(\"export_with_pir\", False):\n                assert (paddle_version >= version.parse(\n                        '3.0.0b2') or paddle_version == version.parse('0.0.0')) and os.environ.get(\"FLAGS_enable_pir_api\", None) not in [\"0\", \"False\"]\n                paddle.jit.save(static_model, os.path.join(save_dir, save_name), input_spec=pruned_input_spec)\n            else:\n                if paddle_version >= version.parse('3.0.0b2') or paddle_version == version.parse('0.0.0'):\n                    static_model.forward.rollback()\n                    with paddle.pir_utils.OldIrGuard():\n                        static_model, pruned_input_spec = self._model_to_static(\n                            model, input_spec)\n                        paddle.jit.save(static_model, os.path.join(save_dir, save_name), input_spec=pruned_input_spec)\n                else:\n                    paddle.jit.save(static_model, os.path.join(save_dir, save_name), input_spec=pruned_input_spec)\n        else:\n            self.cfg.slim.save_quantized_model(\n                self.model,\n                os.path.join(save_dir, save_name),\n                input_spec=pruned_input_spec)\n        logger.info(\"Export model and saved in {}\".format(save_dir))\n\n    def post_quant(self, output_dir='output_inference'):\n        model_name = os.path.splitext(os.path.split(self.cfg.filename)[-1])[0]\n        save_dir = os.path.join(output_dir, model_name)\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n        for idx, data in enumerate(self.loader):\n            self.model(data)\n            if idx == int(self.cfg.get('quant_batch_num', 10)):\n                break\n\n        # TODO: support prune input_spec\n        kl_quant = True if hasattr(self.cfg.slim, 'ptq') else False\n        _, pruned_input_spec = self._get_infer_cfg_and_input_spec(\n            save_dir, prune_input=False, kl_quant=kl_quant)\n\n        self.cfg.slim.save_quantized_model(\n            self.model,\n            os.path.join(save_dir, 'model'),\n            input_spec=pruned_input_spec)\n        logger.info(\"Export Post-Quant model and saved in {}\".format(save_dir))\n\n    def _flops(self, loader):\n        if hasattr(self.model, 'aux_neck'):\n            self.model.__delattr__('aux_neck')\n        if hasattr(self.model, 'aux_head'):\n            self.model.__delattr__('aux_head')\n        self.model.eval()\n        try:\n            import paddleslim\n        except Exception as e:\n            logger.warning(\n                'Unable to calculate flops, please install paddleslim, for example: `pip install paddleslim`'\n            )\n            return\n\n        from paddleslim.analysis import dygraph_flops as flops\n        input_data = None\n        for data in loader:\n            input_data = data\n            break\n\n        input_spec = [{\n            \"image\": input_data['image'][0].unsqueeze(0),\n            \"im_shape\": input_data['im_shape'][0].unsqueeze(0),\n            \"scale_factor\": input_data['scale_factor'][0].unsqueeze(0)\n        }]\n        flops = flops(self.model, input_spec) / (1000**3)\n        logger.info(\" Model FLOPs : {:.6f}G. (image shape is {})\".format(\n            flops, input_data['image'][0].unsqueeze(0).shape))\n\n    def parse_mot_images(self, cfg):\n        import glob\n        # for quant\n        dataset_dir = cfg['EvalMOTDataset'].dataset_dir\n        data_root = cfg['EvalMOTDataset'].data_root\n        data_root = '{}/{}'.format(dataset_dir, data_root)\n        seqs = os.listdir(data_root)\n        seqs.sort()\n        all_images = []\n        for seq in seqs:\n            infer_dir = os.path.join(data_root, seq)\n            assert infer_dir is None or os.path.isdir(infer_dir), \\\n                \"{} is not a directory\".format(infer_dir)\n            images = set()\n            exts = ['jpg', 'jpeg', 'png', 'bmp']\n            exts += [ext.upper() for ext in exts]\n            for ext in exts:\n                images.update(glob.glob('{}/*.{}'.format(infer_dir, ext)))\n            images = list(images)\n            images.sort()\n            assert len(images) > 0, \"no image found in {}\".format(infer_dir)\n            all_images.extend(images)\n            logger.info(\"Found {} inference images in total.\".format(\n                len(images)))\n        return all_images\n\n    def predict_culane(self,\n                       images,\n                       output_dir='output',\n                       save_results=False,\n                       visualize=True):\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        self.dataset.set_images(images)\n        loader = create('TestReader')(self.dataset, 0)\n\n        imid2path = self.dataset.get_imid2path()\n\n        def setup_metrics_for_loader():\n            # mem\n            metrics = copy.deepcopy(self._metrics)\n            mode = self.mode\n            save_prediction_only = self.cfg[\n                'save_prediction_only'] if 'save_prediction_only' in self.cfg else None\n            output_eval = self.cfg[\n                'output_eval'] if 'output_eval' in self.cfg else None\n\n            # modify\n            self.mode = '_test'\n            self.cfg['save_prediction_only'] = True\n            self.cfg['output_eval'] = output_dir\n            self.cfg['imid2path'] = imid2path\n            self._init_metrics()\n\n            # restore\n            self.mode = mode\n            self.cfg.pop('save_prediction_only')\n            if save_prediction_only is not None:\n                self.cfg['save_prediction_only'] = save_prediction_only\n\n            self.cfg.pop('output_eval')\n            if output_eval is not None:\n                self.cfg['output_eval'] = output_eval\n\n            self.cfg.pop('imid2path')\n\n            _metrics = copy.deepcopy(self._metrics)\n            self._metrics = metrics\n\n            return _metrics\n\n        if save_results:\n            metrics = setup_metrics_for_loader()\n        else:\n            metrics = []\n\n        # Run Infer\n        self.status['mode'] = 'test'\n        self.model.eval()\n        if self.cfg.get('print_flops', False):\n            flops_loader = create('TestReader')(self.dataset, 0)\n            self._flops(flops_loader)\n        results = []\n        for step_id, data in enumerate(tqdm(loader)):\n            self.status['step_id'] = step_id\n            # forward\n            outs = self.model(data)\n\n            for _m in metrics:\n                _m.update(data, outs)\n\n            for key in ['im_shape', 'scale_factor', 'im_id']:\n                if isinstance(data, typing.Sequence):\n                    outs[key] = data[0][key]\n                else:\n                    outs[key] = data[key]\n            for key, value in outs.items():\n                if hasattr(value, 'numpy'):\n                    outs[key] = value.numpy()\n            results.append(outs)\n\n        for _m in metrics:\n            _m.accumulate()\n            _m.reset()\n\n        if visualize:\n            import cv2\n\n            for outs in results:\n                for i in range(len(outs['img_path'])):\n                    lanes = outs['lanes'][i]\n                    img_path = outs['img_path'][i]\n                    img = cv2.imread(img_path)\n                    out_file = os.path.join(output_dir,\n                                            os.path.basename(img_path))\n                    lanes = [\n                        lane.to_array(\n                            sample_y_range=[\n                                self.cfg['sample_y']['start'],\n                                self.cfg['sample_y']['end'],\n                                self.cfg['sample_y']['step']\n                            ],\n                            img_w=self.cfg.ori_img_w,\n                            img_h=self.cfg.ori_img_h) for lane in lanes\n                    ]\n                    imshow_lanes(img, lanes, out_file=out_file)\n\n        return results\n\n    def reset_norm_param_attr(self, layer, **kwargs):\n        if isinstance(layer, (nn.BatchNorm2D, nn.LayerNorm, nn.GroupNorm)):\n            src_state_dict = layer.state_dict()\n            if isinstance(layer, nn.BatchNorm2D):\n                layer = nn.BatchNorm2D(\n                    num_features=layer._num_features,\n                    momentum=layer._momentum,\n                    epsilon=layer._epsilon,\n                    **kwargs)\n            elif isinstance(layer, nn.LayerNorm):\n                layer = nn.LayerNorm(\n                    normalized_shape=layer._normalized_shape,\n                    epsilon=layer._epsilon,\n                    **kwargs)\n            else:\n                layer = nn.GroupNorm(\n                    num_groups=layer._num_groups,\n                    num_channels=layer._num_channels,\n                    epsilon=layer._epsilon,\n                    **kwargs)\n            layer.set_state_dict(src_state_dict)\n        else:\n            for name, sublayer in layer.named_children():\n                new_sublayer = self.reset_norm_param_attr(sublayer, **kwargs)\n                if new_sublayer is not sublayer:\n                    setattr(layer, name, new_sublayer)\n\n        return layer\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}