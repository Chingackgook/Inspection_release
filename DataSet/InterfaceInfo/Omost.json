{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/Omost",
    "API_Calls": [
        {
            "Name": "call_StableDiffusionXLOmostPipeline",
            "Description": "gradio_app of StableDiffusionXLOmostPipeline",
            "Code": "import os\n\nos.environ['HF_HOME'] = os.path.join(os.path.dirname(__file__), 'hf_download')\nHF_TOKEN = None\n\nimport lib_omost.memory_management as memory_management\nimport uuid\n\nimport torch\nimport numpy as np\nimport gradio as gr\nimport tempfile\n\ngradio_temp_dir = os.path.join(tempfile.gettempdir(), 'gradio')\nos.makedirs(gradio_temp_dir, exist_ok=True)\n\nfrom threading import Thread\n\n# Phi3 Hijack\nfrom transformers.models.phi3.modeling_phi3 import Phi3PreTrainedModel\n\nPhi3PreTrainedModel._supports_sdpa = True\n\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers.models.attention_processor import AttnProcessor2_0\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom lib_omost.pipeline import StableDiffusionXLOmostPipeline\nfrom chat_interface import ChatInterface\nfrom transformers.generation.stopping_criteria import StoppingCriteriaList\n\nimport lib_omost.canvas as omost_canvas\n\n\n# SDXL\n\nsdxl_name = 'SG161222/RealVisXL_V4.0'\n# sdxl_name = 'stabilityai/stable-diffusion-xl-base-1.0'\n\ntokenizer = CLIPTokenizer.from_pretrained(\n    sdxl_name, subfolder=\"tokenizer\")\ntokenizer_2 = CLIPTokenizer.from_pretrained(\n    sdxl_name, subfolder=\"tokenizer_2\")\ntext_encoder = CLIPTextModel.from_pretrained(\n    sdxl_name, subfolder=\"text_encoder\", torch_dtype=torch.float16, variant=\"fp16\")\ntext_encoder_2 = CLIPTextModel.from_pretrained(\n    sdxl_name, subfolder=\"text_encoder_2\", torch_dtype=torch.float16, variant=\"fp16\")\nvae = AutoencoderKL.from_pretrained(\n    sdxl_name, subfolder=\"vae\", torch_dtype=torch.bfloat16, variant=\"fp16\")  # bfloat16 vae\nunet = UNet2DConditionModel.from_pretrained(\n    sdxl_name, subfolder=\"unet\", torch_dtype=torch.float16, variant=\"fp16\")\n\nunet.set_attn_processor(AttnProcessor2_0())\nvae.set_attn_processor(AttnProcessor2_0())\n\npipeline = StableDiffusionXLOmostPipeline(\n    vae=vae,\n    text_encoder=text_encoder,\n    tokenizer=tokenizer,\n    text_encoder_2=text_encoder_2,\n    tokenizer_2=tokenizer_2,\n    unet=unet,\n    scheduler=None,  # We completely give up diffusers sampling system and use A1111's method\n)\n\nmemory_management.unload_all_models([text_encoder, text_encoder_2, vae, unet])\n\n# LLM\n\n# llm_name = 'lllyasviel/omost-phi-3-mini-128k-8bits'\nllm_name = 'lllyasviel/omost-llama-3-8b-4bits'\n# llm_name = 'lllyasviel/omost-dolphin-2.9-llama3-8b-4bits'\n\nllm_model = AutoModelForCausalLM.from_pretrained(\n    llm_name,\n    torch_dtype=torch.bfloat16,  # This is computation type, not load/memory type. The loading quant type is baked in config.\n    token=HF_TOKEN,\n    device_map=\"auto\"  # This will load model to gpu with an offload system\n)\n\nllm_tokenizer = AutoTokenizer.from_pretrained(\n    llm_name,\n    token=HF_TOKEN\n)\n\nmemory_management.unload_all_models(llm_model)\n\n\n@torch.inference_mode()\ndef pytorch2numpy(imgs):\n    results = []\n    for x in imgs:\n        y = x.movedim(0, -1)\n        y = y * 127.5 + 127.5\n        y = y.detach().float().cpu().numpy().clip(0, 255).astype(np.uint8)\n        results.append(y)\n    return results\n\n\n@torch.inference_mode()\ndef numpy2pytorch(imgs):\n    h = torch.from_numpy(np.stack(imgs, axis=0)).float() / 127.5 - 1.0\n    h = h.movedim(-1, 1)\n    return h\n\n\ndef resize_without_crop(image, target_width, target_height):\n    pil_image = Image.fromarray(image)\n    resized_image = pil_image.resize((target_width, target_height), Image.LANCZOS)\n    return np.array(resized_image)\n\n\n@torch.inference_mode()\ndef chat_fn(message: str, history: list, seed:int, temperature: float, top_p: float, max_new_tokens: int) -> str:\n    np.random.seed(int(seed))\n    torch.manual_seed(int(seed))\n\n    conversation = [{\"role\": \"system\", \"content\": omost_canvas.system_prompt}]\n\n    for user, assistant in history:\n        if isinstance(user, str) and isinstance(assistant, str):\n            if len(user) > 0 and len(assistant) > 0:\n                conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n\n    conversation.append({\"role\": \"user\", \"content\": message})\n\n    memory_management.load_models_to_gpu(llm_model)\n\n    input_ids = llm_tokenizer.apply_chat_template(\n        conversation, return_tensors=\"pt\", add_generation_prompt=True).to(llm_model.device)\n\n    streamer = TextIteratorStreamer(llm_tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n\n    def interactive_stopping_criteria(*args, **kwargs) -> bool:\n        if getattr(streamer, 'user_interrupted', False):\n            print('User stopped generation')\n            return True\n        else:\n            return False\n\n    stopping_criteria = StoppingCriteriaList([interactive_stopping_criteria])\n\n    def interrupter():\n        streamer.user_interrupted = True\n        return\n\n    generate_kwargs = dict(\n        input_ids=input_ids,\n        streamer=streamer,\n        stopping_criteria=stopping_criteria,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=top_p,\n    )\n\n    if temperature == 0:\n        generate_kwargs['do_sample'] = False\n\n    Thread(target=llm_model.generate, kwargs=generate_kwargs).start()\n\n    outputs = []\n    for text in streamer:\n        outputs.append(text)\n        # print(outputs)\n        yield \"\".join(outputs), interrupter\n\n    return\n\n\n@torch.inference_mode()\ndef post_chat(history):\n    canvas_outputs = None\n\n    try:\n        if history:\n            history = [(user, assistant) for user, assistant in history if isinstance(user, str) and isinstance(assistant, str)]\n            last_assistant = history[-1][1] if len(history) > 0 else None\n            canvas = omost_canvas.Canvas.from_bot_response(last_assistant)\n            canvas_outputs = canvas.process()\n    except Exception as e:\n        print('Last assistant response is not valid canvas:', e)\n\n    return canvas_outputs, gr.update(visible=canvas_outputs is not None), gr.update(interactive=len(history) > 0)\n\n\n@torch.inference_mode()\ndef diffusion_fn(chatbot, canvas_outputs, num_samples, seed, image_width, image_height,\n                 highres_scale, steps, cfg, highres_steps, highres_denoise, negative_prompt):\n\n    use_initial_latent = False\n    eps = 0.05\n\n    image_width, image_height = int(image_width // 64) * 64, int(image_height // 64) * 64\n\n    rng = torch.Generator(device=memory_management.gpu).manual_seed(seed)\n\n    memory_management.load_models_to_gpu([text_encoder, text_encoder_2])\n\n    positive_cond, positive_pooler, negative_cond, negative_pooler = pipeline.all_conds_from_canvas(canvas_outputs, negative_prompt)\n\n    if use_initial_latent:\n        memory_management.load_models_to_gpu([vae])\n        initial_latent = torch.from_numpy(canvas_outputs['initial_latent'])[None].movedim(-1, 1) / 127.5 - 1.0\n        initial_latent_blur = 40\n        initial_latent = torch.nn.functional.avg_pool2d(\n            torch.nn.functional.pad(initial_latent, (initial_latent_blur,) * 4, mode='reflect'),\n            kernel_size=(initial_latent_blur * 2 + 1,) * 2, stride=(1, 1))\n        initial_latent = torch.nn.functional.interpolate(initial_latent, (image_height, image_width))\n        initial_latent = initial_latent.to(dtype=vae.dtype, device=vae.device)\n        initial_latent = vae.encode(initial_latent).latent_dist.mode() * vae.config.scaling_factor\n    else:\n        initial_latent = torch.zeros(size=(num_samples, 4, image_height // 8, image_width // 8), dtype=torch.float32)\n\n    memory_management.load_models_to_gpu([unet])\n\n    initial_latent = initial_latent.to(dtype=unet.dtype, device=unet.device)\n\n    latents = pipeline(\n        initial_latent=initial_latent,\n        strength=1.0,\n        num_inference_steps=int(steps),\n        batch_size=num_samples,\n        prompt_embeds=positive_cond,\n        negative_prompt_embeds=negative_cond,\n        pooled_prompt_embeds=positive_pooler,\n        negative_pooled_prompt_embeds=negative_pooler,\n        generator=rng,\n        guidance_scale=float(cfg),\n    ).images\n\n    memory_management.load_models_to_gpu([vae])\n    latents = latents.to(dtype=vae.dtype, device=vae.device) / vae.config.scaling_factor\n    pixels = vae.decode(latents).sample\n    B, C, H, W = pixels.shape\n    pixels = pytorch2numpy(pixels)\n\n    if highres_scale > 1.0 + eps:\n        pixels = [\n            resize_without_crop(\n                image=p,\n                target_width=int(round(W * highres_scale / 64.0) * 64),\n                target_height=int(round(H * highres_scale / 64.0) * 64)\n            ) for p in pixels\n        ]\n\n        pixels = numpy2pytorch(pixels).to(device=vae.device, dtype=vae.dtype)\n        latents = vae.encode(pixels).latent_dist.mode() * vae.config.scaling_factor\n\n        memory_management.load_models_to_gpu([unet])\n        latents = latents.to(device=unet.device, dtype=unet.dtype)\n\n        latents = pipeline(\n            initial_latent=latents,\n            strength=highres_denoise,\n            num_inference_steps=highres_steps,\n            batch_size=num_samples,\n            prompt_embeds=positive_cond,\n            negative_prompt_embeds=negative_cond,\n            pooled_prompt_embeds=positive_pooler,\n            negative_pooled_prompt_embeds=negative_pooler,\n            generator=rng,\n            guidance_scale=float(cfg),\n        ).images\n\n        memory_management.load_models_to_gpu([vae])\n        latents = latents.to(dtype=vae.dtype, device=vae.device) / vae.config.scaling_factor\n        pixels = vae.decode(latents).sample\n        pixels = pytorch2numpy(pixels)\n\n    for i in range(len(pixels)):\n        unique_hex = uuid.uuid4().hex\n        image_path = os.path.join(gradio_temp_dir, f\"{unique_hex}_{i}.png\")\n        image = Image.fromarray(pixels[i])\n        image.save(image_path)\n        chatbot = chatbot + [(None, (image_path, 'image'))]\n\n    return chatbot\n\n\ncss = '''\ncode {white-space: pre-wrap !important;}\n.gradio-container {max-width: none !important;}\n.outer_parent {flex: 1;}\n.inner_parent {flex: 1;}\nfooter {display: none !important; visibility: hidden !important;}\n.translucent {display: none !important; visibility: hidden !important;}\n'''\n\nfrom gradio.themes.utils import colors\n\nwith gr.Blocks(\n        fill_height=True, css=css,\n        theme=gr.themes.Default(primary_hue=colors.blue, secondary_hue=colors.cyan, neutral_hue=colors.gray)\n) as demo:\n    with gr.Row(elem_classes='outer_parent'):\n        with gr.Column(scale=25):\n            with gr.Row():\n                clear_btn = gr.Button(\"➕ New Chat\", variant=\"secondary\", size=\"sm\", min_width=60)\n                retry_btn = gr.Button(\"Retry\", variant=\"secondary\", size=\"sm\", min_width=60, visible=False)\n                undo_btn = gr.Button(\"✏️️ Edit Last Input\", variant=\"secondary\", size=\"sm\", min_width=60, interactive=False)\n\n            seed = gr.Number(label=\"Random Seed\", value=12345, precision=0)\n\n            with gr.Accordion(open=True, label='Language Model'):\n                with gr.Group():\n                    with gr.Row():\n                        temperature = gr.Slider(\n                            minimum=0.0,\n                            maximum=2.0,\n                            step=0.01,\n                            value=0.6,\n                            label=\"Temperature\")\n                        top_p = gr.Slider(\n                            minimum=0.0,\n                            maximum=1.0,\n                            step=0.01,\n                            value=0.9,\n                            label=\"Top P\")\n                    max_new_tokens = gr.Slider(\n                        minimum=128,\n                        maximum=4096,\n                        step=1,\n                        value=4096,\n                        label=\"Max New Tokens\")\n            with gr.Accordion(open=True, label='Image Diffusion Model'):\n                with gr.Group():\n                    with gr.Row():\n                        image_width = gr.Slider(label=\"Image Width\", minimum=256, maximum=2048, value=896, step=64)\n                        image_height = gr.Slider(label=\"Image Height\", minimum=256, maximum=2048, value=1152, step=64)\n\n                    with gr.Row():\n                        num_samples = gr.Slider(label=\"Image Number\", minimum=1, maximum=12, value=1, step=1)\n                        steps = gr.Slider(label=\"Sampling Steps\", minimum=1, maximum=100, value=25, step=1)\n\n            with gr.Accordion(open=False, label='Advanced'):\n                cfg = gr.Slider(label=\"CFG Scale\", minimum=1.0, maximum=32.0, value=5.0, step=0.01)\n                highres_scale = gr.Slider(label=\"HR-fix Scale (\\\"1\\\" is disabled)\", minimum=1.0, maximum=2.0, value=1.0, step=0.01)\n                highres_steps = gr.Slider(label=\"Highres Fix Steps\", minimum=1, maximum=100, value=20, step=1)\n                highres_denoise = gr.Slider(label=\"Highres Fix Denoise\", minimum=0.1, maximum=1.0, value=0.4, step=0.01)\n                n_prompt = gr.Textbox(label=\"Negative Prompt\", value='lowres, bad anatomy, bad hands, cropped, worst quality')\n\n            render_button = gr.Button(\"Render the Image!\", size='lg', variant=\"primary\", visible=False)\n\n            examples = gr.Dataset(\n                samples=[\n                    ['generate an image of the fierce battle of warriors and a dragon'],\n                    ['change the dragon to a dinosaur']\n                ],\n                components=[gr.Textbox(visible=False)],\n                label='Quick Prompts'\n            )\n        with gr.Column(scale=75, elem_classes='inner_parent'):\n            canvas_state = gr.State(None)\n            chatbot = gr.Chatbot(label='Omost', scale=1, show_copy_button=True, layout=\"panel\", render=False)\n            chatInterface = ChatInterface(\n                fn=chat_fn,\n                post_fn=post_chat,\n                post_fn_kwargs=dict(inputs=[chatbot], outputs=[canvas_state, render_button, undo_btn]),\n                pre_fn=lambda: gr.update(visible=False),\n                pre_fn_kwargs=dict(outputs=[render_button]),\n                chatbot=chatbot,\n                retry_btn=retry_btn,\n                undo_btn=undo_btn,\n                clear_btn=clear_btn,\n                additional_inputs=[seed, temperature, top_p, max_new_tokens],\n                examples=examples\n            )\n\n    render_button.click(\n        fn=diffusion_fn, inputs=[\n            chatInterface.chatbot, canvas_state,\n            num_samples, seed, image_width, image_height, highres_scale,\n            steps, cfg, highres_steps, highres_denoise, n_prompt\n        ], outputs=[chatInterface.chatbot]).then(\n        fn=lambda x: x, inputs=[\n            chatInterface.chatbot\n        ], outputs=[chatInterface.chatbot_state])\n\nif __name__ == \"__main__\":\n    demo.queue().launch(inbrowser=True, server_name='0.0.0.0')\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Omost/gradio_app.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "StableDiffusionXLOmostPipeline",
            "Description": "StableDiffusionXLOmostPipeline impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Omost/lib_omost/pipeline.py",
            "Implementation": "import numpy as np\nimport copy\n\nfrom tqdm.auto import trange\nfrom diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl_img2img import *\nfrom diffusers.models.transformers import Transformer2DModel\n\n\nclass StableDiffusionXLOmostPipeline(StableDiffusionXLImg2ImgPipeline):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.k_model = KModel(unet=self.unet)\n\n        attn_procs = {}\n        for name in self.unet.attn_processors.keys():\n            if name.endswith(\"attn2.processor\"):\n                attn_procs[name] = OmostCrossAttnProcessor()\n            else:\n                attn_procs[name] = OmostSelfAttnProcessor()\n\n        self.unet.set_attn_processor(attn_procs)\n        return\n\n    @torch.inference_mode()\n    def encode_bag_of_subprompts_greedy(self, prefixes: list[str], suffixes: list[str]):\n        device = self.text_encoder.device\n\n        @torch.inference_mode()\n        def greedy_partition(items, max_sum):\n            bags = []\n            current_bag = []\n            current_sum = 0\n\n            for item in items:\n                num = item['length']\n                if current_sum + num > max_sum:\n                    if current_bag:\n                        bags.append(current_bag)\n                    current_bag = [item]\n                    current_sum = num\n                else:\n                    current_bag.append(item)\n                    current_sum += num\n\n            if current_bag:\n                bags.append(current_bag)\n\n            return bags\n\n        @torch.inference_mode()\n        def get_77_tokens_in_torch(subprompt_inds, tokenizer):\n            # Note that all subprompt are theoretically less than 75 tokens (without bos/eos)\n            result = [tokenizer.bos_token_id] + subprompt_inds[:75] + [tokenizer.eos_token_id] + [tokenizer.pad_token_id] * 75\n            result = result[:77]\n            result = torch.tensor([result]).to(device=device, dtype=torch.int64)\n            return result\n\n        @torch.inference_mode()\n        def merge_with_prefix(bag):\n            merged_ids_t1 = copy.deepcopy(prefix_ids_t1)\n            merged_ids_t2 = copy.deepcopy(prefix_ids_t2)\n\n            for item in bag:\n                merged_ids_t1.extend(item['ids_t1'])\n                merged_ids_t2.extend(item['ids_t2'])\n\n            return dict(\n                ids_t1=get_77_tokens_in_torch(merged_ids_t1, self.tokenizer),\n                ids_t2=get_77_tokens_in_torch(merged_ids_t2, self.tokenizer_2)\n            )\n\n        @torch.inference_mode()\n        def double_encode(pair_of_inds):\n            inds = [pair_of_inds['ids_t1'], pair_of_inds['ids_t2']]\n            text_encoders = [self.text_encoder, self.text_encoder_2]\n\n            pooled_prompt_embeds = None\n            prompt_embeds_list = []\n\n            for text_input_ids, text_encoder in zip(inds, text_encoders):\n                prompt_embeds = text_encoder(text_input_ids, output_hidden_states=True)\n\n                # Only last pooler_output is needed\n                pooled_prompt_embeds = prompt_embeds.pooler_output\n\n                # \"2\" because SDXL always indexes from the penultimate layer.\n                prompt_embeds = prompt_embeds.hidden_states[-2]\n                prompt_embeds_list.append(prompt_embeds)\n\n            prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n            return prompt_embeds, pooled_prompt_embeds\n\n        # Begin with tokenizing prefixes\n\n        prefix_length = 0\n        prefix_ids_t1 = []\n        prefix_ids_t2 = []\n\n        for prefix in prefixes:\n            ids_t1 = self.tokenizer(prefix, truncation=False, add_special_tokens=False).input_ids\n            ids_t2 = self.tokenizer_2(prefix, truncation=False, add_special_tokens=False).input_ids\n            assert len(ids_t1) == len(ids_t2)\n            prefix_length += len(ids_t1)\n            prefix_ids_t1 += ids_t1\n            prefix_ids_t2 += ids_t2\n\n        # Then tokenizing suffixes\n\n        allowed_suffix_length = 75 - prefix_length\n        suffix_targets = []\n\n        for subprompt in suffixes:\n            # Note that all subprompt are theoretically less than 75 tokens (without bos/eos)\n            # So we can safely just crop it to 75\n            ids_t1 = self.tokenizer(subprompt, truncation=False, add_special_tokens=False).input_ids[:75]\n            ids_t2 = self.tokenizer_2(subprompt, truncation=False, add_special_tokens=False).input_ids[:75]\n            assert len(ids_t1) == len(ids_t2)\n            suffix_targets.append(dict(\n                length=len(ids_t1),\n                ids_t1=ids_t1,\n                ids_t2=ids_t2\n            ))\n\n        # Then merge prefix and suffix tokens\n\n        suffix_targets = greedy_partition(suffix_targets, max_sum=allowed_suffix_length)\n        targets = [merge_with_prefix(b) for b in suffix_targets]\n\n        # Encode!\n\n        conds, poolers = [], []\n\n        for target in targets:\n            cond, pooler = double_encode(target)\n            conds.append(cond)\n            poolers.append(pooler)\n\n        conds_merged = torch.concat(conds, dim=1)\n        poolers_merged = poolers[0]\n\n        return dict(cond=conds_merged, pooler=poolers_merged)\n\n    @torch.inference_mode()\n    def all_conds_from_canvas(self, canvas_outputs, negative_prompt):\n        mask_all = torch.ones(size=(90, 90), dtype=torch.float32)\n        negative_cond, negative_pooler = self.encode_cropped_prompt_77tokens(negative_prompt)\n        negative_result = [(mask_all, negative_cond)]\n\n        positive_result = []\n        positive_pooler = None\n\n        for item in canvas_outputs['bag_of_conditions']:\n            current_mask = torch.from_numpy(item['mask']).to(torch.float32)\n            current_prefixes = item['prefixes']\n            current_suffixes = item['suffixes']\n\n            current_cond = self.encode_bag_of_subprompts_greedy(prefixes=current_prefixes, suffixes=current_suffixes)\n\n            if positive_pooler is None:\n                positive_pooler = current_cond['pooler']\n\n            positive_result.append((current_mask, current_cond['cond']))\n\n        return positive_result, positive_pooler, negative_result, negative_pooler\n\n    @torch.inference_mode()\n    def encode_cropped_prompt_77tokens(self, prompt: str):\n        device = self.text_encoder.device\n        tokenizers = [self.tokenizer, self.tokenizer_2]\n        text_encoders = [self.text_encoder, self.text_encoder_2]\n\n        pooled_prompt_embeds = None\n        prompt_embeds_list = []\n\n        for tokenizer, text_encoder in zip(tokenizers, text_encoders):\n            text_input_ids = tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=tokenizer.model_max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            ).input_ids\n\n            prompt_embeds = text_encoder(text_input_ids.to(device), output_hidden_states=True)\n\n            # Only last pooler_output is needed\n            pooled_prompt_embeds = prompt_embeds.pooler_output\n\n            # \"2\" because SDXL always indexes from the penultimate layer.\n            prompt_embeds = prompt_embeds.hidden_states[-2]\n            prompt_embeds_list.append(prompt_embeds)\n\n        prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n        prompt_embeds = prompt_embeds.to(dtype=self.unet.dtype, device=device)\n\n        return prompt_embeds, pooled_prompt_embeds\n\n    @torch.inference_mode()\n    def __call__(\n            self,\n            initial_latent: torch.FloatTensor = None,\n            strength: float = 1.0,\n            num_inference_steps: int = 25,\n            guidance_scale: float = 5.0,\n            batch_size: Optional[int] = 1,\n            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n            prompt_embeds: Optional[torch.FloatTensor] = None,\n            negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n            pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n            negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n            cross_attention_kwargs: Optional[dict] = None,\n    ):\n\n        device = self.unet.device\n        cross_attention_kwargs = cross_attention_kwargs or {}\n\n        # Sigmas\n\n        sigmas = self.k_model.get_sigmas_karras(int(num_inference_steps / strength))\n        sigmas = sigmas[-(num_inference_steps + 1):].to(device)\n\n        # Initial latents\n\n        _, C, H, W = initial_latent.shape\n        noise = randn_tensor((batch_size, C, H, W), generator=generator, device=device, dtype=self.unet.dtype)\n        latents = initial_latent.to(noise) + noise * sigmas[0].to(noise)\n\n        # Shape\n\n        height, width = latents.shape[-2:]\n        height = height * self.vae_scale_factor\n        width = width * self.vae_scale_factor\n\n        add_time_ids = list((height, width) + (0, 0) + (height, width))\n        add_time_ids = torch.tensor([add_time_ids], dtype=self.unet.dtype)\n        add_neg_time_ids = add_time_ids.clone()\n\n        # Batch\n\n        latents = latents.to(device)\n        add_time_ids = add_time_ids.repeat(batch_size, 1).to(device)\n        add_neg_time_ids = add_neg_time_ids.repeat(batch_size, 1).to(device)\n        prompt_embeds = [(k.to(device), v.repeat(batch_size, 1, 1).to(noise)) for k, v in prompt_embeds]\n        negative_prompt_embeds = [(k.to(device), v.repeat(batch_size, 1, 1).to(noise)) for k, v in negative_prompt_embeds]\n        pooled_prompt_embeds = pooled_prompt_embeds.repeat(batch_size, 1).to(noise)\n        negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.repeat(batch_size, 1).to(noise)\n\n        # Feeds\n\n        sampler_kwargs = dict(\n            cfg_scale=guidance_scale,\n            positive=dict(\n                encoder_hidden_states=prompt_embeds,\n                added_cond_kwargs={\"text_embeds\": pooled_prompt_embeds, \"time_ids\": add_time_ids},\n                cross_attention_kwargs=cross_attention_kwargs\n            ),\n            negative=dict(\n                encoder_hidden_states=negative_prompt_embeds,\n                added_cond_kwargs={\"text_embeds\": negative_pooled_prompt_embeds, \"time_ids\": add_neg_time_ids},\n                cross_attention_kwargs=cross_attention_kwargs\n            )\n        )\n\n        # Sample\n\n        results = sample_dpmpp_2m(self.k_model, latents, sigmas, extra_args=sampler_kwargs, disable=False)\n\n        return StableDiffusionXLPipelineOutput(images=results)\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}