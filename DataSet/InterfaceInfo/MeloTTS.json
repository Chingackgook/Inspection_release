{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/MeloTTS",
    "API_Calls": [
        {
            "Name": "call_TTS",
            "Description": "call_TTS",
            "Code": "import click\nimport warnings\nimport os\n\n\n@click.command\n@click.argument('text')\n@click.argument('output_path')\n@click.option(\"--file\", '-f', is_flag=True, show_default=True, default=False, help=\"Text is a file\")\n@click.option('--language', '-l', default='EN', help='Language, defaults to English', type=click.Choice(['EN', 'ES', 'FR', 'ZH', 'JP', 'KR'], case_sensitive=False))\n@click.option('--speaker', '-spk', default='EN-Default', help='Speaker ID, only for English, leave empty for default, ignored if not English. If English, defaults to \"EN-Default\"', type=click.Choice(['EN-Default', 'EN-US', 'EN-BR', 'EN_INDIA', 'EN-AU']))\n@click.option('--speed', '-s', default=1.0, help='Speed, defaults to 1.0', type=float)\n@click.option('--device', '-d', default='auto', help='Device, defaults to auto')\ndef main(text, file, output_path, language, speaker, speed, device):\n    if file:\n        if not os.path.exists(text):\n            raise FileNotFoundError(f'Trying to load text from file due to --file/-f flag, but file not found. Remove the --file/-f flag to pass a string.')\n        else:\n            with open(text) as f:\n                text = f.read().strip()\n    if text == '':\n        raise ValueError('You entered empty text or the file you passed was empty.')\n    language = language.upper()\n    if language == '': language = 'EN'\n    if speaker == '': speaker = None\n    if (not language == 'EN') and speaker:\n        warnings.warn('You specified a speaker but the language is English.')\n    from melo.api import TTS\n    model = TTS(language=language, device=device)\n    speaker_ids = model.hps.data.spk2id\n    if language == 'EN':\n        if not speaker: speaker = 'EN-Default'\n        spkr = speaker_ids[speaker]\n    else:\n        spkr = speaker_ids[list(speaker_ids.keys())[0]]\n    model.tts_to_file(text, spkr, output_path, speed=speed)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/MeloTTS/melo/main.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "TTS",
            "Description": "TTS impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/MeloTTS/melo/api.py",
            "Implementation": "import os\nimport re\nimport json\nimport torch\nimport librosa\nimport soundfile\nimport torchaudio\nimport numpy as np\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport torch\n\nfrom . import utils\nfrom . import commons\nfrom .models import SynthesizerTrn\nfrom .split_utils import split_sentence\nfrom .mel_processing import spectrogram_torch, spectrogram_torch_conv\nfrom .download_utils import load_or_download_config, load_or_download_model\n\nclass TTS(nn.Module):\n    def __init__(self, \n                language,\n                device='auto',\n                use_hf=True,\n                config_path=None,\n                ckpt_path=None):\n        super().__init__()\n        if device == 'auto':\n            device = 'cpu'\n            if torch.cuda.is_available(): device = 'cuda'\n            if torch.backends.mps.is_available(): device = 'mps'\n        if 'cuda' in device:\n            assert torch.cuda.is_available()\n\n        # config_path = \n        hps = load_or_download_config(language, use_hf=use_hf, config_path=config_path)\n\n        num_languages = hps.num_languages\n        num_tones = hps.num_tones\n        symbols = hps.symbols\n\n        model = SynthesizerTrn(\n            len(symbols),\n            hps.data.filter_length // 2 + 1,\n            hps.train.segment_size // hps.data.hop_length,\n            n_speakers=hps.data.n_speakers,\n            num_tones=num_tones,\n            num_languages=num_languages,\n            **hps.model,\n        ).to(device)\n\n        model.eval()\n        self.model = model\n        self.symbol_to_id = {s: i for i, s in enumerate(symbols)}\n        self.hps = hps\n        self.device = device\n    \n        # load state_dict\n        checkpoint_dict = load_or_download_model(language, device, use_hf=use_hf, ckpt_path=ckpt_path)\n        self.model.load_state_dict(checkpoint_dict['model'], strict=True)\n        \n        language = language.split('_')[0]\n        self.language = 'ZH_MIX_EN' if language == 'ZH' else language # we support a ZH_MIX_EN model\n\n    @staticmethod\n    def audio_numpy_concat(segment_data_list, sr, speed=1.):\n        audio_segments = []\n        for segment_data in segment_data_list:\n            audio_segments += segment_data.reshape(-1).tolist()\n            audio_segments += [0] * int((sr * 0.05) / speed)\n        audio_segments = np.array(audio_segments).astype(np.float32)\n        return audio_segments\n\n    @staticmethod\n    def split_sentences_into_pieces(text, language, quiet=False):\n        texts = split_sentence(text, language_str=language)\n        if not quiet:\n            print(\" > Text split to sentences.\")\n            print('\\n'.join(texts))\n            print(\" > ===========================\")\n        return texts\n\n    def tts_to_file(self, text, speaker_id, output_path=None, sdp_ratio=0.2, noise_scale=0.6, noise_scale_w=0.8, speed=1.0, pbar=None, format=None, position=None, quiet=False,):\n        language = self.language\n        texts = self.split_sentences_into_pieces(text, language, quiet)\n        audio_list = []\n        if pbar:\n            tx = pbar(texts)\n        else:\n            if position:\n                tx = tqdm(texts, position=position)\n            elif quiet:\n                tx = texts\n            else:\n                tx = tqdm(texts)\n        for t in tx:\n            if language in ['EN', 'ZH_MIX_EN']:\n                t = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', t)\n            device = self.device\n            bert, ja_bert, phones, tones, lang_ids = utils.get_text_for_tts_infer(t, language, self.hps, device, self.symbol_to_id)\n            with torch.no_grad():\n                x_tst = phones.to(device).unsqueeze(0)\n                tones = tones.to(device).unsqueeze(0)\n                lang_ids = lang_ids.to(device).unsqueeze(0)\n                bert = bert.to(device).unsqueeze(0)\n                ja_bert = ja_bert.to(device).unsqueeze(0)\n                x_tst_lengths = torch.LongTensor([phones.size(0)]).to(device)\n                del phones\n                speakers = torch.LongTensor([speaker_id]).to(device)\n                audio = self.model.infer(\n                        x_tst,\n                        x_tst_lengths,\n                        speakers,\n                        tones,\n                        lang_ids,\n                        bert,\n                        ja_bert,\n                        sdp_ratio=sdp_ratio,\n                        noise_scale=noise_scale,\n                        noise_scale_w=noise_scale_w,\n                        length_scale=1. / speed,\n                    )[0][0, 0].data.cpu().float().numpy()\n                del x_tst, tones, lang_ids, bert, ja_bert, x_tst_lengths, speakers\n                # \n            audio_list.append(audio)\n        torch.cuda.empty_cache()\n        audio = self.audio_numpy_concat(audio_list, sr=self.hps.data.sampling_rate, speed=speed)\n\n        if output_path is None:\n            return audio\n        else:\n            if format:\n                soundfile.write(output_path, audio, self.hps.data.sampling_rate, format=format)\n            else:\n                soundfile.write(output_path, audio, self.hps.data.sampling_rate)\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}