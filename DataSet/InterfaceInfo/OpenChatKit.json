{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/OpenChatKit/inference",
    "API_Calls": [
        {
            "Name": "call_ChatModel",
            "Description": "call_ChatModel",
            "Code": "import os\nimport sys\n\nINFERENCE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# TODO: PYTHONPATH hacks are never a good idea. clean this up later\nsys.path.append(os.path.join(INFERENCE_DIR, '..'))\n\nimport cmd\nimport torch\nimport argparse\nimport conversation as convo\nimport retrieval.wikipedia as wp\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, StoppingCriteria, StoppingCriteriaList\nfrom accelerate import infer_auto_device_map, init_empty_weights\n\n\n\nclass OpenChatKitShell(cmd.Cmd):\n    intro = \"Welcome to OpenChatKit shell.   Type /help or /? to list commands.\\n\"\n    prompt = \">>> \"\n\n    def __init__(self, gpu_id, model_name_or_path, max_tokens, sample, temperature, top_k, retrieval, max_memory, do_stream):\n        super().__init__()\n        self._gpu_id = gpu_id\n        self._model_name_or_path = model_name_or_path\n        self._max_tokens = max_tokens\n        self._sample = sample\n        self._temperature = temperature\n        self._top_k = top_k\n        self._retrieval = retrieval\n        self._max_memory = max_memory\n        self._do_stream = do_stream\n\n    def preloop(self):\n        print(f\"Loading {self._model_name_or_path} to cuda:{self._gpu_id}...\")\n        self._model = ChatModel(self._model_name_or_path, self._gpu_id, self._max_memory)\n\n        if self._retrieval:\n            print(f\"Loading retrieval index...\")\n            self._index = wp.WikipediaIndex()\n\n        self._convo = convo.Conversation(\n            self._model.human_id, self._model.bot_id)\n\n    def precmd(self, line):\n        if line.startswith('/'):\n            return line[1:]\n        else:\n            return 'say ' + line\n\n    def do_say(self, arg):\n        if self._retrieval:\n            results = self._index.search(arg)\n            if len(results) > 0:\n                self._convo.push_context_turn(results[0])\n\n        self._convo.push_human_turn(arg)\n\n        output = self._model.do_inference(\n            self._convo.get_raw_prompt(),\n            self._max_tokens,\n            self._sample,\n            self._temperature,\n            self._top_k,\n            lambda x : print(x, end='', flush=True) if self._do_stream else None,\n        )\n\n        self._convo.push_model_response(output)\n\n        print(\"\" if self._do_stream else self._convo.get_last_turn())\n\n    def do_raw_say(self, arg):\n        output = self._model.do_inference(\n            arg,\n            self._max_tokens,\n            self._sample,\n            self._temperature,\n            self._top_k\n        )\n\n        print(output)\n\n    def do_raw_prompt(self, arg):\n        print(self._convo.get_raw_prompt())\n\n    def do_reset(self, arg):\n        self._convo = convo.Conversation(\n            self._model.human_id, self._model.bot_id)\n\n    def do_hyperparameters(self, arg):\n        print(\n            f\"Hyperparameters:\\n\"\n            f\"  max_tokens: {self._max_tokens}\\n\"\n            f\"  sample: {self._sample}\\n\"\n            f\"  temperature: {self._temperature}\\n\"\n            f\"  top_k: {self._top_k}\"\n        )\n\n    def do_quit(self, arg):\n        return True\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='test harness for OpenChatKit')\n\n    parser.add_argument(\n        '--gpu-id',\n        default=0,\n        type=int,\n        help='the ID of the GPU to run on'\n    )\n    parser.add_argument(\n        '--model',\n        default=f\"{INFERENCE_DIR}/../huggingface_models/Pythia-Chat-Base-7B\",\n        help='name/path of the model'\n    )\n    parser.add_argument(\n        '--max-tokens',\n        default=128,\n        type=int,\n        help='the maximum number of tokens to generate'\n    )\n    parser.add_argument(\n        '--sample',\n        default=True,\n        action='store_true',\n        help='indicates whether to sample'\n    )\n    parser.add_argument(\n        '--no-stream',\n        action='store_true',\n        help='indicates whether to stream tokens'\n    )\n    parser.add_argument(\n        '--temperature',\n        default=0.6,\n        type=float,\n        help='temperature for the LM'\n    )\n    parser.add_argument(\n        '--top-k',\n        default=40,\n        type=int,\n        help='top-k for the LM'\n    )\n    parser.add_argument(\n        '--retrieval',\n        default=False,\n        action='store_true',\n        help='augment queries with context from the retrieval index'\n    )\n    parser.add_argument(\n        '-g',\n        '--gpu-vram',\n        action='store',\n        help='max VRAM to allocate per GPU',\n        nargs='+',\n        required=False,\n    )\n    parser.add_argument(\n        '-r',\n        '--cpu-ram',\n        default=None,\n        type=int,\n        help='max CPU RAM to allocate',\n        required=False\n    )\n    args = parser.parse_args()\n\n    # set max_memory dictionary if given\n    if args.gpu_vram is None:\n        max_memory = None\n    else:\n        max_memory = {}\n        for i in range(len(args.gpu_vram)):\n            # assign CUDA ID as label and XGiB as value\n            max_memory[int(args.gpu_vram[i].split(':')[0])] = f\"{args.gpu_vram[i].split(':')[1]}GiB\"\n\n        if args.cpu_ram is not None:\n            # add cpu to max-memory if given\n            max_memory['cpu'] = f\"{int(args.cpu_ram)}GiB\"\n\n    OpenChatKitShell(\n        args.gpu_id,\n        args.model,\n        args.max_tokens,\n        args.sample,\n        args.temperature,\n        args.top_k,\n        args.retrieval,\n        max_memory,\n        not args.no_stream,\n    ).cmdloop()\n\n\nif __name__ == '__main__':\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/OpenChatKit/inference/bot.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "ChatModel",
            "Description": "ChatModel",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/OpenChatKit/inference/bot.py",
            "Implementation": "import os\nimport sys\n\nINFERENCE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# TODO: PYTHONPATH hacks are never a good idea. clean this up later\nsys.path.append(os.path.join(INFERENCE_DIR, '..'))\n\nimport cmd\nimport torch\nimport argparse\nimport conversation as convo\nimport retrieval.wikipedia as wp\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, StoppingCriteria, StoppingCriteriaList\nfrom accelerate import infer_auto_device_map, init_empty_weights\n\n\nclass ChatModel:\n    human_id = \"<human>\"\n    bot_id = \"<bot>\"\n\n    def __init__(self, model_name, gpu_id, max_memory):\n        device = torch.device('cuda', gpu_id)   # TODO: allow sending to cpu\n\n        # recommended default for devices with > 40 GB VRAM\n        # load model onto one device\n        if max_memory is None:\n            self._model = AutoModelForCausalLM.from_pretrained(\n                model_name, torch_dtype=torch.float16, device_map=f\"cuda:{gpu_id}\")\n            self._model.to(device)\n        # load the model with the given max_memory config (for devices with insufficient VRAM or multi-gpu)\n        else:\n            config = AutoConfig.from_pretrained(model_name)\n            # load empty weights\n            with init_empty_weights():\n                model_from_conf = AutoModelForCausalLM.from_config(config)\n\n            model_from_conf.tie_weights()\n\n            # create a device_map from max_memory\n            device_map = infer_auto_device_map(\n                model_from_conf,\n                max_memory=max_memory,\n                no_split_module_classes=[\"GPTNeoXLayer\"],\n                dtype=\"float16\"\n            )\n            # load the model with the above device_map\n            self._model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                device_map=device_map,\n                offload_folder=\"offload\",  # optional offload-to-disk overflow directory (auto-created)\n                offload_state_dict=True,\n                torch_dtype=torch.float16\n            )\n        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def do_inference(self, prompt, max_new_tokens, do_sample, temperature, top_k, stream_callback=None):\n        stop_criteria = StopWordsCriteria(self._tokenizer, [self.human_id], stream_callback)\n        inputs = (\n            self._tokenizer(prompt, return_tensors='pt')\n            .to(self._model.device)\n        )\n        outputs = self._model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            temperature=temperature,\n            top_k=top_k,\n            pad_token_id=self._tokenizer.eos_token_id,\n            stopping_criteria=StoppingCriteriaList([stop_criteria]),\n        )\n        output = self._tokenizer.batch_decode(outputs)[0]\n\n        # remove the context from the output\n        output = output[len(prompt):]\n\n        return output",
            "Examples": [
                "\n"
            ]
        }
    ]
}