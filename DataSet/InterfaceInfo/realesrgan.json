{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/Real-ESRGAN",
    "API_Calls": [
        {
            "Name": "inference_realesrgan",
            "Description": "实现了一个基于Real-ESRGAN的图像超分辨率工具，支持多模型选择、分块处理、人脸增强和批量处理，可灵活提升普通照片/动漫图像的清晰度并修复细节",
            "Code": "import argparse\nimport cv2\nimport glob\nimport os\nfrom basicsr.archs.rrdbnet_arch import RRDBNet\nfrom basicsr.utils.download_util import load_file_from_url\n\nfrom realesrgan import RealESRGANer\nfrom realesrgan.archs.srvgg_arch import SRVGGNetCompact\n\n\ndef main():\n    \"\"\"Inference demo for Real-ESRGAN.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-i', '--input', type=str, default='inputs', help='Input image or folder')\n    parser.add_argument(\n        '-n',\n        '--model_name',\n        type=str,\n        default='RealESRGAN_x4plus',\n        help=('Model names: RealESRGAN_x4plus | RealESRNet_x4plus | RealESRGAN_x4plus_anime_6B | RealESRGAN_x2plus | '\n              'realesr-animevideov3 | realesr-general-x4v3'))\n    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder')\n    parser.add_argument(\n        '-dn',\n        '--denoise_strength',\n        type=float,\n        default=0.5,\n        help=('Denoise strength. 0 for weak denoise (keep noise), 1 for strong denoise ability. '\n              'Only used for the realesr-general-x4v3 model'))\n    parser.add_argument('-s', '--outscale', type=float, default=4, help='The final upsampling scale of the image')\n    parser.add_argument(\n        '--model_path', type=str, default=None, help='[Option] Model path. Usually, you do not need to specify it')\n    parser.add_argument('--suffix', type=str, default='out', help='Suffix of the restored image')\n    parser.add_argument('-t', '--tile', type=int, default=0, help='Tile size, 0 for no tile during testing')\n    parser.add_argument('--tile_pad', type=int, default=10, help='Tile padding')\n    parser.add_argument('--pre_pad', type=int, default=0, help='Pre padding size at each border')\n    parser.add_argument('--face_enhance', action='store_true', help='Use GFPGAN to enhance face')\n    parser.add_argument(\n        '--fp32', action='store_true', help='Use fp32 precision during inference. Default: fp16 (half precision).')\n    parser.add_argument(\n        '--alpha_upsampler',\n        type=str,\n        default='realesrgan',\n        help='The upsampler for the alpha channels. Options: realesrgan | bicubic')\n    parser.add_argument(\n        '--ext',\n        type=str,\n        default='auto',\n        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs')\n    parser.add_argument(\n        '-g', '--gpu-id', type=int, default=None, help='gpu device to use (default=None) can be 0,1,2 for multi-gpu')\n\n    args = parser.parse_args()\n\n    # determine models according to model names\n    args.model_name = args.model_name.split('.')[0]\n    if args.model_name == 'RealESRGAN_x4plus':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth']\n    elif args.model_name == 'RealESRNet_x4plus':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/RealESRNet_x4plus.pth']\n    elif args.model_name == 'RealESRGAN_x4plus_anime_6B':  # x4 RRDBNet model with 6 blocks\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth']\n    elif args.model_name == 'RealESRGAN_x2plus':  # x2 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n        netscale = 2\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth']\n    elif args.model_name == 'realesr-animevideov3':  # x4 VGG-style model (XS size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth']\n    elif args.model_name == 'realesr-general-x4v3':  # x4 VGG-style model (S size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n        netscale = 4\n        file_url = [\n            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth',\n            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth'\n        ]\n\n    # determine model paths\n    if args.model_path is not None:\n        model_path = args.model_path\n    else:\n        model_path = os.path.join('weights', args.model_name + '.pth')\n        if not os.path.isfile(model_path):\n            ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n            for url in file_url:\n                # model_path will be updated\n                model_path = load_file_from_url(\n                    url=url, model_dir=os.path.join(ROOT_DIR, 'weights'), progress=True, file_name=None)\n\n    # use dni to control the denoise strength\n    dni_weight = None\n    if args.model_name == 'realesr-general-x4v3' and args.denoise_strength != 1:\n        wdn_model_path = model_path.replace('realesr-general-x4v3', 'realesr-general-wdn-x4v3')\n        model_path = [model_path, wdn_model_path]\n        dni_weight = [args.denoise_strength, 1 - args.denoise_strength]\n\n    # restorer\n    upsampler = RealESRGANer(\n        scale=netscale,\n        model_path=model_path,\n        dni_weight=dni_weight,\n        model=model,\n        tile=args.tile,\n        tile_pad=args.tile_pad,\n        pre_pad=args.pre_pad,\n        half=not args.fp32,\n        gpu_id=args.gpu_id)\n\n    if args.face_enhance:  # Use GFPGAN for face enhancement\n        from gfpgan import GFPGANer\n        face_enhancer = GFPGANer(\n            model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n            upscale=args.outscale,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=upsampler)\n    os.makedirs(args.output, exist_ok=True)\n\n    if os.path.isfile(args.input):\n        paths = [args.input]\n    else:\n        paths = sorted(glob.glob(os.path.join(args.input, '*')))\n\n    for idx, path in enumerate(paths):\n        imgname, extension = os.path.splitext(os.path.basename(path))\n        print('Testing', idx, imgname)\n\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        # if len(img.shape) == 3 and img.shape[2] == 4:\n        #     img_mode = 'RGBA'\n        # else:\n        #     img_mode = None\n\n        # 处理图像通道\n        if img is None:\n            print(f\"Warning: Failed to read image {path}\")\n            continue\n            \n        if len(img.shape) == 2:\n            # 灰度转RGB\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n            img_mode = 'RGB'\n        elif len(img.shape) == 3:\n            if img.shape[2] == 4:\n                # 处理RGBA\n                img_mode = 'RGBA'\n            elif img.shape[2] == 3:\n                # BGR转RGB\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                img_mode = 'RGB'\n        else:\n            raise ValueError(f\"Invalid image dimensions: {img.shape}\")\n        \n        try:\n            if args.face_enhance:\n                _, _, output = face_enhancer.enhance(img, has_aligned=False, only_center_face=False, paste_back=True)\n            else:\n                output, _ = upsampler.enhance(img, outscale=args.outscale)\n        except RuntimeError as error:\n            print('Error', error)\n            print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n        else:\n            if args.ext == 'auto':\n                extension = extension[1:]\n            else:\n                extension = args.ext\n            if img_mode == 'RGBA':  # RGBA images should be saved in png format\n                extension = 'png'\n            if args.suffix == '':\n                save_path = os.path.join(args.output, f'{imgname}.{extension}')\n            else:\n                save_path = os.path.join(args.output, f'{imgname}_{args.suffix}.{extension}')\n            cv2.imwrite(save_path, output)\n\n\nif __name__ == '__main__':\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Real-ESRGAN/inference_realesrgan.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class RealESRGANer",
            "Description": "\nRealESRGANer类封装了RealESRGAN模型的完整推理流程，提供高效、灵活且鲁棒的图像超分辨率功能，尤其适合处理大尺寸图像及复杂场景",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Real-ESRGAN/realesrgan/utils.py",
            "Implementation": "class RealESRGANer():\n    \"\"\"A helper class for upsampling images with RealESRGAN.\n\n    Args:\n        scale (int): Upsampling scale factor used in the networks. It is usually 2 or 4.\n        model_path (str): The path to the pretrained model. It can be urls (will first download it automatically).\n        model (nn.Module): The defined network. Default: None.\n        tile (int): As too large images result in the out of GPU memory issue, so this tile option will first crop\n            input images into tiles, and then process each of them. Finally, they will be merged into one image.\n            0 denotes for do not use tile. Default: 0.\n        tile_pad (int): The pad size for each tile, to remove border artifacts. Default: 10.\n        pre_pad (int): Pad the input images to avoid border artifacts. Default: 10.\n        half (float): Whether to use half precision during inference. Default: False.\n    \"\"\"\n\n    def __init__(self,\n                 scale,\n                 model_path,\n                 dni_weight=None,\n                 model=None,\n                 tile=0,\n                 tile_pad=10,\n                 pre_pad=10,\n                 half=False,\n                 device=None,\n                 gpu_id=None):\n        self.scale = scale\n        self.tile_size = tile\n        self.tile_pad = tile_pad\n        self.pre_pad = pre_pad\n        self.mod_scale = None\n        self.half = half\n\n        # initialize model\n        if gpu_id:\n            self.device = torch.device(\n                f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu') if device is None else device\n        else:\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n\n        if isinstance(model_path, list):\n            # dni\n            assert len(model_path) == len(dni_weight), 'model_path and dni_weight should have the save length.'\n            loadnet = self.dni(model_path[0], model_path[1], dni_weight)\n        else:\n            # if the model_path starts with https, it will first download models to the folder: weights\n            if model_path.startswith('https://'):\n                model_path = load_file_from_url(\n                    url=model_path, model_dir=os.path.join(ROOT_DIR, 'weights'), progress=True, file_name=None)\n            loadnet = torch.load(model_path, map_location=torch.device('cpu'))\n\n        # prefer to use params_ema\n        if 'params_ema' in loadnet:\n            keyname = 'params_ema'\n        else:\n            keyname = 'params'\n        model.load_state_dict(loadnet[keyname], strict=True)\n\n        model.eval()\n        self.model = model.to(self.device)\n        if self.half:\n            self.model = self.model.half()\n\n    def dni(self, net_a, net_b, dni_weight, key='params', loc='cpu'):\n        \"\"\"Deep network interpolation.\n\n        ``Paper: Deep Network Interpolation for Continuous Imagery Effect Transition``\n        \"\"\"\n        net_a = torch.load(net_a, map_location=torch.device(loc))\n        net_b = torch.load(net_b, map_location=torch.device(loc))\n        for k, v_a in net_a[key].items():\n            net_a[key][k] = dni_weight[0] * v_a + dni_weight[1] * net_b[key][k]\n        return net_a\n\n    def pre_process(self, img):\n        \"\"\"Pre-process, such as pre-pad and mod pad, so that the images can be divisible\n        \"\"\"\n        img = torch.from_numpy(np.transpose(img, (2, 0, 1))).float()\n        self.img = img.unsqueeze(0).to(self.device)\n        if self.half:\n            self.img = self.img.half()\n\n        # pre_pad\n        if self.pre_pad != 0:\n            self.img = F.pad(self.img, (0, self.pre_pad, 0, self.pre_pad), 'reflect')\n        # mod pad for divisible borders\n        if self.scale == 2:\n            self.mod_scale = 2\n        elif self.scale == 1:\n            self.mod_scale = 4\n        if self.mod_scale is not None:\n            self.mod_pad_h, self.mod_pad_w = 0, 0\n            _, _, h, w = self.img.size()\n            if (h % self.mod_scale != 0):\n                self.mod_pad_h = (self.mod_scale - h % self.mod_scale)\n            if (w % self.mod_scale != 0):\n                self.mod_pad_w = (self.mod_scale - w % self.mod_scale)\n            self.img = F.pad(self.img, (0, self.mod_pad_w, 0, self.mod_pad_h), 'reflect')\n\n    def process(self):\n        # model inference\n        self.output = self.model(self.img)\n\n    def tile_process(self):\n        \"\"\"It will first crop input images to tiles, and then process each tile.\n        Finally, all the processed tiles are merged into one images.\n\n        Modified from: https://github.com/ata4/esrgan-launcher\n        \"\"\"\n        batch, channel, height, width = self.img.shape\n        output_height = height * self.scale\n        output_width = width * self.scale\n        output_shape = (batch, channel, output_height, output_width)\n\n        # start with black image\n        self.output = self.img.new_zeros(output_shape)\n        tiles_x = math.ceil(width / self.tile_size)\n        tiles_y = math.ceil(height / self.tile_size)\n\n        # loop over all tiles\n        for y in range(tiles_y):\n            for x in range(tiles_x):\n                # extract tile from input image\n                ofs_x = x * self.tile_size\n                ofs_y = y * self.tile_size\n                # input tile area on total image\n                input_start_x = ofs_x\n                input_end_x = min(ofs_x + self.tile_size, width)\n                input_start_y = ofs_y\n                input_end_y = min(ofs_y + self.tile_size, height)\n\n                # input tile area on total image with padding\n                input_start_x_pad = max(input_start_x - self.tile_pad, 0)\n                input_end_x_pad = min(input_end_x + self.tile_pad, width)\n                input_start_y_pad = max(input_start_y - self.tile_pad, 0)\n                input_end_y_pad = min(input_end_y + self.tile_pad, height)\n\n                # input tile dimensions\n                input_tile_width = input_end_x - input_start_x\n                input_tile_height = input_end_y - input_start_y\n                tile_idx = y * tiles_x + x + 1\n                input_tile = self.img[:, :, input_start_y_pad:input_end_y_pad, input_start_x_pad:input_end_x_pad]\n\n                # upscale tile\n                try:\n                    with torch.no_grad():\n                        output_tile = self.model(input_tile)\n                except RuntimeError as error:\n                    print('Error', error)\n                print(f'\\tTile {tile_idx}/{tiles_x * tiles_y}')\n\n                # output tile area on total image\n                output_start_x = input_start_x * self.scale\n                output_end_x = input_end_x * self.scale\n                output_start_y = input_start_y * self.scale\n                output_end_y = input_end_y * self.scale\n\n                # output tile area without padding\n                output_start_x_tile = (input_start_x - input_start_x_pad) * self.scale\n                output_end_x_tile = output_start_x_tile + input_tile_width * self.scale\n                output_start_y_tile = (input_start_y - input_start_y_pad) * self.scale\n                output_end_y_tile = output_start_y_tile + input_tile_height * self.scale\n\n                # put tile into output image\n                self.output[:, :, output_start_y:output_end_y,\n                            output_start_x:output_end_x] = output_tile[:, :, output_start_y_tile:output_end_y_tile,\n                                                                       output_start_x_tile:output_end_x_tile]\n\n    def post_process(self):\n        # remove extra pad\n        if self.mod_scale is not None:\n            _, _, h, w = self.output.size()\n            self.output = self.output[:, :, 0:h - self.mod_pad_h * self.scale, 0:w - self.mod_pad_w * self.scale]\n        # remove prepad\n        if self.pre_pad != 0:\n            _, _, h, w = self.output.size()\n            self.output = self.output[:, :, 0:h - self.pre_pad * self.scale, 0:w - self.pre_pad * self.scale]\n        return self.output\n\n    @torch.no_grad()\n    def enhance(self, img, outscale=None, alpha_upsampler='realesrgan'):\n        h_input, w_input = img.shape[0:2]\n        # img: numpy\n        img = img.astype(np.float32)\n        if np.max(img) > 256:  # 16-bit image\n            max_range = 65535\n            print('\\tInput is a 16-bit image')\n        else:\n            max_range = 255\n        img = img / max_range\n        if len(img.shape) == 2:  # gray image\n            img_mode = 'L'\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        elif img.shape[2] == 4:  # RGBA image with alpha channel\n            img_mode = 'RGBA'\n            alpha = img[:, :, 3]\n            img = img[:, :, 0:3]\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            if alpha_upsampler == 'realesrgan':\n                alpha = cv2.cvtColor(alpha, cv2.COLOR_GRAY2RGB)\n        else:\n            img_mode = 'RGB'\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # ------------------- process image (without the alpha channel) ------------------- #\n        self.pre_process(img)\n        if self.tile_size > 0:\n            self.tile_process()\n        else:\n            self.process()\n        output_img = self.post_process()\n        output_img = output_img.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n        output_img = np.transpose(output_img[[2, 1, 0], :, :], (1, 2, 0))\n        if img_mode == 'L':\n            output_img = cv2.cvtColor(output_img, cv2.COLOR_BGR2GRAY)\n\n        # ------------------- process the alpha channel if necessary ------------------- #\n        if img_mode == 'RGBA':\n            if alpha_upsampler == 'realesrgan':\n                self.pre_process(alpha)\n                if self.tile_size > 0:\n                    self.tile_process()\n                else:\n                    self.process()\n                output_alpha = self.post_process()\n                output_alpha = output_alpha.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n                output_alpha = np.transpose(output_alpha[[2, 1, 0], :, :], (1, 2, 0))\n                output_alpha = cv2.cvtColor(output_alpha, cv2.COLOR_BGR2GRAY)\n            else:  # use the cv2 resize for alpha channel\n                h, w = alpha.shape[0:2]\n                output_alpha = cv2.resize(alpha, (w * self.scale, h * self.scale), interpolation=cv2.INTER_LINEAR)\n\n            # merge the alpha channel\n            output_img = cv2.cvtColor(output_img, cv2.COLOR_BGR2BGRA)\n            output_img[:, :, 3] = output_alpha\n\n        # ------------------------------ return ------------------------------ #\n        if max_range == 65535:  # 16-bit image\n            output = (output_img * 65535.0).round().astype(np.uint16)\n        else:\n            output = (output_img * 255.0).round().astype(np.uint8)\n\n        if outscale is not None and outscale != float(self.scale):\n            output = cv2.resize(\n                output, (\n                    int(w_input * outscale),\n                    int(h_input * outscale),\n                ), interpolation=cv2.INTER_LANCZOS4)\n\n        return output, img_mode",
            "Example": [
            ]
        }
    ]
}