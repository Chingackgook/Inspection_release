{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/Deep-Live-Cam",
    "API_Calls": [
        {
            "Name": "call_process_image",
            "Description": "api调用关键接口process_image",
            "Code": "import os\nimport sys\n# single thread doubles cuda performance - needs to be set before torch import\nif any(arg.startswith('--execution-provider') for arg in sys.argv):\n    os.environ['OMP_NUM_THREADS'] = '1'\n# reduce tensorflow log level\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport warnings\nfrom typing import List\nimport platform\nimport signal\nimport shutil\nimport argparse\nimport torch\nimport onnxruntime\nimport tensorflow\n\nimport modules.globals\nimport modules.metadata\nimport modules.ui as ui\nfrom modules.processors.frame.core import get_frame_processors_modules\nfrom modules.utilities import has_image_extension, is_image, is_video, detect_fps, create_video, extract_frames, get_temp_frame_paths, restore_audio, create_temp, move_temp, clean_temp, normalize_output_path\n\nif 'ROCMExecutionProvider' in modules.globals.execution_providers:\n    del torch\n\nwarnings.filterwarnings('ignore', category=FutureWarning, module='insightface')\nwarnings.filterwarnings('ignore', category=UserWarning, module='torchvision')\n\n\ndef parse_args() -> None:\n    signal.signal(signal.SIGINT, lambda signal_number, frame: destroy())\n    program = argparse.ArgumentParser()\n    program.add_argument('-s', '--source', help='select an source image', dest='source_path')\n    program.add_argument('-t', '--target', help='select an target image or video', dest='target_path')\n    program.add_argument('-o', '--output', help='select output file or directory', dest='output_path')\n    program.add_argument('--frame-processor', help='pipeline of frame processors', dest='frame_processor', default=['face_swapper'], choices=['face_swapper', 'face_enhancer'], nargs='+')\n    program.add_argument('--keep-fps', help='keep original fps', dest='keep_fps', action='store_true', default=False)\n    program.add_argument('--keep-audio', help='keep original audio', dest='keep_audio', action='store_true', default=True)\n    program.add_argument('--keep-frames', help='keep temporary frames', dest='keep_frames', action='store_true', default=False)\n    program.add_argument('--many-faces', help='process every face', dest='many_faces', action='store_true', default=False)\n    program.add_argument('--nsfw-filter', help='filter the NSFW image or video', dest='nsfw_filter', action='store_true', default=False)\n    program.add_argument('--map-faces', help='map source target faces', dest='map_faces', action='store_true', default=False)\n    program.add_argument('--mouth-mask', help='mask the mouth region', dest='mouth_mask', action='store_true', default=False)\n    program.add_argument('--video-encoder', help='adjust output video encoder', dest='video_encoder', default='libx264', choices=['libx264', 'libx265', 'libvpx-vp9'])\n    program.add_argument('--video-quality', help='adjust output video quality', dest='video_quality', type=int, default=18, choices=range(52), metavar='[0-51]')\n    program.add_argument('-l', '--lang', help='Ui language', default=\"en\")\n    program.add_argument('--live-mirror', help='The live camera display as you see it in the front-facing camera frame', dest='live_mirror', action='store_true', default=False)\n    program.add_argument('--live-resizable', help='The live camera frame is resizable', dest='live_resizable', action='store_true', default=False)\n    program.add_argument('--max-memory', help='maximum amount of RAM in GB', dest='max_memory', type=int, default=suggest_max_memory())\n    program.add_argument('--execution-provider', help='execution provider', dest='execution_provider', default=['cpu'], choices=suggest_execution_providers(), nargs='+')\n    program.add_argument('--execution-threads', help='number of execution threads', dest='execution_threads', type=int, default=suggest_execution_threads())\n    program.add_argument('-v', '--version', action='version', version=f'{modules.metadata.name} {modules.metadata.version}')\n\n    # register deprecated args\n    program.add_argument('-f', '--face', help=argparse.SUPPRESS, dest='source_path_deprecated')\n    program.add_argument('--cpu-cores', help=argparse.SUPPRESS, dest='cpu_cores_deprecated', type=int)\n    program.add_argument('--gpu-vendor', help=argparse.SUPPRESS, dest='gpu_vendor_deprecated')\n    program.add_argument('--gpu-threads', help=argparse.SUPPRESS, dest='gpu_threads_deprecated', type=int)\n\n    args = program.parse_args()\n\n    modules.globals.source_path = args.source_path\n    modules.globals.target_path = args.target_path\n    modules.globals.output_path = normalize_output_path(modules.globals.source_path, modules.globals.target_path, args.output_path)\n    modules.globals.frame_processors = args.frame_processor\n    modules.globals.headless = args.source_path or args.target_path or args.output_path\n    modules.globals.keep_fps = args.keep_fps\n    modules.globals.keep_audio = args.keep_audio\n    modules.globals.keep_frames = args.keep_frames\n    modules.globals.many_faces = args.many_faces\n    modules.globals.mouth_mask = args.mouth_mask\n    modules.globals.nsfw_filter = args.nsfw_filter\n    modules.globals.map_faces = args.map_faces\n    modules.globals.video_encoder = args.video_encoder\n    modules.globals.video_quality = args.video_quality\n    modules.globals.live_mirror = args.live_mirror\n    modules.globals.live_resizable = args.live_resizable\n    modules.globals.max_memory = args.max_memory\n    modules.globals.execution_providers = decode_execution_providers(args.execution_provider)\n    modules.globals.execution_threads = args.execution_threads\n    modules.globals.lang = args.lang\n\n    #for ENHANCER tumbler:\n    if 'face_enhancer' in args.frame_processor:\n        modules.globals.fp_ui['face_enhancer'] = True\n    else:\n        modules.globals.fp_ui['face_enhancer'] = False\n\n    # translate deprecated args\n    if args.source_path_deprecated:\n        print('\\033[33mArgument -f and --face are deprecated. Use -s and --source instead.\\033[0m')\n        modules.globals.source_path = args.source_path_deprecated\n        modules.globals.output_path = normalize_output_path(args.source_path_deprecated, modules.globals.target_path, args.output_path)\n    if args.cpu_cores_deprecated:\n        print('\\033[33mArgument --cpu-cores is deprecated. Use --execution-threads instead.\\033[0m')\n        modules.globals.execution_threads = args.cpu_cores_deprecated\n    if args.gpu_vendor_deprecated == 'apple':\n        print('\\033[33mArgument --gpu-vendor apple is deprecated. Use --execution-provider coreml instead.\\033[0m')\n        modules.globals.execution_providers = decode_execution_providers(['coreml'])\n    if args.gpu_vendor_deprecated == 'nvidia':\n        print('\\033[33mArgument --gpu-vendor nvidia is deprecated. Use --execution-provider cuda instead.\\033[0m')\n        modules.globals.execution_providers = decode_execution_providers(['cuda'])\n    if args.gpu_vendor_deprecated == 'amd':\n        print('\\033[33mArgument --gpu-vendor amd is deprecated. Use --execution-provider cuda instead.\\033[0m')\n        modules.globals.execution_providers = decode_execution_providers(['rocm'])\n    if args.gpu_threads_deprecated:\n        print('\\033[33mArgument --gpu-threads is deprecated. Use --execution-threads instead.\\033[0m')\n        modules.globals.execution_threads = args.gpu_threads_deprecated\n\n\ndef encode_execution_providers(execution_providers: List[str]) -> List[str]:\n    return [execution_provider.replace('ExecutionProvider', '').lower() for execution_provider in execution_providers]\n\n\ndef decode_execution_providers(execution_providers: List[str]) -> List[str]:\n    return [provider for provider, encoded_execution_provider in zip(onnxruntime.get_available_providers(), encode_execution_providers(onnxruntime.get_available_providers()))\n            if any(execution_provider in encoded_execution_provider for execution_provider in execution_providers)]\n\n\ndef suggest_max_memory() -> int:\n    if platform.system().lower() == 'darwin':\n        return 4\n    return 16\n\n\ndef suggest_execution_providers() -> List[str]:\n    return encode_execution_providers(onnxruntime.get_available_providers())\n\n\ndef suggest_execution_threads() -> int:\n    if 'DmlExecutionProvider' in modules.globals.execution_providers:\n        return 1\n    if 'ROCMExecutionProvider' in modules.globals.execution_providers:\n        return 1\n    return 8\n\n\ndef limit_resources() -> None:\n    # prevent tensorflow memory leak\n    gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n    for gpu in gpus:\n        tensorflow.config.experimental.set_memory_growth(gpu, True)\n    # limit memory usage\n    if modules.globals.max_memory:\n        memory = modules.globals.max_memory * 1024 ** 3\n        if platform.system().lower() == 'darwin':\n            memory = modules.globals.max_memory * 1024 ** 6\n        if platform.system().lower() == 'windows':\n            import ctypes\n            kernel32 = ctypes.windll.kernel32\n            kernel32.SetProcessWorkingSetSize(-1, ctypes.c_size_t(memory), ctypes.c_size_t(memory))\n        else:\n            import resource\n            resource.setrlimit(resource.RLIMIT_DATA, (memory, memory))\n\n\ndef release_resources() -> None:\n    if 'CUDAExecutionProvider' in modules.globals.execution_providers:\n        torch.cuda.empty_cache()\n\n\ndef pre_check() -> bool:\n    if sys.version_info < (3, 9):\n        update_status('Python version is not supported - please upgrade to 3.9 or higher.')\n        return False\n    if not shutil.which('ffmpeg'):\n        update_status('ffmpeg is not installed.')\n        return False\n    return True\n\n\ndef update_status(message: str, scope: str = 'DLC.CORE') -> None:\n    print(f'[{scope}] {message}')\n    if not modules.globals.headless:\n        ui.update_status(message)\n\ndef start() -> None:\n    for frame_processor in get_frame_processors_modules(modules.globals.frame_processors):\n        if not frame_processor.pre_start():\n            return\n    update_status('Processing...')\n    # process image to image\n    if has_image_extension(modules.globals.target_path):\n        if modules.globals.nsfw_filter and ui.check_and_ignore_nsfw(modules.globals.target_path, destroy):\n            return\n        try:\n            shutil.copy2(modules.globals.target_path, modules.globals.output_path)\n        except Exception as e:\n            print(\"Error copying file:\", str(e))\n        for frame_processor in get_frame_processors_modules(modules.globals.frame_processors):\n            update_status('Progressing...', frame_processor.NAME)\n            frame_processor.process_image(modules.globals.source_path, modules.globals.output_path, modules.globals.output_path)\n            release_resources()\n        if is_image(modules.globals.target_path):\n            update_status('Processing to image succeed!')\n        else:\n            update_status('Processing to image failed!')\n        return\n    # process image to videos\n    if modules.globals.nsfw_filter and ui.check_and_ignore_nsfw(modules.globals.target_path, destroy):\n        return\n\n    if not modules.globals.map_faces:\n        update_status('Creating temp resources...')\n        create_temp(modules.globals.target_path)\n        update_status('Extracting frames...')\n        extract_frames(modules.globals.target_path)\n\n    temp_frame_paths = get_temp_frame_paths(modules.globals.target_path)\n    for frame_processor in get_frame_processors_modules(modules.globals.frame_processors):\n        update_status('Progressing...', frame_processor.NAME)\n        frame_processor.process_video(modules.globals.source_path, temp_frame_paths)\n        release_resources()\n    # handles fps\n    if modules.globals.keep_fps:\n        update_status('Detecting fps...')\n        fps = detect_fps(modules.globals.target_path)\n        update_status(f'Creating video with {fps} fps...')\n        create_video(modules.globals.target_path, fps)\n    else:\n        update_status('Creating video with 30.0 fps...')\n        create_video(modules.globals.target_path)\n    # handle audio\n    if modules.globals.keep_audio:\n        if modules.globals.keep_fps:\n            update_status('Restoring audio...')\n        else:\n            update_status('Restoring audio might cause issues as fps are not kept...')\n        restore_audio(modules.globals.target_path, modules.globals.output_path)\n    else:\n        move_temp(modules.globals.target_path, modules.globals.output_path)\n    # clean and validate\n    clean_temp(modules.globals.target_path)\n    if is_video(modules.globals.target_path):\n        update_status('Processing to video succeed!')\n    else:\n        update_status('Processing to video failed!')\n\n\ndef destroy(to_quit=True) -> None:\n    if modules.globals.target_path:\n        clean_temp(modules.globals.target_path)\n    if to_quit: quit()\n\n\ndef run() -> None:\n    parse_args()\n    if not pre_check():\n        return\n    for frame_processor in get_frame_processors_modules(modules.globals.frame_processors):\n        if not frame_processor.pre_check():\n            return\n    limit_resources()\n    if modules.globals.headless:\n        start()\n    else:\n        window = ui.init(start, destroy, modules.globals.lang)\n        window.mainloop()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Deep-Live-Cam/modules/core.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "face_implementations",
            "Description": "\n一些关键调用接口的实现",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Deep-Live-Cam/modules/processors/frame/face_swapper.py",
            "Implementation": "from typing import Any, List\nimport cv2\nimport insightface\nimport threading\nimport numpy as np\nimport modules.globals\nimport logging\nimport modules.processors.frame.core\n#from modules.core import update_status\nfrom modules.face_analyser import get_one_face, get_many_faces, default_source_face\nfrom modules.typing import Face, Frame\n\n\n\nfrom typing import Any\n\nfrom insightface.app.common import Face\nimport numpy\n\nFace = Face\nFrame = numpy.ndarray[Any, Any]\nfrom modules.utilities import (\n    conditional_download,\n    is_image,\n    is_video,\n)\nfrom modules.cluster_analysis import find_closest_centroid\nimport os\n\nFACE_SWAPPER = None\nTHREAD_LOCK = threading.Lock()\nNAME = \"DLC.FACE-SWAPPER\"\n\nabs_dir = os.path.dirname(os.path.abspath(__file__))\nmodels_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.dirname(abs_dir))), \"models\"\n)\n\n\ndef pre_check() -> bool:\n    download_directory_path = abs_dir\n    conditional_download(\n        download_directory_path,\n        [\n            \"https://huggingface.co/hacksider/deep-live-cam/blob/main/inswapper_128_fp16.onnx\"\n        ],\n    )\n    return True\n\n\ndef pre_start() -> bool:\n    if not modules.globals.map_faces and not is_image(modules.globals.source_path):\n        #update_status(\"Select an image for source path.\", NAME)\n        return False\n    elif not modules.globals.map_faces and not get_one_face(\n        cv2.imread(modules.globals.source_path)\n    ):\n        #update_status(\"No face in source path detected.\", NAME)\n        return False\n    if not is_image(modules.globals.target_path) and not is_video(\n        modules.globals.target_path\n    ):\n        #update_status(\"Select an image or video for target path.\", NAME)\n        return False\n    return True\n\n\ndef get_face_swapper() -> Any:\n    global FACE_SWAPPER\n\n    with THREAD_LOCK:\n        if FACE_SWAPPER is None:\n            model_path = os.path.join(models_dir, \"inswapper_128_fp16.onnx\")\n            FACE_SWAPPER = insightface.model_zoo.get_model(\n                model_path, providers=modules.globals.execution_providers\n            )\n    return FACE_SWAPPER\n\n\ndef swap_face(source_face: Face, target_face: Face, temp_frame: Frame) -> Frame:\n    face_swapper = get_face_swapper()\n\n    # Apply the face swap\n    swapped_frame = face_swapper.get(\n        temp_frame, target_face, source_face, paste_back=True\n    )\n\n    if modules.globals.mouth_mask:\n        # Create a mask for the target face\n        face_mask = create_face_mask(target_face, temp_frame)\n\n        # Create the mouth mask\n        mouth_mask, mouth_cutout, mouth_box, lower_lip_polygon = (\n            create_lower_mouth_mask(target_face, temp_frame)\n        )\n\n        # Apply the mouth area\n        swapped_frame = apply_mouth_area(\n            swapped_frame, mouth_cutout, mouth_box, face_mask, lower_lip_polygon\n        )\n\n        if modules.globals.show_mouth_mask_box:\n            mouth_mask_data = (mouth_mask, mouth_cutout, mouth_box, lower_lip_polygon)\n            swapped_frame = draw_mouth_mask_visualization(\n                swapped_frame, target_face, mouth_mask_data\n            )\n\n    return swapped_frame\n\n\ndef process_frame(source_face: Face, temp_frame: Frame) -> Frame:\n    if modules.globals.color_correction:\n        temp_frame = cv2.cvtColor(temp_frame, cv2.COLOR_BGR2RGB)\n\n    if modules.globals.many_faces:\n        many_faces = get_many_faces(temp_frame)\n        if many_faces:\n            for target_face in many_faces:\n                if source_face and target_face:\n                    temp_frame = swap_face(source_face, target_face, temp_frame)\n                else:\n                    print(\"Face detection failed for target/source.\")\n    else:\n        target_face = get_one_face(temp_frame)\n        if target_face and source_face:\n            temp_frame = swap_face(source_face, target_face, temp_frame)\n        else:\n            logging.error(\"Face detection failed for target or source.\")\n    return temp_frame\n\n\n\ndef process_frame_v2(temp_frame: Frame, temp_frame_path: str = \"\") -> Frame:\n    if is_image(modules.globals.target_path):\n        if modules.globals.many_faces:\n            source_face = default_source_face()\n            for map in modules.globals.source_target_map:\n                target_face = map[\"target\"][\"face\"]\n                temp_frame = swap_face(source_face, target_face, temp_frame)\n\n        elif not modules.globals.many_faces:\n            for map in modules.globals.source_target_map:\n                if \"source\" in map:\n                    source_face = map[\"source\"][\"face\"]\n                    target_face = map[\"target\"][\"face\"]\n                    temp_frame = swap_face(source_face, target_face, temp_frame)\n\n    elif is_video(modules.globals.target_path):\n        if modules.globals.many_faces:\n            source_face = default_source_face()\n            for map in modules.globals.source_target_map:\n                target_frame = [\n                    f\n                    for f in map[\"target_faces_in_frame\"]\n                    if f[\"location\"] == temp_frame_path\n                ]\n\n                for frame in target_frame:\n                    for target_face in frame[\"faces\"]:\n                        temp_frame = swap_face(source_face, target_face, temp_frame)\n\n        elif not modules.globals.many_faces:\n            for map in modules.globals.source_target_map:\n                if \"source\" in map:\n                    target_frame = [\n                        f\n                        for f in map[\"target_faces_in_frame\"]\n                        if f[\"location\"] == temp_frame_path\n                    ]\n                    source_face = map[\"source\"][\"face\"]\n\n                    for frame in target_frame:\n                        for target_face in frame[\"faces\"]:\n                            temp_frame = swap_face(source_face, target_face, temp_frame)\n\n    else:\n        detected_faces = get_many_faces(temp_frame)\n        if modules.globals.many_faces:\n            if detected_faces:\n                source_face = default_source_face()\n                for target_face in detected_faces:\n                    temp_frame = swap_face(source_face, target_face, temp_frame)\n\n        elif not modules.globals.many_faces:\n            if detected_faces:\n                if len(detected_faces) <= len(\n                    modules.globals.simple_map[\"target_embeddings\"]\n                ):\n                    for detected_face in detected_faces:\n                        closest_centroid_index, _ = find_closest_centroid(\n                            modules.globals.simple_map[\"target_embeddings\"],\n                            detected_face.normed_embedding,\n                        )\n\n                        temp_frame = swap_face(\n                            modules.globals.simple_map[\"source_faces\"][\n                                closest_centroid_index\n                            ],\n                            detected_face,\n                            temp_frame,\n                        )\n                else:\n                    detected_faces_centroids = []\n                    for face in detected_faces:\n                        detected_faces_centroids.append(face.normed_embedding)\n                    i = 0\n                    for target_embedding in modules.globals.simple_map[\n                        \"target_embeddings\"\n                    ]:\n                        closest_centroid_index, _ = find_closest_centroid(\n                            detected_faces_centroids, target_embedding\n                        )\n\n                        temp_frame = swap_face(\n                            modules.globals.simple_map[\"source_faces\"][i],\n                            detected_faces[closest_centroid_index],\n                            temp_frame,\n                        )\n                        i += 1\n    return temp_frame\n\n\ndef process_frames(\n    source_path: str, temp_frame_paths: List[str], progress: Any = None\n) -> None:\n    if not modules.globals.map_faces:\n        source_face = get_one_face(cv2.imread(source_path))\n        for temp_frame_path in temp_frame_paths:\n            temp_frame = cv2.imread(temp_frame_path)\n            try:\n                result = process_frame(source_face, temp_frame)\n                cv2.imwrite(temp_frame_path, result)\n            except Exception as exception:\n                print(exception)\n                pass\n            if progress:\n                progress.update(1)\n    else:\n        for temp_frame_path in temp_frame_paths:\n            temp_frame = cv2.imread(temp_frame_path)\n            try:\n                result = process_frame_v2(temp_frame, temp_frame_path)\n                cv2.imwrite(temp_frame_path, result)\n            except Exception as exception:\n                print(exception)\n                pass\n            if progress:\n                progress.update(1)\n\n\ndef process_image(source_path: str, target_path: str, output_path: str) -> None:\n    if not modules.globals.map_faces:\n        source_face = get_one_face(cv2.imread(source_path))\n        target_frame = cv2.imread(target_path)\n        result = process_frame(source_face, target_frame)\n        cv2.imwrite(output_path, result)\n    else:\n        if modules.globals.many_faces:\n            \"\"\"update_status(\n                \"Many faces enabled. Using first source image. Progressing...\", NAME\n            )\"\"\"\n        target_frame = cv2.imread(output_path)\n        result = process_frame_v2(target_frame)\n        cv2.imwrite(output_path, result)\n\n\ndef process_video(source_path: str, temp_frame_paths: List[str]) -> None:\n    if modules.globals.map_faces and modules.globals.many_faces:\n        \"\"\"update_status(\n            \"Many faces enabled. Using first source image. Progressing...\", NAME\n        )\"\"\"\n    modules.processors.frame.core.process_video(\n        source_path, temp_frame_paths, process_frames\n    )\n\n\ndef create_lower_mouth_mask(\n    face: Face, frame: Frame\n) -> (np.ndarray, np.ndarray, tuple, np.ndarray):\n    mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n    mouth_cutout = None\n    landmarks = face.landmark_2d_106\n    if landmarks is not None:\n        #                  0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20\n        lower_lip_order = [\n            65,\n            66,\n            62,\n            70,\n            69,\n            18,\n            19,\n            20,\n            21,\n            22,\n            23,\n            24,\n            0,\n            8,\n            7,\n            6,\n            5,\n            4,\n            3,\n            2,\n            65,\n        ]\n        lower_lip_landmarks = landmarks[lower_lip_order].astype(\n            np.float32\n        )  # Use float for precise calculations\n\n        # Calculate the center of the landmarks\n        center = np.mean(lower_lip_landmarks, axis=0)\n\n        # Expand the landmarks outward\n        expansion_factor = (\n            1 + modules.globals.mask_down_size\n        )  # Adjust this for more or less expansion\n        expanded_landmarks = (lower_lip_landmarks - center) * expansion_factor + center\n\n        # Extend the top lip part\n        toplip_indices = [\n            20,\n            0,\n            1,\n            2,\n            3,\n            4,\n            5,\n        ]  # Indices for landmarks 2, 65, 66, 62, 70, 69, 18\n        toplip_extension = (\n            modules.globals.mask_size * 0.5\n        )  # Adjust this factor to control the extension\n        for idx in toplip_indices:\n            direction = expanded_landmarks[idx] - center\n            direction = direction / np.linalg.norm(direction)\n            expanded_landmarks[idx] += direction * toplip_extension\n\n        # Extend the bottom part (chin area)\n        chin_indices = [\n            11,\n            12,\n            13,\n            14,\n            15,\n            16,\n        ]  # Indices for landmarks 21, 22, 23, 24, 0, 8\n        chin_extension = 2 * 0.2  # Adjust this factor to control the extension\n        for idx in chin_indices:\n            expanded_landmarks[idx][1] += (\n                expanded_landmarks[idx][1] - center[1]\n            ) * chin_extension\n\n        # Convert back to integer coordinates\n        expanded_landmarks = expanded_landmarks.astype(np.int32)\n\n        # Calculate bounding box for the expanded lower mouth\n        min_x, min_y = np.min(expanded_landmarks, axis=0)\n        max_x, max_y = np.max(expanded_landmarks, axis=0)\n\n        # Add some padding to the bounding box\n        padding = int((max_x - min_x) * 0.1)  # 10% padding\n        min_x = max(0, min_x - padding)\n        min_y = max(0, min_y - padding)\n        max_x = min(frame.shape[1], max_x + padding)\n        max_y = min(frame.shape[0], max_y + padding)\n\n        # Ensure the bounding box dimensions are valid\n        if max_x <= min_x or max_y <= min_y:\n            if (max_x - min_x) <= 1:\n                max_x = min_x + 1\n            if (max_y - min_y) <= 1:\n                max_y = min_y + 1\n\n        # Create the mask\n        mask_roi = np.zeros((max_y - min_y, max_x - min_x), dtype=np.uint8)\n        cv2.fillPoly(mask_roi, [expanded_landmarks - [min_x, min_y]], 255)\n\n        # Apply Gaussian blur to soften the mask edges\n        mask_roi = cv2.GaussianBlur(mask_roi, (15, 15), 5)\n\n        # Place the mask ROI in the full-sized mask\n        mask[min_y:max_y, min_x:max_x] = mask_roi\n\n        # Extract the masked area from the frame\n        mouth_cutout = frame[min_y:max_y, min_x:max_x].copy()\n\n        # Return the expanded lower lip polygon in original frame coordinates\n        lower_lip_polygon = expanded_landmarks\n\n    return mask, mouth_cutout, (min_x, min_y, max_x, max_y), lower_lip_polygon\n\n\ndef draw_mouth_mask_visualization(\n    frame: Frame, face: Face, mouth_mask_data: tuple\n) -> Frame:\n    landmarks = face.landmark_2d_106\n    if landmarks is not None and mouth_mask_data is not None:\n        mask, mouth_cutout, (min_x, min_y, max_x, max_y), lower_lip_polygon = (\n            mouth_mask_data\n        )\n\n        vis_frame = frame.copy()\n\n        # Ensure coordinates are within frame bounds\n        height, width = vis_frame.shape[:2]\n        min_x, min_y = max(0, min_x), max(0, min_y)\n        max_x, max_y = min(width, max_x), min(height, max_y)\n\n        # Adjust mask to match the region size\n        mask_region = mask[0 : max_y - min_y, 0 : max_x - min_x]\n\n        # Remove the color mask overlay\n        # color_mask = cv2.applyColorMap((mask_region * 255).astype(np.uint8), cv2.COLORMAP_JET)\n\n        # Ensure shapes match before blending\n        vis_region = vis_frame[min_y:max_y, min_x:max_x]\n        # Remove blending with color_mask\n        # if vis_region.shape[:2] == color_mask.shape[:2]:\n        #     blended = cv2.addWeighted(vis_region, 0.7, color_mask, 0.3, 0)\n        #     vis_frame[min_y:max_y, min_x:max_x] = blended\n\n        # Draw the lower lip polygon\n        cv2.polylines(vis_frame, [lower_lip_polygon], True, (0, 255, 0), 2)\n\n        # Remove the red box\n        # cv2.rectangle(vis_frame, (min_x, min_y), (max_x, max_y), (0, 0, 255), 2)\n\n        # Visualize the feathered mask\n        feather_amount = max(\n            1,\n            min(\n                30,\n                (max_x - min_x) // modules.globals.mask_feather_ratio,\n                (max_y - min_y) // modules.globals.mask_feather_ratio,\n            ),\n        )\n        # Ensure kernel size is odd\n        kernel_size = 2 * feather_amount + 1\n        feathered_mask = cv2.GaussianBlur(\n            mask_region.astype(float), (kernel_size, kernel_size), 0\n        )\n        feathered_mask = (feathered_mask / feathered_mask.max() * 255).astype(np.uint8)\n        # Remove the feathered mask color overlay\n        # color_feathered_mask = cv2.applyColorMap(feathered_mask, cv2.COLORMAP_VIRIDIS)\n\n        # Ensure shapes match before blending feathered mask\n        # if vis_region.shape == color_feathered_mask.shape:\n        #     blended_feathered = cv2.addWeighted(vis_region, 0.7, color_feathered_mask, 0.3, 0)\n        #     vis_frame[min_y:max_y, min_x:max_x] = blended_feathered\n\n        # Add labels\n        cv2.putText(\n            vis_frame,\n            \"Lower Mouth Mask\",\n            (min_x, min_y - 10),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (255, 255, 255),\n            1,\n        )\n        cv2.putText(\n            vis_frame,\n            \"Feathered Mask\",\n            (min_x, max_y + 20),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (255, 255, 255),\n            1,\n        )\n\n        return vis_frame\n    return frame\n\n\ndef apply_mouth_area(\n    frame: np.ndarray,\n    mouth_cutout: np.ndarray,\n    mouth_box: tuple,\n    face_mask: np.ndarray,\n    mouth_polygon: np.ndarray,\n) -> np.ndarray:\n    min_x, min_y, max_x, max_y = mouth_box\n    box_width = max_x - min_x\n    box_height = max_y - min_y\n\n    if (\n        mouth_cutout is None\n        or box_width is None\n        or box_height is None\n        or face_mask is None\n        or mouth_polygon is None\n    ):\n        return frame\n\n    try:\n        resized_mouth_cutout = cv2.resize(mouth_cutout, (box_width, box_height))\n        roi = frame[min_y:max_y, min_x:max_x]\n\n        if roi.shape != resized_mouth_cutout.shape:\n            resized_mouth_cutout = cv2.resize(\n                resized_mouth_cutout, (roi.shape[1], roi.shape[0])\n            )\n\n        color_corrected_mouth = apply_color_transfer(resized_mouth_cutout, roi)\n\n        # Use the provided mouth polygon to create the mask\n        polygon_mask = np.zeros(roi.shape[:2], dtype=np.uint8)\n        adjusted_polygon = mouth_polygon - [min_x, min_y]\n        cv2.fillPoly(polygon_mask, [adjusted_polygon], 255)\n\n        # Apply feathering to the polygon mask\n        feather_amount = min(\n            30,\n            box_width // modules.globals.mask_feather_ratio,\n            box_height // modules.globals.mask_feather_ratio,\n        )\n        feathered_mask = cv2.GaussianBlur(\n            polygon_mask.astype(float), (0, 0), feather_amount\n        )\n        feathered_mask = feathered_mask / feathered_mask.max()\n\n        face_mask_roi = face_mask[min_y:max_y, min_x:max_x]\n        combined_mask = feathered_mask * (face_mask_roi / 255.0)\n\n        combined_mask = combined_mask[:, :, np.newaxis]\n        blended = (\n            color_corrected_mouth * combined_mask + roi * (1 - combined_mask)\n        ).astype(np.uint8)\n\n        # Apply face mask to blended result\n        face_mask_3channel = (\n            np.repeat(face_mask_roi[:, :, np.newaxis], 3, axis=2) / 255.0\n        )\n        final_blend = blended * face_mask_3channel + roi * (1 - face_mask_3channel)\n\n        frame[min_y:max_y, min_x:max_x] = final_blend.astype(np.uint8)\n    except Exception as e:\n        pass\n\n    return frame\n\n\ndef create_face_mask(face: Face, frame: Frame) -> np.ndarray:\n    mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n    landmarks = face.landmark_2d_106\n    if landmarks is not None:\n        # Convert landmarks to int32\n        landmarks = landmarks.astype(np.int32)\n\n        # Extract facial features\n        right_side_face = landmarks[0:16]\n        left_side_face = landmarks[17:32]\n        right_eye = landmarks[33:42]\n        right_eye_brow = landmarks[43:51]\n        left_eye = landmarks[87:96]\n        left_eye_brow = landmarks[97:105]\n\n        # Calculate forehead extension\n        right_eyebrow_top = np.min(right_eye_brow[:, 1])\n        left_eyebrow_top = np.min(left_eye_brow[:, 1])\n        eyebrow_top = min(right_eyebrow_top, left_eyebrow_top)\n\n        face_top = np.min([right_side_face[0, 1], left_side_face[-1, 1]])\n        forehead_height = face_top - eyebrow_top\n        extended_forehead_height = int(forehead_height * 5.0)  # Extend by 50%\n\n        # Create forehead points\n        forehead_left = right_side_face[0].copy()\n        forehead_right = left_side_face[-1].copy()\n        forehead_left[1] -= extended_forehead_height\n        forehead_right[1] -= extended_forehead_height\n\n        # Combine all points to create the face outline\n        face_outline = np.vstack(\n            [\n                [forehead_left],\n                right_side_face,\n                left_side_face[\n                    ::-1\n                ],  # Reverse left side to create a continuous outline\n                [forehead_right],\n            ]\n        )\n\n        # Calculate padding\n        padding = int(\n            np.linalg.norm(right_side_face[0] - left_side_face[-1]) * 0.05\n        )  # 5% of face width\n\n        # Create a slightly larger convex hull for padding\n        hull = cv2.convexHull(face_outline)\n        hull_padded = []\n        for point in hull:\n            x, y = point[0]\n            center = np.mean(face_outline, axis=0)\n            direction = np.array([x, y]) - center\n            direction = direction / np.linalg.norm(direction)\n            padded_point = np.array([x, y]) + direction * padding\n            hull_padded.append(padded_point)\n\n        hull_padded = np.array(hull_padded, dtype=np.int32)\n\n        # Fill the padded convex hull\n        cv2.fillConvexPoly(mask, hull_padded, 255)\n\n        # Smooth the mask edges\n        mask = cv2.GaussianBlur(mask, (5, 5), 3)\n\n    return mask\n\n\ndef apply_color_transfer(source, target):\n    \"\"\"\n    Apply color transfer from target to source image\n    \"\"\"\n    source = cv2.cvtColor(source, cv2.COLOR_BGR2LAB).astype(\"float32\")\n    target = cv2.cvtColor(target, cv2.COLOR_BGR2LAB).astype(\"float32\")\n\n    source_mean, source_std = cv2.meanStdDev(source)\n    target_mean, target_std = cv2.meanStdDev(target)\n\n    # Reshape mean and std to be broadcastable\n    source_mean = source_mean.reshape(1, 1, 3)\n    source_std = source_std.reshape(1, 1, 3)\n    target_mean = target_mean.reshape(1, 1, 3)\n    target_std = target_std.reshape(1, 1, 3)\n\n    # Perform the color transfer\n    source = (source - source_mean) * (target_std / source_std) + target_mean\n\n    return cv2.cvtColor(np.clip(source, 0, 255).astype(\"uint8\"), cv2.COLOR_LAB2BGR)\n",
            "Example": [
                "\nimport os\nimport modules.globals as globals\nfrom modules.processors.frame.face_swapper import process_image, pre_check, pre_start\n\ndef test_face_swap_image():\n    # 假设 source.jpg 是带有人脸的源图像，target.jpg 是目标图像\n    source_path = \"./1.jpeg\"\n    target_path = \"./2.jpeg\"\n    output_path = \"output.jpg\"\n\n    # 设置全局变量\n    globals.source_path = source_path\n    globals.target_path = target_path\n    globals.map_faces = False  # 不使用预设脸部映射\n    globals.many_faces = False  # 是否处理多张脸\n    globals.color_correction = True\n    globals.mouth_mask = False\n    globals.show_mouth_mask_box = False\n    globals.mask_down_size = 0.1\n    globals.mask_size = 5\n    globals.execution_providers = [\"CPUExecutionProvider\"]\n\n    # 步骤：预检查 -> 启动检查 -> 执行换脸\n    # if not pre_check():\n    #     print(\"预检查失败\")\n    #     return\n    # if not pre_start():\n    #     print(\"启动检查失败\")\n    #     return\n\n    process_image(source_path, target_path, output_path)\n\n    # 检查输出文件是否生成\n    if os.path.exists(output_path):\n        print(f\"测试成功，结果已保存至 {output_path}\")\n    else:\n        print(\"测试失败，输出文件未生成\")\n\nif __name__ == \"__main__\":\n    test_face_swap_image()\n\n\n"
            ]
        }
    ]
}