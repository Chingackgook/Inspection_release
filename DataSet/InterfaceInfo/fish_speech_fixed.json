{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/fish-speech",
    "API_Calls": [
        {
            "Name": "generate_long_call",
            "Description": "the function main call the generate_long interface.",
            "Code": "import os\nimport queue\nimport threading\nimport time\nfrom contextlib import nullcontext\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Literal, Optional, Tuple, Union\n\nimport click\nimport numpy as np\nimport torch\nimport torch._dynamo.config\nimport torch._inductor.config\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom fish_speech.conversation import (\n    CODEBOOK_PAD_TOKEN_ID,\n    Conversation,\n    Message,\n    TextPart,\n    VQPart,\n)\nfrom fish_speech.models.text2semantic.llama import BaseModelArgs\nfrom fish_speech.text import clean_text, split_text\nfrom fish_speech.tokenizer import IM_END_TOKEN, FishTokenizer\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.triton.unique_kernel_names = True\n\nif hasattr(torch._inductor.config, \"fx_graph_cache\"):\n    # Experimental feature to reduce compilation times, will be on by default in future\n    torch._inductor.config.fx_graph_cache = True\n\n\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\n\nfrom fish_speech.models.text2semantic.llama import (\n    BaseTransformer,\n    DualARTransformer,\n    NaiveTransformer,\n)\n\n\n@dataclass\nclass WrappedGenerateResponse:\n    status: Literal[\"success\", \"error\"]\n    response: Optional[GenerateResponse | Exception] = None\n\n\n@dataclass\nclass GenerateRequest:\n    request: dict\n    response_queue: queue.Queue\n\n\ndef launch_thread_safe_queue(\n    checkpoint_path,\n    device,\n    precision,\n    compile: bool = False,\n):\n    input_queue = queue.Queue()\n    init_event = threading.Event()\n\n    def worker():\n        model, decode_one_token = load_model(\n            checkpoint_path, device, precision, compile=compile\n        )\n        with torch.device(device):\n            model.setup_caches(\n                max_batch_size=1,\n                max_seq_len=model.config.max_seq_len,\n                dtype=next(model.parameters()).dtype,\n            )\n        init_event.set()\n\n        while True:\n            item: GenerateRequest | None = input_queue.get()\n            if item is None:\n                break\n\n            kwargs = item.request\n            response_queue = item.response_queue\n\n            try:\n                for chunk in generate_long(\n                    model=model, decode_one_token=decode_one_token, **kwargs\n                ):\n                    response_queue.put(\n                        WrappedGenerateResponse(status=\"success\", response=chunk)\n                    )\n            except Exception as e:\n                response_queue.put(WrappedGenerateResponse(status=\"error\", response=e))\n\n    threading.Thread(target=worker, daemon=True).start()\n    init_event.wait()\n\n    return input_queue\n\n\ndef launch_thread_safe_queue_agent(\n    checkpoint_path,\n    device,\n    precision,\n    compile: bool = False,\n):\n    input_queue = queue.Queue()\n    init_event = threading.Event()\n\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n    config = BaseModelArgs.from_pretrained(checkpoint_path)\n\n    def worker():\n        model, decode_one_token = load_model(\n            checkpoint_path, device, precision, compile=compile, is_agent=True\n        )\n\n        with torch.device(device):\n            model.setup_caches(\n                max_batch_size=1,\n                max_seq_len=model.config.max_seq_len,\n                dtype=next(model.parameters()).dtype,\n            )\n        init_event.set()\n\n        while True:\n            item: GenerateRequest | None = input_queue.get()\n            if item is None:\n                break\n\n            kwargs = item.request\n            response_queue = item.response_queue\n\n            try:\n                for token in generate_agent(\n                    model=model,\n                    decode_one_token=decode_one_token,\n                    **kwargs,\n                ):\n                    response_queue.put(token)\n\n                response_queue.put(\"stop\")\n            except Exception as e:\n                import traceback\n\n                logger.exception(f\"Error in worker: {traceback.format_exc()}\")\n                response_queue.put(\"error\")\n\n    threading.Thread(target=worker, daemon=True).start()\n    init_event.wait()\n\n    return input_queue, tokenizer, config\n\n\n@click.command()\n@click.option(\n    \"--text\",\n    type=str,\n    default=\"你说的对, 但是原神是一款由米哈游自主研发的开放世界手游.\",\n)\n@click.option(\"--prompt-text\", type=str, default=None, multiple=True)\n@click.option(\n    \"--prompt-tokens\",\n    type=click.Path(path_type=Path, exists=True),\n    default=None,\n    multiple=True,\n)\n@click.option(\"--num-samples\", type=int, default=1)\n@click.option(\"--max-new-tokens\", type=int, default=0)\n@click.option(\"--top-p\", type=float, default=0.7)\n@click.option(\"--repetition-penalty\", type=float, default=1.2)\n@click.option(\"--temperature\", type=float, default=0.7)\n@click.option(\n    \"--checkpoint-path\",\n    type=click.Path(path_type=Path, exists=True),\n    default=\"checkpoints/fish-speech-1.5\",\n)\n@click.option(\"--device\", type=str, default=\"cuda\")\n@click.option(\"--compile/--no-compile\", default=False)\n@click.option(\"--seed\", type=int, default=42)\n@click.option(\"--half/--no-half\", default=False)\n@click.option(\"--iterative-prompt/--no-iterative-prompt\", default=True)\n@click.option(\"--chunk-length\", type=int, default=100)\n@click.option(\"--output-dir\", type=Path, default=\"temp\")\ndef main(\n    text: str,\n    prompt_text: Optional[list[str]],\n    prompt_tokens: Optional[list[Path]],\n    num_samples: int,\n    max_new_tokens: int,\n    top_p: int,\n    repetition_penalty: float,\n    temperature: float,\n    checkpoint_path: Path,\n    device: str,\n    compile: bool,\n    seed: int,\n    half: bool,\n    iterative_prompt: bool,\n    chunk_length: int,\n    output_dir: Path,\n) -> None:\n    os.makedirs(output_dir, exist_ok=True)\n    precision = torch.half if half else torch.bfloat16\n\n    if prompt_text is not None and len(prompt_text) != len(prompt_tokens):\n        raise ValueError(\n            f\"Number of prompt text ({len(prompt_text)}) and prompt tokens ({len(prompt_tokens)}) should be the same\"\n        )\n\n    logger.info(\"Loading model ...\")\n    t0 = time.time()\n    model, decode_one_token = load_model(\n        checkpoint_path, device, precision, compile=compile\n    )\n    with torch.device(device):\n        model.setup_caches(\n            max_batch_size=1,\n            max_seq_len=model.config.max_seq_len,\n            dtype=next(model.parameters()).dtype,\n        )\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n\n    logger.info(f\"Time to load model: {time.time() - t0:.02f} seconds\")\n\n    if prompt_tokens is not None:\n        prompt_tokens = [torch.from_numpy(np.load(p)).to(device) for p in prompt_tokens]\n\n    torch.manual_seed(seed)\n\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\n    generator = generate_long(\n        model=model,\n        device=device,\n        decode_one_token=decode_one_token,\n        text=text,\n        num_samples=num_samples,\n        max_new_tokens=max_new_tokens,\n        top_p=top_p,\n        repetition_penalty=repetition_penalty,\n        temperature=temperature,\n        compile=compile,\n        iterative_prompt=iterative_prompt,\n        chunk_length=chunk_length,\n        prompt_text=prompt_text,\n        prompt_tokens=prompt_tokens,\n    )\n\n    idx = 0\n    codes = []\n\n    for response in generator:\n        if response.action == \"sample\":\n            codes.append(response.codes)\n            logger.info(f\"Sampled text: {response.text}\")\n        elif response.action == \"next\":\n            if codes:\n                codes_npy_path = os.path.join(output_dir, f\"codes_{idx}.npy\")\n                np.save(codes_npy_path, torch.cat(codes, dim=1).cpu().numpy())\n                logger.info(f\"Saved codes to {codes_npy_path}\")\n            logger.info(f\"Next sample\")\n            codes = []\n            idx += 1\n        else:\n            logger.error(f\"Error: {response}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/fish-speech/fish_speech/models/text2semantic/inference.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "generate_long_and_other_apis",
            "Description": "some APIs are implemented in this file, such as generate_long.",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/fish-speech/fish_speech/models/text2semantic/inference.py",
            "Implementation": "import os\nimport queue\nimport threading\nimport time\nfrom contextlib import nullcontext\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Literal, Optional, Tuple, Union\n\nimport click\nimport numpy as np\nimport torch\nimport torch._dynamo.config\nimport torch._inductor.config\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom fish_speech.conversation import (\n    CODEBOOK_PAD_TOKEN_ID,\n    Conversation,\n    Message,\n    TextPart,\n    VQPart,\n)\nfrom fish_speech.models.text2semantic.llama import BaseModelArgs\nfrom fish_speech.text import clean_text, split_text\nfrom fish_speech.tokenizer import IM_END_TOKEN, FishTokenizer\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.triton.unique_kernel_names = True\n\nif hasattr(torch._inductor.config, \"fx_graph_cache\"):\n    # Experimental feature to reduce compilation times, will be on by default in future\n    torch._inductor.config.fx_graph_cache = True\n\n\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\n\nfrom fish_speech.models.text2semantic.llama import (\n    BaseTransformer,\n    DualARTransformer,\n    NaiveTransformer,\n)\n\n\ndef multinomial_sample_one_no_sync(\n    probs_sort,\n):  # Does multinomial sampling without a cuda synchronization\n    q = torch.empty_like(probs_sort).exponential_(1)\n    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n\n\ndef logits_to_probs(\n    logits,\n    previous_tokens: Optional[torch.Tensor] = None,\n    temperature: torch.Tensor = 1.0,\n    top_p: torch.Tensor = 1.0,\n    repetition_penalty: torch.Tensor = 1.0,\n) -> torch.Tensor:\n    # Apply repetition penalty\n    if previous_tokens is not None:\n        previous_tokens = previous_tokens.long()\n        score = torch.gather(logits, dim=0, index=previous_tokens)\n        score = torch.where(\n            score < 0, score * repetition_penalty, score / repetition_penalty\n        )\n        logits.scatter_(dim=0, index=previous_tokens, src=score)\n\n    # Apply top-p sampling\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    cum_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n    sorted_indices_to_remove = cum_probs > top_p\n    sorted_indices_to_remove[0] = False  # keep at least one option\n    indices_to_remove = sorted_indices_to_remove.scatter(\n        dim=0, index=sorted_indices, src=sorted_indices_to_remove\n    )\n    logits = logits.masked_fill(indices_to_remove, -float(\"Inf\"))\n\n    logits = logits / max(temperature, 1e-5)\n\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    return probs\n\n\ndef multinomial_sample_one_no_sync_agent(\n    probs_sort,\n):  # Does multinomial sampling without a cuda synchronization\n    q = torch.empty_like(probs_sort).exponential_(1)\n    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n\n\ndef logits_to_probs_agent(\n    logits,\n    previous_tokens: Optional[torch.Tensor] = None,\n    temperature: torch.Tensor = 1.0,\n    top_p: torch.Tensor = 1.0,\n    repetition_penalty: torch.Tensor = 1.0,\n) -> torch.Tensor:\n    # Apply repetition penalty\n    if previous_tokens is not None:\n        previous_tokens = previous_tokens.long()\n        score = torch.gather(logits, dim=-1, index=previous_tokens)\n        score = torch.where(\n            score < 0, score * repetition_penalty, score / repetition_penalty\n        )\n        logits.scatter_(dim=-1, index=previous_tokens, src=score)\n\n    # Apply top-p sampling\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    cum_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n    sorted_indices_to_remove = cum_probs > top_p\n    sorted_indices_to_remove[..., 0] = False  # keep at least one option\n    indices_to_remove = sorted_indices_to_remove.scatter(\n        dim=-1, index=sorted_indices, src=sorted_indices_to_remove\n    )\n    logits = logits.masked_fill(indices_to_remove, -float(\"Inf\"))\n\n    logits = logits / max(temperature, 1e-5)\n\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    return probs\n\n\ndef sample(\n    logits,\n    previous_tokens: Optional[torch.Tensor] = None,\n    **sampling_kwargs,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    probs = logits_to_probs(\n        logits=logits[0, -1], previous_tokens=previous_tokens, **sampling_kwargs\n    )\n    idx_next = multinomial_sample_one_no_sync(probs)\n    return idx_next, probs\n\n\ndef sample_agent(\n    logits,\n    previous_tokens: Optional[torch.Tensor] = None,\n    **sampling_kwargs,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    probs = logits_to_probs_agent(\n        logits=logits[:, -1], previous_tokens=previous_tokens, **sampling_kwargs\n    )\n    idx_next = multinomial_sample_one_no_sync_agent(probs)\n    return idx_next, probs\n\n\ndef decode_one_token_ar_agent(\n    model: DualARTransformer,\n    x: torch.Tensor,\n    input_pos: torch.Tensor,\n    semantic_ids: list,\n    previous_tokens: torch.Tensor = None,\n    **sampling_kwargs,\n) -> torch.Tensor:\n    # print(x, input_pos)\n    x = model.forward_generate(x, input_pos)\n    logits = x.logits  # [:, -1:]\n    hidden_states = x.hidden_states  # [:, -1:]\n\n    sampling_kwargs_main = sampling_kwargs.copy()\n    sampling_kwargs_main[\"temperature\"] = 0.1\n    sampling_kwargs_main[\"top_p\"] = 0.1\n    sampling_kwargs_main[\"repetition_penalty\"] = 1.0\n\n    codebooks = [\n        sample_agent(\n            logits,\n            previous_tokens=None,  # Disable repetition penalty for the token codebook\n            **sampling_kwargs_main,\n        )[0]\n    ]\n\n    # Cleanup the cache\n    for layer in model.fast_layers:\n        layer.attention.kv_cache.k_cache.fill_(0)\n        layer.attention.kv_cache.v_cache.fill_(0)\n\n    for codebook_idx in range(model.config.num_codebooks):\n        input_pos = torch.tensor(\n            [codebook_idx], device=hidden_states.device, dtype=torch.long\n        )\n        logits = model.forward_generate_fast(hidden_states, input_pos)\n        a = sample_agent(\n            logits,\n            previous_tokens=(\n                previous_tokens[:, codebook_idx + 1]\n                if previous_tokens is not None\n                else None\n            ),\n            **sampling_kwargs,\n        )[0]\n        hidden_states = model.fast_embeddings(a)\n        codebooks.append(a)\n\n    codebooks = torch.stack(codebooks, dim=1)\n    semantic_ids_tensor = torch.tensor(semantic_ids, device=codebooks.device)\n    codebooks[:, 1:, :] = torch.masked_fill(\n        codebooks[:, 1:, :],\n        ~torch.isin(codebooks[:, :1, :], semantic_ids_tensor),\n        CODEBOOK_PAD_TOKEN_ID,\n    )\n\n    return codebooks\n\n\ndef decode_one_token_naive_agent(\n    model: NaiveTransformer,\n    x: torch.Tensor,\n    input_pos: torch.Tensor,\n    semantic_ids: list,\n    previous_tokens: torch.Tensor = None,\n    **sampling_kwargs,\n) -> torch.Tensor:\n    x = model.forward_generate(x, input_pos)\n\n    codebooks = [\n        sample(\n            x.token_logits,\n            previous_tokens=None,  # Disable repetition penalty for the token codebook\n            **sampling_kwargs,\n        )[0]\n    ]\n\n    for i in range(model.config.num_codebooks):\n        codebooks.append(\n            sample_agent(\n                x.codebook_logits[:, :, i],\n                previous_tokens=(\n                    previous_tokens[:, i + 1] if previous_tokens is not None else None\n                ),\n                **sampling_kwargs,\n            )[0]\n        )\n\n    codebooks = torch.stack(codebooks, dim=1)\n    semantic_ids_tensor = torch.tensor(semantic_ids, device=codebooks.device)\n    codebooks[:, 1:, :] = torch.masked_fill(\n        codebooks[:, 1:, :],\n        ~torch.isin(codebooks[:, :1, :], semantic_ids_tensor),\n        CODEBOOK_PAD_TOKEN_ID,\n    )\n\n    return codebooks\n\n\ndef decode_one_token_ar(\n    model: DualARTransformer,\n    x: torch.Tensor,\n    input_pos: torch.Tensor,\n    semantic_ids: list,\n    previous_tokens: torch.Tensor = None,\n    **sampling_kwargs,\n) -> torch.Tensor:\n    x = model.forward_generate(x, input_pos)\n\n    sampling_kwargs_main = sampling_kwargs.copy()\n    # sampling_kwargs_main[\"temperature\"] = 0.1\n    # sampling_kwargs_main[\"top_p\"] = 0.1\n    # sampling_kwargs_main[\"repetition_penalty\"] = 1.0\n\n    codebooks = [\n        sample(\n            x.logits,\n            previous_tokens=(\n                previous_tokens[0] if previous_tokens is not None else None\n            ),  # Disable repetition penalty for the token codebook\n            **sampling_kwargs_main,\n        )[0]\n    ]\n\n    hidden_states = x.hidden_states\n\n    # Cleanup the cache\n    for layer in model.fast_layers:\n        layer.attention.kv_cache.k_cache.fill_(0)\n        layer.attention.kv_cache.v_cache.fill_(0)\n\n    input_pos = torch.tensor([0], device=hidden_states.device, dtype=torch.long)\n    model.forward_generate_fast(hidden_states, input_pos)\n    a = codebooks[0] - model.tokenizer.semantic_begin_id\n    a[a < 0] = 0\n    hidden_states = model.fast_embeddings(a)\n    codebooks.append(a)\n\n    for codebook_idx in range(1, model.config.num_codebooks):\n        input_pos = torch.tensor(\n            [codebook_idx], device=hidden_states.device, dtype=torch.long\n        )\n        logits = model.forward_generate_fast(hidden_states, input_pos)\n        a = sample(\n            logits,\n            previous_tokens=(\n                previous_tokens[codebook_idx + 1]\n                if previous_tokens is not None\n                else None\n            ),\n            **sampling_kwargs,\n        )[0]\n        hidden_states = model.fast_embeddings(a)\n        codebooks.append(a)\n\n    codebooks = torch.stack(codebooks, dim=0)\n    # semantic_ids_tensor = torch.tensor(semantic_ids, device=codebooks.device)\n    # codebooks[1:, :] = torch.masked_fill(\n    #     codebooks[1:, :], ~torch.isin(codebooks[:1, :], semantic_ids_tensor), CODEBOOK_PAD_TOKEN_ID\n    # )\n\n    # print(codebooks)\n    return codebooks\n\n\ndef decode_one_token_naive(\n    model: NaiveTransformer,\n    x: torch.Tensor,\n    input_pos: torch.Tensor,\n    previous_tokens: torch.Tensor = None,\n    **sampling_kwargs,\n) -> torch.Tensor:\n    x = model.forward_generate(x, input_pos)\n\n    sampling_kwargs_main = sampling_kwargs.copy()\n    sampling_kwargs_main[\"temperature\"] = 0.1\n    sampling_kwargs_main[\"top_p\"] = 0.1\n    sampling_kwargs_main[\"repetition_penalty\"] = 1.0\n\n    codebooks = [\n        sample(\n            x.logits,\n            previous_tokens=None,  # Disable repetition penalty for the token codebook\n            **sampling_kwargs_main,\n        )[0]\n    ]\n\n    for i in range(model.config.num_codebooks):\n        codebooks.append(\n            sample(\n                x.codebook_logits[:, :, i],\n                previous_tokens=(\n                    previous_tokens[i + 1] if previous_tokens is not None else None\n                ),\n                **sampling_kwargs,\n            )[0]\n        )\n\n    return torch.stack(codebooks, dim=0)\n\n\ndef decode_n_tokens(\n    model: NaiveTransformer,\n    cur_token: torch.Tensor,\n    input_pos: torch.Tensor,\n    num_new_tokens: int,\n    semantic_ids: list,\n    decode_one_token=decode_one_token_naive,\n    **sampling_kwargs,\n):\n    previous_tokens = torch.zeros(\n        (model.config.num_codebooks + 1, model.config.max_seq_len),\n        dtype=torch.int,\n        device=cur_token.device,\n    )\n\n    for i in tqdm(range(num_new_tokens)):\n        # We need to get windowed repeat penalty\n        win_size = 16\n        if i < win_size:\n            window = previous_tokens[:, :win_size]\n        else:\n            window = previous_tokens[:, i - win_size : i]\n\n        with (\n            torch.backends.cuda.sdp_kernel(\n                enable_flash=False, enable_mem_efficient=False, enable_math=True\n            )\n            if torch.cuda.is_available()\n            else nullcontext()\n        ):  # Actually better for Inductor to codegen attention here\n            next_token = decode_one_token(\n                model=model,\n                x=cur_token,\n                input_pos=input_pos,\n                previous_tokens=window,\n                semantic_ids=semantic_ids,\n                **sampling_kwargs,\n            )\n\n        input_pos += 1\n        cur_token = next_token.view(1, model.config.num_codebooks + 1, -1)\n        previous_tokens[:, i : i + 1] = next_token.view(\n            model.config.num_codebooks + 1, -1\n        )\n\n        if cur_token[0, 0, -1] == model.tokenizer.get_token_id(IM_END_TOKEN):\n            break\n\n    return previous_tokens[:, : i + 1]\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef generate(\n    *,\n    model: NaiveTransformer,\n    prompt: torch.Tensor,\n    max_new_tokens: int,\n    decode_one_token=decode_one_token_naive,\n    **sampling_kwargs,\n) -> torch.Tensor:\n    \"\"\"\n    Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n    \"\"\"\n\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = prompt.size(1)\n    # semantic_id = model.tokenizer.convert_tokens_to_ids(\"<|semantic|>\")\n    semantic_ids = [\n        model.tokenizer.get_token_id(f\"<|semantic:{i}|>\") for i in range(1024)\n    ]\n\n    if max_new_tokens:\n        if T + max_new_tokens > model.config.max_seq_len:\n            max_new_tokens = model.config.max_seq_len - T\n            logger.info(f\"Truncating max_new_tokens to {max_new_tokens}\")\n\n        T_new = T + max_new_tokens\n    else:\n        T_new = model.config.max_seq_len\n        max_new_tokens = T_new - T\n\n    device, dtype = prompt.device, prompt.dtype\n\n    codebook_dim = 1 + model.config.num_codebooks\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty(\n        (codebook_dim, model.config.max_seq_len), dtype=dtype, device=device\n    )\n    empty[:, :T] = prompt\n    seq = empty\n    input_pos = torch.arange(0, T, device=device)\n\n    # Use non-accelerated version for now, to avoid compilation overhead\n    prefill_decode = (\n        decode_one_token_naive\n        if isinstance(model, NaiveTransformer)\n        else decode_one_token_ar\n    )\n\n    next_token = prefill_decode(\n        model,\n        prompt.view(1, codebook_dim, -1),\n        input_pos,\n        semantic_ids=semantic_ids,\n        **sampling_kwargs,\n    )\n    seq[:, T : T + 1] = next_token\n\n    input_pos = torch.tensor([T], device=device, dtype=torch.int)\n    x = decode_n_tokens(\n        model,\n        next_token.view(1, codebook_dim, -1),\n        input_pos,\n        max_new_tokens - 1,\n        decode_one_token=decode_one_token,\n        semantic_ids=semantic_ids,\n        **sampling_kwargs,\n    )\n    # x = torch.cat(generated_tokens, dim=1)\n    seq = seq[:, : T + 1 + x.size(1)]\n    seq[:, T + 1 :] = x\n\n    return seq\n\n\ndef decode_n_tokens_agent(\n    model: NaiveTransformer,\n    cur_token: torch.Tensor,\n    input_pos: torch.Tensor,\n    num_new_tokens: int,\n    semantic_ids: list,\n    im_end_id: int = 4,\n    decode_one_token=decode_one_token_naive_agent,\n    early_stop_threshold: float = 0.6,\n    **sampling_kwargs,\n):\n    batch_size = cur_token.size(0)\n    previous_tokens = torch.zeros(\n        (batch_size, model.config.num_codebooks + 1, model.config.max_seq_len),\n        dtype=torch.int,\n        device=cur_token.device,\n    )\n    finished = torch.zeros(batch_size, dtype=torch.bool, device=cur_token.device)\n    finished = finished | (cur_token[:, 0, -1] == im_end_id)\n    start_time = time.time()\n\n    for i in tqdm(range(num_new_tokens), desc=\"Decoding: \", total=num_new_tokens):\n        # We need to get windowed repeat penalty\n        win_size = 16\n        if i < win_size:\n            window = previous_tokens[:, :, :win_size]\n        else:\n            window = previous_tokens[:, :, i - win_size : i]\n\n        with sdpa_kernel(\n            SDPBackend.MATH\n        ):  # Actually better for Inductor to codegen attention here\n            next_token = decode_one_token(\n                model=model,\n                x=cur_token,\n                input_pos=input_pos,\n                previous_tokens=window,\n                semantic_ids=semantic_ids,\n                **sampling_kwargs,\n            )\n\n        input_pos += 1\n        cur_token = next_token.view(batch_size, model.config.num_codebooks + 1, -1)\n        previous_tokens[:, :, i : i + 1] = next_token.view(\n            batch_size, model.config.num_codebooks + 1, -1\n        )\n\n        yield cur_token.cpu()\n\n        finished = finished | (cur_token[:, 0, -1] == im_end_id)\n        if finished.all() or (\n            0 < early_stop_threshold < 1\n            and finished.sum() >= round(batch_size * early_stop_threshold)\n        ):\n            break\n\n    total_time = time.time() - start_time\n    generated_tokens = i + 1\n    tokens_per_second = (generated_tokens / total_time) * batch_size\n    logger.info(\n        f\"Decoded {generated_tokens} x {batch_size} tokens in {total_time:.2f}s ({tokens_per_second:.2f} tokens/s)\"\n    )\n\n\n@torch.no_grad()\n@torch.inference_mode()\ndef generate_agent(\n    *,\n    model: BaseTransformer,\n    prompt: torch.Tensor,\n    max_new_tokens: int,\n    semantic_ids: list,\n    im_end_id: int = 4,\n    decode_one_token=decode_one_token_naive_agent,\n    num_samples: int = 1,\n    early_stop_threshold: float = 0.6,\n    **sampling_kwargs,\n):\n    \"\"\"\n    Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n    \"\"\"\n\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = prompt.size(1)\n    prompt = prompt[None].repeat(num_samples, 1, 1)\n\n    if T >= model.config.max_seq_len:\n        raise ValueError(\n            f\"Input sequence length {T} exceeds max_seq_len {model.config.max_seq_len}\"\n        )\n\n    if max_new_tokens:\n        if T + max_new_tokens > model.config.max_seq_len:\n            max_new_tokens = model.config.max_seq_len - T\n            logger.info(f\"Truncating max_new_tokens to {max_new_tokens}\")\n\n        T_new = T + max_new_tokens\n    else:\n        T_new = model.config.max_seq_len\n        max_new_tokens = T_new - T\n\n    device, dtype = prompt.device, prompt.dtype\n\n    codebook_dim = 1 + model.config.num_codebooks\n    input_pos = torch.arange(0, T, device=device)\n\n    # Use non-accelerated version for now, to avoid compilation overhead\n    prefill_decode = (\n        decode_one_token_naive_agent\n        if isinstance(model, NaiveTransformer)\n        else decode_one_token_ar_agent\n    )\n    next_token = prefill_decode(\n        model,\n        prompt,\n        input_pos,\n        semantic_ids=semantic_ids,\n        **sampling_kwargs,\n    ).view(num_samples, codebook_dim, -1)\n    yield next_token.cpu()\n\n    input_pos = torch.tensor([T], device=device, dtype=torch.int)\n\n    yield from decode_n_tokens_agent(\n        model,\n        next_token,\n        input_pos,\n        max_new_tokens - 1,\n        im_end_id=im_end_id,\n        semantic_ids=semantic_ids,\n        decode_one_token=decode_one_token,\n        early_stop_threshold=early_stop_threshold,\n        **sampling_kwargs,\n    )\n\n\ndef encode_tokens(\n    tokenizer,\n    string,\n    device=\"cuda\",\n    prompt_tokens=None,\n    num_codebooks=4,\n):\n    string = clean_text(string)\n\n    messages = []\n    messages.append(\n        Message(\n            role=\"user\",\n            parts=[TextPart(text=string)],\n            cal_loss=False,\n        )\n    )\n\n    if prompt_tokens is not None:\n        if prompt_tokens.ndim == 3:\n            assert (\n                prompt_tokens.shape[0] == 1\n            ), \"3D prompt tokens should have shape (1, num_codebooks, seq_len)\"\n            prompt_tokens = prompt_tokens[0]\n\n        assert prompt_tokens.ndim == 2, \"Prompt tokens should be 2D tensor\"\n\n        if prompt_tokens.shape[0] > num_codebooks:\n            logger.warning(\n                f\"Prompt tokens shape {prompt_tokens.shape} is larger than num_codebooks {num_codebooks}, getting first {num_codebooks} codebooks\"\n            )\n            prompt_tokens = prompt_tokens[:num_codebooks]\n\n        vq_part = VQPart(codes=prompt_tokens.to(device))\n\n        messages.append(\n            Message(\n                role=\"assistant\",\n                parts=[TextPart(text=\"<|voice|>\"), vq_part],\n                cal_loss=False,\n            )\n        )\n    else:\n        messages.append(\n            Message(\n                role=\"assistant\",\n                parts=[TextPart(text=\"<|voice|>\")],\n                cal_loss=False,\n                add_im_end=False,\n            )\n        )\n\n    conversation = Conversation(messages=messages)\n    # conversation.visualize(tokenizer)\n    encoded = conversation.encode_for_inference(\n        tokenizer=tokenizer,\n        num_codebooks=num_codebooks,\n    )\n\n    return encoded.to(device)\n\n\ndef load_model(checkpoint_path, device, precision, compile=False, is_agent=False):\n    model: Union[NaiveTransformer, DualARTransformer] = BaseTransformer.from_pretrained(\n        checkpoint_path, load_weights=True, is_agent=is_agent\n    )\n\n    model = model.to(device=device, dtype=precision)\n    logger.info(f\"Restored model from checkpoint\")\n\n    if isinstance(model, DualARTransformer):\n        decode_one_token = (\n            decode_one_token_ar_agent if is_agent else decode_one_token_ar\n        )\n        logger.info(\"Using DualARTransformer\")\n    else:\n        decode_one_token = (\n            decode_one_token_naive_agent if is_agent else decode_one_token_naive\n        )\n        logger.info(\"Using NaiveTransformer\")\n\n    if compile:\n        logger.info(\"Compiling function...\")\n        decode_one_token = torch.compile(\n            decode_one_token,\n            fullgraph=True,\n            backend=\"inductor\" if torch.cuda.is_available() else \"aot_eager\",\n            mode=\"reduce-overhead\" if torch.cuda.is_available() else None,\n        )\n\n    return model.eval(), decode_one_token\n\n\n@dataclass\nclass GenerateResponse:\n    action: Literal[\"sample\", \"next\"]\n    codes: Optional[torch.Tensor] = None\n    text: Optional[str] = None\n\n\ndef generate_long(\n    *,\n    model,\n    device: str | torch.device,\n    decode_one_token: callable,\n    text: str,\n    num_samples: int = 1,\n    max_new_tokens: int = 0,\n    top_p: int = 0.7,\n    repetition_penalty: float = 1.5,\n    temperature: float = 0.7,\n    compile: bool = False,\n    iterative_prompt: bool = True,\n    max_length: int = 2048,\n    chunk_length: int = 150,\n    prompt_text: Optional[str | list[str]] = None,\n    prompt_tokens: Optional[torch.Tensor | list[torch.Tensor]] = None,\n):\n    assert 0 < top_p <= 1, \"top_p must be in (0, 1]\"\n    assert 0 < repetition_penalty < 2, \"repetition_penalty must be in (0, 2)\"\n    assert 0 < temperature < 2, \"temperature must be in (0, 2)\"\n\n    use_prompt = prompt_text is not None and prompt_tokens is not None\n    if use_prompt and isinstance(prompt_text, str):\n        prompt_text = [prompt_text]\n        prompt_tokens = [prompt_tokens]\n\n    assert use_prompt is False or len(prompt_text) == len(\n        prompt_tokens\n    ), \"Prompt text and tokens must have the same length\"\n\n    model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    tokenizer = model.tokenizer\n    im_end_id = tokenizer.get_token_id(\"<|im_end|>\")\n\n    encoded = []\n    texts = split_text(text, chunk_length) if iterative_prompt else [text]\n    encoded_prompts = [\n        Conversation(\n            messages=[\n                Message(\n                    role=\"system\",\n                    parts=[TextPart(text=\"Speak out the provided text.\")],\n                    cal_loss=False,\n                )\n            ]\n        )\n        .encode_for_inference(\n            tokenizer=tokenizer,\n            num_codebooks=model.config.num_codebooks,\n        )\n        .to(device)\n    ]\n\n    if use_prompt:\n        for idx, (t, c) in enumerate(zip(prompt_text, prompt_tokens)):\n            encoded_prompts.append(\n                encode_tokens(\n                    tokenizer,\n                    string=t,\n                    device=device,\n                    prompt_tokens=c,\n                    num_codebooks=model.config.num_codebooks,\n                )\n            )\n\n    for idx, text in enumerate(texts):\n        encoded.append(\n            encode_tokens(\n                tokenizer,\n                string=text,\n                device=device,\n                num_codebooks=model.config.num_codebooks,\n            )\n        )\n        logger.info(f\"Encoded text: {text}\")\n\n    # Move temperature, top_p, repetition_penalty to device\n    # This is important so that changing params doesn't trigger recompile\n    temperature = torch.tensor(temperature, device=device, dtype=torch.float)\n    top_p = torch.tensor(top_p, device=device, dtype=torch.float)\n    repetition_penalty = torch.tensor(\n        repetition_penalty, device=device, dtype=torch.float\n    )\n\n    for sample_idx in range(num_samples):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n        global_encoded = []\n        seg_idx = 0\n\n        while seg_idx < len(encoded):\n            logger.info(\n                f\"Generating sentence {seg_idx + 1}/{len(encoded)} of sample {sample_idx + 1}/{num_samples}\"\n            )\n\n            seg = encoded[seg_idx]\n            global_encoded.append(seg)\n\n            lengths = reversed([seg.size(1) for seg in global_encoded])\n\n            # Pick last 2000 tokens\n            count = 0\n            for i, length in enumerate(lengths):\n                count += length\n                if count + length > max_length - 1024 - sum(\n                    t.shape[1] for t in encoded_prompts\n                ):\n                    break\n\n            if i != 0 and i % 2 == 0:\n                i -= 1\n\n            # Rotate the list, always make sure first segment is included to avoid drift\n            if i < len(global_encoded) - 2:\n                partial_encoded = global_encoded[:2] + global_encoded[-i:]\n            else:\n                partial_encoded = global_encoded\n\n            if use_prompt:\n                partial_encoded = encoded_prompts + partial_encoded\n\n            cat_encoded = torch.cat(partial_encoded, dim=1)\n            prompt_length = cat_encoded.size(1)\n\n            t0 = time.perf_counter()\n            y = generate(\n                model=model,\n                prompt=cat_encoded,\n                max_new_tokens=max_new_tokens,\n                decode_one_token=decode_one_token,\n                temperature=temperature,\n                top_p=top_p,\n                repetition_penalty=repetition_penalty,\n            )\n\n            if sample_idx == 0 and seg_idx == 0 and compile:\n                logger.info(f\"Compilation time: {time.perf_counter() - t0:.2f} seconds\")\n\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n\n            t = time.perf_counter() - t0\n\n            tokens_generated = y.size(1) - prompt_length\n            tokens_sec = tokens_generated / t\n            logger.info(\n                f\"Generated {tokens_generated} tokens in {t:.02f} seconds, {tokens_sec:.02f} tokens/sec\"\n            )\n            logger.info(\n                f\"Bandwidth achieved: {model_size * tokens_sec / 1e9:.02f} GB/s\"\n            )\n\n            if torch.cuda.is_available():\n                logger.info(\n                    f\"GPU Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\"\n                )\n\n            # Put the generated tokens\n            # since there is <im_end>, we remove last token\n            codes = y[1:, prompt_length + 1 :].clone()\n            assert (codes >= 0).all(), f\"Negative code found\"\n\n            decoded = y[:, prompt_length:].clone()\n            # But for global encoding, we should keep the <im_end> token\n\n            global_encoded.append(decoded)\n            assert (codes >= 0).all(), f\"Negative code found: {codes}\"\n            yield GenerateResponse(action=\"sample\", codes=codes, text=texts[seg_idx])\n            seg_idx += 1\n\n        # This indicates the end of the current sample\n        yield GenerateResponse(action=\"next\")\n\n\n@dataclass\nclass WrappedGenerateResponse:\n    status: Literal[\"success\", \"error\"]\n    response: Optional[GenerateResponse | Exception] = None\n\n\n@dataclass\nclass GenerateRequest:\n    request: dict\n    response_queue: queue.Queue\n\n\ndef launch_thread_safe_queue(\n    checkpoint_path,\n    device,\n    precision,\n    compile: bool = False,\n):\n    input_queue = queue.Queue()\n    init_event = threading.Event()\n\n    def worker():\n        model, decode_one_token = load_model(\n            checkpoint_path, device, precision, compile=compile\n        )\n        with torch.device(device):\n            model.setup_caches(\n                max_batch_size=1,\n                max_seq_len=model.config.max_seq_len,\n                dtype=next(model.parameters()).dtype,\n            )\n        init_event.set()\n\n        while True:\n            item: GenerateRequest | None = input_queue.get()\n            if item is None:\n                break\n\n            kwargs = item.request\n            response_queue = item.response_queue\n\n            try:\n                for chunk in generate_long(\n                    model=model, decode_one_token=decode_one_token, **kwargs\n                ):\n                    response_queue.put(\n                        WrappedGenerateResponse(status=\"success\", response=chunk)\n                    )\n            except Exception as e:\n                response_queue.put(WrappedGenerateResponse(status=\"error\", response=e))\n\n    threading.Thread(target=worker, daemon=True).start()\n    init_event.wait()\n\n    return input_queue\n\n\ndef launch_thread_safe_queue_agent(\n    checkpoint_path,\n    device,\n    precision,\n    compile: bool = False,\n):\n    input_queue = queue.Queue()\n    init_event = threading.Event()\n\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n    config = BaseModelArgs.from_pretrained(checkpoint_path)\n\n    def worker():\n        model, decode_one_token = load_model(\n            checkpoint_path, device, precision, compile=compile, is_agent=True\n        )\n\n        with torch.device(device):\n            model.setup_caches(\n                max_batch_size=1,\n                max_seq_len=model.config.max_seq_len,\n                dtype=next(model.parameters()).dtype,\n            )\n        init_event.set()\n\n        while True:\n            item: GenerateRequest | None = input_queue.get()\n            if item is None:\n                break\n\n            kwargs = item.request\n            response_queue = item.response_queue\n\n            try:\n                for token in generate_agent(\n                    model=model,\n                    decode_one_token=decode_one_token,\n                    **kwargs,\n                ):\n                    response_queue.put(token)\n\n                response_queue.put(\"stop\")\n            except Exception as e:\n                import traceback\n\n                logger.exception(f\"Error in worker: {traceback.format_exc()}\")\n                response_queue.put(\"error\")\n\n    threading.Thread(target=worker, daemon=True).start()\n    init_event.wait()\n\n    return input_queue, tokenizer, config\n\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}