{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/VideoLingo",
    "API_Calls": [
        {
            "Name": "call_ask_gpt",
            "Description": "translate_lines call ask_gpt to translate lines of text in two steps: first for faithfulness, then for expressiveness.",
            "Code": "from core.prompts import generate_shared_prompt, get_prompt_faithfulness, get_prompt_expressiveness\nfrom rich.panel import Panel\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich import box\nfrom core.utils import *\nconsole = Console()\n\ndef valid_translate_result(result: dict, required_keys: list, required_sub_keys: list):\n    # Check for the required key\n    if not all(key in result for key in required_keys):\n        return {\"status\": \"error\", \"message\": f\"Missing required key(s): {', '.join(set(required_keys) - set(result.keys()))}\"}\n    \n    # Check for required sub-keys in all items\n    for key in result:\n        if not all(sub_key in result[key] for sub_key in required_sub_keys):\n            return {\"status\": \"error\", \"message\": f\"Missing required sub-key(s) in item {key}: {', '.join(set(required_sub_keys) - set(result[key].keys()))}\"}\n\n    return {\"status\": \"success\", \"message\": \"Translation completed\"}\n\ndef translate_lines(lines, previous_content_prompt, after_content_prompt, things_to_note_prompt, summary_prompt, index = 0):\n    shared_prompt = generate_shared_prompt(previous_content_prompt, after_content_prompt, summary_prompt, things_to_note_prompt)\n\n    # Retry translation if the length of the original text and the translated text are not the same, or if the specified key is missing\n    def retry_translation(prompt, length, step_name):\n        def valid_faith(response_data):\n            return valid_translate_result(response_data, [str(i) for i in range(1, length+1)], ['direct'])\n        def valid_express(response_data):\n            return valid_translate_result(response_data, [str(i) for i in range(1, length+1)], ['free'])\n        for retry in range(3):\n            if step_name == 'faithfulness':\n                result = ask_gpt(prompt+retry* \" \", resp_type='json', valid_def=valid_faith, log_title=f'translate_{step_name}')\n            elif step_name == 'expressiveness':\n                result = ask_gpt(prompt+retry* \" \", resp_type='json', valid_def=valid_express, log_title=f'translate_{step_name}')\n            if len(lines.split('\\n')) == len(result):\n                return result\n            if retry != 2:\n                console.print(f'[yellow]⚠️ {step_name.capitalize()} translation of block {index} failed, Retry...[/yellow]')\n        raise ValueError(f'[red]❌ {step_name.capitalize()} translation of block {index} failed after 3 retries. Please check `output/gpt_log/error.json` for more details.[/red]')\n\n    ## Step 1: Faithful to the Original Text\n    prompt1 = get_prompt_faithfulness(lines, shared_prompt)\n    faith_result = retry_translation(prompt1, len(lines.split('\\n')), 'faithfulness')\n\n    for i in faith_result:\n        faith_result[i][\"direct\"] = faith_result[i][\"direct\"].replace('\\n', ' ')\n\n    # If reflect_translate is False or not set, use faithful translation directly\n    reflect_translate = load_key('reflect_translate')\n    if not reflect_translate:\n        # If reflect_translate is False or not set, use faithful translation directly\n        translate_result = \"\\n\".join([faith_result[i][\"direct\"].strip() for i in faith_result])\n        \n        table = Table(title=\"Translation Results\", show_header=False, box=box.ROUNDED)\n        table.add_column(\"Translations\", style=\"bold\")\n        for i, key in enumerate(faith_result):\n            table.add_row(f\"[cyan]Origin:  {faith_result[key]['origin']}[/cyan]\")\n            table.add_row(f\"[magenta]Direct:  {faith_result[key]['direct']}[/magenta]\")\n            if i < len(faith_result) - 1:\n                table.add_row(\"[yellow]\" + \"-\" * 50 + \"[/yellow]\")\n        \n        console.print(table)\n        return translate_result, lines\n\n    ## Step 2: Express Smoothly  \n    prompt2 = get_prompt_expressiveness(faith_result, lines, shared_prompt)\n    express_result = retry_translation(prompt2, len(lines.split('\\n')), 'expressiveness')\n\n    table = Table(title=\"Translation Results\", show_header=False, box=box.ROUNDED)\n    table.add_column(\"Translations\", style=\"bold\")\n    for i, key in enumerate(express_result):\n        table.add_row(f\"[cyan]Origin:  {faith_result[key]['origin']}[/cyan]\")\n        table.add_row(f\"[magenta]Direct:  {faith_result[key]['direct']}[/magenta]\")\n        table.add_row(f\"[green]Free:    {express_result[key]['free']}[/green]\")\n        if i < len(express_result) - 1:\n            table.add_row(\"[yellow]\" + \"-\" * 50 + \"[/yellow]\")\n\n    console.print(table)\n\n    translate_result = \"\\n\".join([express_result[i][\"free\"].replace('\\n', ' ').strip() for i in express_result])\n\n    if len(lines.split('\\n')) != len(translate_result.split('\\n')):\n        console.print(Panel(f'[red]❌ Translation of block {index} failed, Length Mismatch, Please check `output/gpt_log/translate_expressiveness.json`[/red]'))\n        raise ValueError(f'Origin ···{lines}···,\\nbut got ···{translate_result}···')\n\n    return translate_result, lines\n\n\nif __name__ == '__main__':\n    # test e.g.\n    lines = '''All of you know Andrew Ng as a famous computer science professor at Stanford.\nHe was really early on in the development of neural networks with GPUs.\nOf course, a creator of Coursera and popular courses like deeplearning.ai.\nAlso the founder and creator and early lead of Google Brain.'''\n    previous_content_prompt = None\n    after_cotent_prompt = None\n    things_to_note_prompt = None\n    summary_prompt = None\n    translate_lines(lines, previous_content_prompt, after_cotent_prompt, things_to_note_prompt, summary_prompt)",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/VideoLingo/core/translate_lines.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "ask_gpt",
            "Description": "a ask gpt interface , get response from gpt",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/VideoLingo/core/utils/ask_gpt.py",
            "Implementation": "import os\nimport json\nfrom threading import Lock\nimport json_repair\nfrom openai import OpenAI\nfrom core.utils.config_utils import load_key\nfrom rich import print as rprint\nfrom core.utils.decorator import except_handler\n\n# ------------\n# cache gpt response\n# ------------\n\nLOCK = Lock()\nGPT_LOG_FOLDER = 'output/gpt_log'\n\ndef _save_cache(model, prompt, resp_content, resp_type, resp, message=None, log_title=\"default\"):\n    with LOCK:\n        logs = []\n        file = os.path.join(GPT_LOG_FOLDER, f\"{log_title}.json\")\n        os.makedirs(os.path.dirname(file), exist_ok=True)\n        if os.path.exists(file):\n            with open(file, 'r', encoding='utf-8') as f:\n                logs = json.load(f)\n        logs.append({\"model\": model, \"prompt\": prompt, \"resp_content\": resp_content, \"resp_type\": resp_type, \"resp\": resp, \"message\": message})\n        with open(file, 'w', encoding='utf-8') as f:\n            json.dump(logs, f, ensure_ascii=False, indent=4)\n\ndef _load_cache(prompt, resp_type, log_title):\n    with LOCK:\n        file = os.path.join(GPT_LOG_FOLDER, f\"{log_title}.json\")\n        if os.path.exists(file):\n            with open(file, 'r', encoding='utf-8') as f:\n                for item in json.load(f):\n                    if item[\"prompt\"] == prompt and item[\"resp_type\"] == resp_type:\n                        return item[\"resp\"]\n        return False\n\n# ------------\n# ask gpt once\n# ------------\n\n@except_handler(\"GPT request failed\", retry=5)\ndef ask_gpt(prompt, resp_type=None, valid_def=None, log_title=\"default\"):\n    if not load_key(\"api.key\"):\n        raise ValueError(\"API key is not set\")\n    # check cache\n    cached = _load_cache(prompt, resp_type, log_title)\n    if cached:\n        rprint(\"use cache response\")\n        return cached\n\n    model = load_key(\"api.model\")\n    base_url = load_key(\"api.base_url\")\n    if 'ark' in base_url:\n        base_url = \"https://ark.cn-beijing.volces.com/api/v3\" # huoshan base url\n    elif 'v1' not in base_url:\n        base_url = base_url.strip('/') + '/v1'\n    client = OpenAI(api_key=load_key(\"api.key\"), base_url=base_url)\n    response_format = {\"type\": \"json_object\"} if resp_type == \"json\" and load_key(\"api.llm_support_json\") else None\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    params = dict(\n        model=model,\n        messages=messages,\n        response_format=response_format,\n        timeout=300\n    )\n    resp_raw = client.chat.completions.create(**params)\n\n    # process and return full result\n    resp_content = resp_raw.choices[0].message.content\n    if resp_type == \"json\":\n        resp = json_repair.loads(resp_content)\n    else:\n        resp = resp_content\n    \n    # check if the response format is valid\n    if valid_def:\n        valid_resp = valid_def(resp)\n        if valid_resp['status'] != 'success':\n            _save_cache(model, prompt, resp_content, resp_type, resp, log_title=\"error\", message=valid_resp['message'])\n            raise ValueError(f\"❎ API response error: {valid_resp['message']}\")\n\n    _save_cache(model, prompt, resp_content, resp_type, resp, log_title=log_title)\n    return resp\n\n\nif __name__ == '__main__':\n    from rich import print as rprint\n    \n    result = ask_gpt(\"\"\"test respond ```json\\n{\\\"code\\\": 200, \\\"message\\\": \\\"success\\\"}\\n```\"\"\", resp_type=\"json\")\n    rprint(f\"Test json output result: {result}\")\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}