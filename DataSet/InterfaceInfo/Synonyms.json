{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/Synonyms",
    "API_Calls": [
        {
            "Name": "call_compare",
            "Description": "call_compare",
            "Code": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#=========================================================================\n#\n# Copyright (c) 2017 <> All Rights Reserved\n#\n#\n# File: /Users/hain/ai/Synonyms/demo.py\n# Author: Hai Liang Wang\n# Date: 2017-09-28:22:23:34\n#\n#=========================================================================\n\n\"\"\"\n\n\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\n\n__copyright__ = \"Copyright (c) (2017-2022) Chatopera Inc. All Rights Reserved\"\n__author__ = \"Hai Liang Wang\"\n__date__ = \"2017-09-28:22:23:34\"\n\n\nimport os\nimport sys\ncurdir = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, curdir)\n\nif sys.version_info[0] < 3:\n    reload(sys)\n    sys.setdefaultencoding(\"utf-8\")\n    # raise \"Must be using Python 3\"\n    # \n\nimport synonyms  # https://github.com/chatopera/Synonyms\nimport numpy\nimport unittest\n\ncompare_ = lambda x,y,z: \"%s vs %s: %f\" % (x, y, synonyms.compare(x, y, seg=z)) + \"\\n\" +\"*\"* 30 + \"\\n\"\n\n# run testcase: python /Users/hain/ai/Synonyms/demo.py Test.testExample\nclass Test(unittest.TestCase):\n    '''\n\n    '''\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_wordseg(self):\n        print(\"test_wordseg\")\n        print(synonyms.seg(\"中文近义词工具包\"))\n\n\n    def test_word_vector(self):\n        print(\"test_word_vector\")\n        word = \"三国\"\n        print(word, \"向量\", synonyms.v(word))\n\n    def test_diff(self):\n        print(\"test_diff\")\n        result = []\n        # 30个  评测词对中的左侧词\n        left = ['轿车', '宝石', '旅游', '男孩子', '海岸', '庇护所', '魔术师', '中午', '火炉', '食物', '鸟', '鸟', '工具', '兄弟', '起重机', '小伙子',\n                '旅行', '和尚', '墓地', '食物', '海岸', '森林', '岸边', '和尚', '海岸', '小伙子', '琴弦', '玻璃', '中午', '公鸡']\n        # 30个  评测词对中的右侧词\n        right = ['汽车', '宝物', '游历', '小伙子', '海滨', '精神病院', '巫师', '正午', '炉灶', '水果', '公鸡', '鹤', '器械', '和尚', '器械', '兄弟',\n                 '轿车', '圣贤', '林地', '公鸡', '丘陵', '墓地', '林地', '奴隶', '森林', '巫师', '微笑', '魔术师', '绳子', '航行']\n        # 人工评定的相似度列表。\n        human = [0.98, 0.96, 0.96, 0.94, 0.925, 0.9025, 0.875, 0.855, 0.7775, 0.77, 0.7625, 0.7425, 0.7375, 0.705, 0.42, 0.415,\n                 0.29, 0.275, 0.2375,\n                 0.2225, 0.2175, 0.21, 0.1575, 0.1375, 0.105, 0.105, 0.0325, 0.0275, 0.02, 0.02]\n        result.append(\"# synonyms 分数评测 [(v%s)](https://pypi.python.org/pypi/synonyms/%s)\" % (synonyms.__version__, synonyms.__version__))\n        result.append(\"| %s |  %s |   %s  |  %s |\" % (\"词1\", \"词2\", \"synonyms\", \"人工评定\"))\n        result.append(\"| --- | --- | --- | --- |\")\n        for x,y,z in zip(left, right, human):\n            result.append(\"| %s | %s | %s  |  %s |\" % (x, y, synonyms.compare(x, y), z))\n        for x in result: print(x)\n        with open(os.path.join(curdir, \"VALUATION.md\"), \"w\") as fout:\n            for x in result: fout.write(x + \"\\n\")\n\n    def test_similarity(self):\n        '''\n        Generate sentence similarity\n        '''\n        sen1 = \"旗帜引领方向\"\n        sen2 = \"道路决定命运\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n        print(\"旗帜引领方向 vs 道路决定命运:\", r)\n        # assert r == 0.0, \"the similarity should be zero\"\n\n        sen1 = \"旗帜引领方向\"\n        sen2 = \"旗帜指引道路\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n        print(\"旗帜引领方向 vs 旗帜指引道路:\", r)\n        # assert r > 0, \"the similarity should be bigger then zero\"\n\n        sen1 = \"发生历史性变革\"\n        sen2 = \"发生历史性变革\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n        print(\"发生历史性变革 vs 发生历史性变革:\", r)\n        # assert r > 0, \"the similarity should be bigger then zero\"\n\n        sen1 = \"骨折\"\n        sen2 = \"巴赫\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n        print(\"%s vs %s\" % (sen1, sen2), r)\n\n\n        sen1 = \"你们好呀\"\n        sen2 = \"大家好\"\n        r = synonyms.compare(sen1, sen2, seg=False)\n        print(\"%s vs %s\" % (sen1, sen2), r)\n\n\n    def test_swap_sent(self):\n        print(\"test_swap_sent\")        \n        s1 = synonyms.compare(\"教学\", \"老师\")\n        s2 = synonyms.compare(\"老师\", \"教学\")\n        print('\"教学\", \"老师\": %s ' % s1)\n        print('\"老师\", \"教学\": %s ' % s2)\n        assert s1 == s2, \"Scores should be the same after swap sents\"\n\n    def test_nearby(self):\n        synonyms.display(\"奥运\")  # synonyms.display calls synonyms.nearby\n        synonyms.display(\"北新桥\")  # synonyms.display calls synonyms.nearby\n\n\n    def test_badcase_1(self):\n        synonyms.display(\"人脸\")  # synonyms.display calls synonyms.nearby\n\n\n    def test_basecase_2(self):\n        print(\"test_basecase_2\")\n        sen1 = \"今天天气\"\n        sen2 = \"今天天气怎么样\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n\n\n    def test_analyse_extract_tags(self):\n        '''\n        使用 Tag 方式获得关键词\n        https://github.com/fxsjy/jieba/tree/v0.39\n        '''\n        sentence = \"华为芯片被断供，源于美国关于华为的修订版禁令生效——9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。\"\n        keywords = synonyms.keywords(sentence, topK=5, withWeight=False, allowPOS=())\n        print(\"[test_analyse_extract_tags] keywords %s\" % keywords)\n\ndef test():\n    unittest.main()\n\n\nif __name__ == '__main__':\n    test()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Synonyms/demo.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "compare",
            "Description": "compare",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Synonyms/synonyms/synonyms.py",
            "Implementation": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#=========================================================================\n#\n# File: /Users/hain/ai/Synonyms/synonyms/__init__.py\n# Author: Hai Liang Wang\n# Date: 2017-09-27\n#\n#=========================================================================\n\n\"\"\"\nChinese Synonyms for Natural Language Processing and Understanding.\n\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\n\n__copyright__ = \"Copyright (c) (2017-2023) Chatopera Inc. All Rights Reserved\"\n__author__ = \"Hu Ying Xi<>, Hai Liang Wang<hai@chatopera.com>\"\n__date__ = \"2020-09-24\"\n__version__ = \"3.23.6\"\n\nimport os\nimport sys\nimport numpy as np\ncurdir = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, curdir)\n\nPLT = 2\n\nif sys.version_info[0] < 3:\n    default_stdout = sys.stdout\n    default_stderr = sys.stderr\n    reload(sys)\n    sys.stdout = default_stdout\n    sys.stderr = default_stderr\n    sys.setdefaultencoding(\"utf-8\")\n    # raise \"Must be using Python 3\"\nelse:\n    PLT = 3\n\n# Get Environment variables\nENVIRON = os.environ.copy()\n\nimport json\nimport gzip\nimport shutil\nfrom .word2vec import KeyedVectors\nfrom .utils import any2utf8\nfrom .utils import any2unicode\nfrom .utils import sigmoid\nfrom .utils import cosine\nfrom .utils import is_digit\nfrom jieba import posseg, analyse\nfrom chatoperastore import download_licensedfile, LicensedfileDownloadException\n\n'''\nglobals\n'''\n_vocab = dict()\n_size = 0\n_vectors = None\n_stopwords = set()\n_cache_nearby = dict()\n_debug = False\n\nif \"SYNONYMS_DEBUG\" in ENVIRON:\n    if ENVIRON[\"SYNONYMS_DEBUG\"].lower() == \"true\": _debug = True\n\n'''\nlambda fns\n'''\n# combine similarity scores\n_similarity_smooth = lambda x, y, z, u: (x * y) + z - u\n_flat_sum_array = lambda x: np.sum(x, axis=0)  # 分子\n_logging_debug = lambda x: print(\">> Synonyms DEBUG %s\" % x) if _debug else None\n\n'''\nSponsorship\n'''\nprint(\"\\n Synonyms: v%s, Project home: %s\" % (__version__, \"https://github.com/chatopera/Synonyms/\"))\nprint(\"\\n Project Sponsored by Chatopera\")\nprint(\"\\n  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com\\n\")\nprint(\"\\n Module file path: %s\" % __file__)\nprint(\"\\n ************ NOTICE ************\")\nprint(\"  Require license to download model package, purchase from https://store.chatopera.com/product/syns001\")\nprint(\" ********************************\\n\")\n\n'''\ntokenizer settings\n'''\ntokenizer_dict = os.path.join(curdir, 'data', 'vocab.txt')\nif \"SYNONYMS_WORDSEG_DICT\" in ENVIRON:\n    if os.path.exists(ENVIRON[\"SYNONYMS_WORDSEG_DICT\"]):\n        print(\"info: set wordseg dict with %s\" % tokenizer_dict)\n        tokenizer_dict = ENVIRON[\"SYNONYMS_WORDSEG_DICT\"]\n    else: print(\"warning: can not find dict at [%s]\" % tokenizer_dict)\n\nprint(\">> Synonyms load wordseg dict [%s] ... \" % tokenizer_dict)\nposseg.initialize(tokenizer_dict)\n\n# stopwords\n_fin_stopwords_path = os.path.join(curdir, 'data', 'stopwords.txt')\ndef _load_stopwords(file_path):\n    '''\n    load stop words\n    '''\n    global _stopwords\n    if sys.version_info[0] < 3:\n        words = open(file_path, 'r')\n    else:\n        words = open(file_path, 'r', encoding='utf-8')\n    stopwords = words.readlines()\n    for w in stopwords:\n        _stopwords.add(any2unicode(w).strip())\n\n    words.close()\n\nprint(\">> Synonyms on loading stopwords [%s] ...\" % _fin_stopwords_path)\n_load_stopwords(_fin_stopwords_path)\n\ndef _segment_words(sen, HMM=True):\n    '''\n    segment words\n    '''\n    words, tags = [], []\n    m = posseg.cut(sen, HMM=HMM)  # HMM更好的识别新词\n    for x in m:\n        words.append(x.word)\n        tags.append(x.flag)\n    return words, tags\n\ndef keywords(sentence, topK=5, withWeight=False, allowPOS=()):\n    '''\n    extract keywords with Jieba Tokenizer\n    '''\n    return analyse.extract_tags(sentence, topK=topK, withWeight=withWeight, allowPOS=allowPOS)\n\n'''\nword embedding\n'''\n# vectors\n_licenseid = os.environ.get(\"SYNONYMS_DL_LICENSE\", None)\n_f_model = os.path.join(curdir, 'data', 'words.vector.gz')\n_download_model = not os.path.exists(_f_model)\n\nif \"SYNONYMS_WORD2VEC_BIN_MODEL_ZH_CN\" in ENVIRON:\n    _f_model = ENVIRON[\"SYNONYMS_WORD2VEC_BIN_MODEL_ZH_CN\"]\n    _download_model = False\n\ndef _load_w2v(model_file=_f_model, binary=True):\n    '''\n    load word2vec model\n    '''\n    if not os.path.exists(model_file) and _download_model:\n        if not _licenseid:\n            raise Exception(\"SYNONYMS_DL_LICENSE is not in Environment variables, check out Installation Guide on https://github.com/chatopera/Synonyms\")\n\n        print(\"\\n>> Synonyms downloading data with licenseId %s, save to %s ... \\n this only happens if Synonyms initialization for the first time. \\n It would take minutes that depends on network.\" % (_licenseid, model_file))\n        download_licensedfile(_licenseid, model_file)\n        dl_file_size = os.path.getsize(model_file)\n        min_file_size = 40900000 # ~ 40MB\n\n        if dl_file_size < min_file_size:\n            os.remove(model_file)\n            raise Exception(\"Download File Error, please read the installation guide on https://github.com/chatopera/Synonyms, reach out for help with info@chatopera.com by describing the problem and procedures.\")\n\n        print(\"\\n>> Synonyms downloaded\\n\")\n\n    elif not os.path.exists(model_file):\n        print(\">> Synonyms os.path : \", os.path)\n        raise Exception(\"Model file [%s] does not exist.\" % model_file)\n\n    return KeyedVectors.load_word2vec_format(\n        model_file, binary=binary, unicode_errors='ignore')\nprint(\">> Synonyms on loading vectors [%s] ...\" % _f_model)\n_vectors = _load_w2v(model_file=_f_model)\n\ndef _get_wv(sentence, ignore=False):\n    '''\n    get word2vec data by sentence\n    sentence is segmented string.\n    '''\n    global _vectors\n    vectors = []\n    for y in sentence:\n        y_ = any2unicode(y).strip()\n        if y_ not in _stopwords:\n            syns = nearby(y_)[0]\n            _logging_debug(\"sentence %s word: %s\" %(sentence, y_))\n            _logging_debug(\"sentence %s word nearby: %s\" %(sentence, \" \".join(syns)))\n            c = []\n            try:\n                c.append(_vectors.word_vec(y_))\n            except KeyError as error:\n                if ignore:\n                    continue\n                else:\n                    _logging_debug(\"not exist in w2v model: %s\" % y_)\n                    # c.append(np.zeros((100,), dtype=float))\n                    random_state = np.random.RandomState(seed=(hash(y_) % (2**32 - 1)))\n                    c.append(random_state.uniform(low=-10.0, high=10.0, size=(100,)))\n            for n in syns:\n                if n is None: continue\n                try:\n                    v = _vectors.word_vec(any2unicode(n))\n                except KeyError as error:\n                    # v = np.zeros((100,), dtype=float)\n                    random_state = np.random.RandomState(seed=(hash(n) % (2 ** 32 - 1)))\n                    v = random_state.uniform(low=10.0, high=10.0, size=(100,))\n                c.append(v)\n            r = np.average(c, axis=0)\n            vectors.append(r)\n    return vectors\n\n'''\nDistance\n'''\n# Levenshtein Distance\ndef _levenshtein_distance(sentence1, sentence2):\n    '''\n    Return the Levenshtein distance between two strings.\n    Based on:\n        http://rosettacode.org/wiki/Levenshtein_distance#Python\n    '''\n    first = any2utf8(sentence1).decode('utf-8', 'ignore')\n    second = any2utf8(sentence2).decode('utf-8', 'ignore')\n    sentence1_len, sentence2_len = len(first), len(second)\n    maxlen = max(sentence1_len, sentence2_len)\n    if sentence1_len > sentence2_len:\n        first, second = second, first\n\n    distances = range(len(first) + 1)\n    for index2, char2 in enumerate(second):\n        new_distances = [index2 + 1]\n        for index1, char1 in enumerate(first):\n            if char1 == char2:\n                new_distances.append(distances[index1])\n            else:\n                new_distances.append(1 + min((distances[index1],\n                                             distances[index1 + 1],\n                                             new_distances[-1])))\n        distances = new_distances\n    levenshtein = distances[-1]\n    d = float((maxlen - levenshtein)/maxlen)\n    # smoothing\n    s = (sigmoid(d * 6) - 0.5) * 2\n    # print(\"smoothing[%s| %s]: %s -> %s\" % (sentence1, sentence2, d, s))\n    return s\n\ndef sv(sentence, ignore=False):\n    '''\n    获得一个分词后句子的向量，向量以 array[array[]] 方式组成，即获取 sentence 中每个词的向量 array[] 放在一个 array 中\n\n    sentence: 句子是分词后通过空格联合起来\n    ignore: 是否忽略OOV，False时，随机生成一个向量\n    '''\n    return _get_wv(sentence, ignore = ignore)\n\ndef bow(sentence, ignore=False):\n    '''\n    获得一个分词后句子的向量，向量以BoW方式组成\n\n    sentence: 句子是分词后通过空格联合起来\n    ignore: 是否忽略OOV，False时，随机生成一个向量\n    '''\n    return _flat_sum_array(_get_wv(sentence, ignore))\n\n\ndef v(word):\n    '''\n    获得一个词语的向量，OOV时抛出 KeyError 异常\n    '''\n    y_ = any2unicode(word).strip()\n    return _vectors.word_vec(y_)\n\ndef _nearby_levenshtein_distance(s1, s2):\n    '''\n    使用空间距离近的词汇优化编辑距离计算\n    '''\n    s1_len, s2_len = len(s1), len(s2)\n    maxlen = s1_len\n    if s1_len == s2_len:\n        first, second = sorted([s1, s2])\n    elif s1_len < s2_len:\n        first = s1\n        second = s2\n        maxlen = s2_len\n    else:\n        first = s2\n        second = s1\n\n    ft = set() # all related words with first sentence \n    for x in first:\n        ft.add(x)\n        n, _ = nearby(x)\n        for o in n[:10]:\n            ft.add(o)\n    \n    scores = []\n    for x in second:\n        choices = [_levenshtein_distance(x, y) for y in ft]\n        if len(choices) > 0: scores.append(max(choices))\n\n    s = np.sum(scores) / maxlen if len(scores) > 0 else 0\n    return s\n\ndef _similarity_distance(s1, s2, ignore):\n    '''\n    compute similarity with distance measurement\n    '''\n    g = 0.0\n    try:\n        g_ = cosine(_flat_sum_array(_get_wv(s1, ignore)), _flat_sum_array(_get_wv(s2, ignore)))\n        if is_digit(g_): g = g_\n    except: pass\n\n    u = _nearby_levenshtein_distance(s1, s2)\n    if u >= 0.99:\n        r = 1.0\n    elif u > 0.9:\n        r = _similarity_smooth(g, 0.05, u, 0.05)\n    elif u > 0.8:\n        r = _similarity_smooth(g, 0.1, u, 0.2)\n    elif u > 0.4:\n        r = _similarity_smooth(g, 0.2, u, 0.15)\n    elif u > 0.2:\n        r = _similarity_smooth(g, 0.3, u, 0.1)\n    else:\n        r = _similarity_smooth(g, 0.4, u, 0)\n\n    if r < 0: r = abs(r)\n    r = min(r, 1.0)\n    return float(\"%.3f\" % r)\n\n'''\nPublic Methods\n'''\nseg = _segment_words # word segmenter\n\ndef nearby(word, size = 10):\n    '''\n    Nearby word\n    '''\n    w = any2unicode(word)\n    wk = w + '-' + str(size)\n    # read from cache\n    if wk in _cache_nearby: return _cache_nearby[wk]\n\n    words, scores = [], []\n    try:\n        for x in _vectors.neighbours(w, size):\n            words.append(x[0])\n            scores.append(x[1])\n    except: pass # ignore key error, OOV\n    # put into cache\n    _cache_nearby[wk] = (words, scores)\n    return words, scores\n\ndef compare(s1, s2, seg=True, ignore=False, stopwords=False):\n    '''\n    compare similarity\n    s1 : sentence1\n    s2 : sentence2\n    seg : True : The original sentences need be cut\n          False : The original sentences have been cut\n    ignore: True: ignore OOV words\n            False: get vector randomly for OOV words\n    '''\n    if s1 == s2: return 1.0\n    \n    s1_words = []\n    s2_words = []\n\n    if seg:\n        s1, _ = _segment_words(s1)\n        s2, _ = _segment_words(s2)\n    else:\n        s1 = s1.split()\n        s2 = s2.split()\n\n    # check stopwords\n    if not stopwords:\n        global _stopwords\n        for x in s1: \n            if not x in _stopwords:\n                s1_words.append(x)\n        for x in s2:\n            if not x in _stopwords:\n                s2_words.append(x)\n    else:\n        s1_words = s1 \n        s2_words = s2\n\n    assert len(s1) > 0 and len(s2) > 0, \"The length of s1 and s2 should > 0.\"\n    return _similarity_distance(s1_words, s2_words, ignore)\n\ndef describe():\n    '''\n    summary info of vectors\n    '''\n    vocab_size = len(_vectors.vocab.keys())\n    print(\"Vocab size in vector model: %d\" % vocab_size)\n    print(\"model_path: %s\" % _f_model)\n    print(\"version: %s\" % __version__)\n    return dict({\n        \"vocab_size\": vocab_size,\n        \"version\": __version__,\n        \"model_path\": _f_model\n    })\n\ndef display(word, size = 10):\n    print(\"'%s'近义词：\" % word)\n    o = nearby(word, size)\n    assert len(o) == 2, \"should contain 2 list\"\n    if len(o[0]) == 0:\n        print(\" out of vocabulary\")\n    for k, v in enumerate(o[0]):\n        print(\"  %d. %s:%s\" % (k + 1, v, o[1][k]))\n\ndef main():\n    display(\"人脸\")\n    display(\"NOT_EXIST\")\n\nif __name__ == '__main__':\n    main()\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}