{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/mem0",
    "API_Calls": [
        {
            "Name": "SupportChatbot",
            "Description": "通过调用Memory类实现了一个​​具备长期记忆与上下文感知能力的智能客服系统",
            "Code": "import os\nfrom typing import List, Dict\nfrom mem0 import Memory\nfrom datetime import datetime\nimport anthropic\n\n# Set up environment variables\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\" # needed for embedding model\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your_anthropic_api_key\"\n\nclass SupportChatbot:\n    def __init__(self):\n        # Initialize Mem0 with Anthropic's Claude\n        self.config = {\n            \"llm\": {\n                \"provider\": \"anthropic\",\n                \"config\": {\n                    \"model\": \"claude-3-5-sonnet-latest\",\n                    \"temperature\": 0.1,\n                    \"max_tokens\": 2000,\n                }\n            }\n        }\n        self.client = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n        self.memory = Memory.from_config(self.config)\n\n        # Define support context\n        self.system_context = \"\"\"\n        You are a helpful customer support agent. Use the following guidelines:\n        - Be polite and professional\n        - Show empathy for customer issues\n        - Reference past interactions when relevant\n        - Maintain consistent information across conversations\n        - If you're unsure about something, ask for clarification\n        - Keep track of open issues and follow-ups\n        \"\"\"\n\n    def store_customer_interaction(self,\n                                 user_id: str,\n                                 message: str,\n                                 response: str,\n                                 metadata: Dict = None):\n        \"\"\"Store customer interaction in memory.\"\"\"\n        if metadata is None:\n            metadata = {}\n\n        # Add timestamp to metadata\n        metadata[\"timestamp\"] = datetime.now().isoformat()\n\n        # Format conversation for storage\n        conversation = [\n            {\"role\": \"user\", \"content\": message},\n            {\"role\": \"assistant\", \"content\": response}\n        ]\n\n        # Store in Mem0\n        self.memory.add(\n            conversation,\n            user_id=user_id,\n            metadata=metadata\n        )\n\n    def get_relevant_history(self, user_id: str, query: str) -> List[Dict]:\n        \"\"\"Retrieve relevant past interactions.\"\"\"\n        return self.memory.search(\n            query=query,\n            user_id=user_id,\n            limit=5  # Adjust based on needs\n        )\n\n    def handle_customer_query(self, user_id: str, query: str) -> str:\n        \"\"\"Process customer query with context from past interactions.\"\"\"\n\n        # Get relevant past interactions\n        relevant_history = self.get_relevant_history(user_id, query)\n\n        # Build context from relevant history\n        context = \"Previous relevant interactions:\\n\"\n        for memory in relevant_history:\n            context += f\"Customer: {memory['memory']}\\n\"\n            context += f\"Support: {memory['memory']}\\n\"\n            context += \"---\\n\"\n\n        # Prepare prompt with context and current query\n        prompt = f\"\"\"\n        {self.system_context}\n\n        {context}\n\n        Current customer query: {query}\n\n        Provide a helpful response that takes into account any relevant past interactions.\n        \"\"\"\n\n        # Generate response using Claude\n        response = self.client.messages.create(\n            model=\"claude-3-5-sonnet-latest\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=2000,\n            temperature=0.1\n        )\n\n        # Store interaction\n        self.store_customer_interaction(\n            user_id=user_id,\n            message=query,\n            response=response,\n            metadata={\"type\": \"support_query\"}\n        )\n\n        return response.content[0].text\n        chatbot = SupportChatbot()\n        \nuser_id = \"customer_bot\"\nprint(\"Welcome to Customer Support! Type 'exit' to end the conversation.\")\n\nwhile True:\n    # Get user input\n    query = input()\n    print(\"Customer:\", query)\n    \n    # Check if user wants to exit\n    if query.lower() == 'exit':\n        print(\"Thank you for using our support service. Goodbye!\")\n        break\n    \n    # Handle the query and print the response\n    response = chatbot.handle_customer_query(user_id, query)\n    print(\"Support:\", response, \"\\n\\n\")\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/mem0/cookbooks/customer-support-chatbot.ipynb"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class Memory",
            "Description": "\nMemory类实现了​​面向AI代理的多模态记忆管理系统​​，支持跨模态信息存储、语义检索、动态更新与关联推理",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/mem0/mem0/memory/main.py",
            "Implementation": "class Memory(MemoryBase):\n    def __init__(self, config: MemoryConfig = MemoryConfig()):\n        self.config = config\n\n        self.custom_fact_extraction_prompt = self.config.custom_fact_extraction_prompt\n        self.custom_update_memory_prompt = self.config.custom_update_memory_prompt\n        self.embedding_model = EmbedderFactory.create(\n            self.config.embedder.provider,\n            self.config.embedder.config,\n            self.config.vector_store.config,\n        )\n        self.vector_store = VectorStoreFactory.create(\n            self.config.vector_store.provider, self.config.vector_store.config\n        )\n        self.llm = LlmFactory.create(self.config.llm.provider, self.config.llm.config)\n        self.db = SQLiteManager(self.config.history_db_path)\n        self.collection_name = self.config.vector_store.config.collection_name\n        self.api_version = self.config.version\n\n        self.enable_graph = False\n\n        if self.config.graph_store.config:\n            if self.config.graph_store.provider == \"memgraph\":\n                from mem0.memory.memgraph_memory import MemoryGraph\n            else:\n                from mem0.memory.graph_memory import MemoryGraph\n\n            self.graph = MemoryGraph(self.config)\n            self.enable_graph = True\n\n        self.config.vector_store.config.collection_name = \"mem0-migrations\"\n        if self.config.vector_store.provider in [\"faiss\", \"qdrant\"]:\n            provider_path = f\"migrations_{self.config.vector_store.provider}\"\n            self.config.vector_store.config.path = os.path.join(mem0_dir, provider_path)\n            os.makedirs(self.config.vector_store.config.path, exist_ok=True)\n\n        self._telemetry_vector_store = VectorStoreFactory.create(\n            self.config.vector_store.provider, self.config.vector_store.config\n        )\n\n        capture_event(\"mem0.init\", self, {\"sync_type\": \"sync\"})\n\n    @classmethod\n    def from_config(cls, config_dict: Dict[str, Any]):\n        try:\n            config = cls._process_config(config_dict)\n            config = MemoryConfig(**config_dict)\n        except ValidationError as e:\n            logger.error(f\"Configuration validation error: {e}\")\n            raise\n        return cls(config)\n\n    @staticmethod\n    def _process_config(config_dict: Dict[str, Any]) -> Dict[str, Any]:\n        if \"graph_store\" in config_dict:\n            if \"vector_store\" not in config_dict and \"embedder\" in config_dict:\n                config_dict[\"vector_store\"] = {}\n                config_dict[\"vector_store\"][\"config\"] = {}\n                config_dict[\"vector_store\"][\"config\"][\"embedding_model_dims\"] = config_dict[\"embedder\"][\"config\"][\n                    \"embedding_dims\"\n                ]\n        try:\n            return config_dict\n        except ValidationError as e:\n            logger.error(f\"Configuration validation error: {e}\")\n            raise\n\n    def add(\n        self,\n        messages,\n        user_id=None,\n        agent_id=None,\n        run_id=None,\n        metadata=None,\n        filters=None,\n        infer=True,\n        memory_type=None,\n        prompt=None,\n    ):\n        \"\"\"\n        Create a new memory.\n\n        Args:\n            messages (str or List[Dict[str, str]]): Messages to store in the memory.\n            user_id (str, optional): ID of the user creating the memory. Defaults to None.\n            agent_id (str, optional): ID of the agent creating the memory. Defaults to None.\n            run_id (str, optional): ID of the run creating the memory. Defaults to None.\n            metadata (dict, optional): Metadata to store with the memory. Defaults to None.\n            filters (dict, optional): Filters to apply to the search. Defaults to None.\n            infer (bool, optional): Whether to infer the memories. Defaults to True.\n            memory_type (str, optional): Type of memory to create. Defaults to None. By default, it creates the short term memories and long term (semantic and episodic) memories. Pass \"procedural_memory\" to create procedural memories.\n            prompt (str, optional): Prompt to use for the memory creation. Defaults to None.\n        Returns:\n            dict: A dictionary containing the result of the memory addition operation.\n            result: dict of affected events with each dict has the following key:\n              'memories': affected memories\n              'graph': affected graph memories\n\n              'memories' and 'graph' is a dict, each with following subkeys:\n                'add': added memory\n                'update': updated memory\n                'delete': deleted memory\n\n\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n\n        filters = filters or {}\n        if user_id:\n            filters[\"user_id\"] = metadata[\"user_id\"] = user_id\n        if agent_id:\n            filters[\"agent_id\"] = metadata[\"agent_id\"] = agent_id\n        if run_id:\n            filters[\"run_id\"] = metadata[\"run_id\"] = run_id\n\n        if not any(key in filters for key in (\"user_id\", \"agent_id\", \"run_id\")):\n            raise ValueError(\"One of the filters: user_id, agent_id or run_id is required!\")\n\n        if memory_type is not None and memory_type != MemoryType.PROCEDURAL.value:\n            raise ValueError(\n                f\"Invalid 'memory_type'. Please pass {MemoryType.PROCEDURAL.value} to create procedural memories.\"\n            )\n\n        if isinstance(messages, str):\n            messages = [{\"role\": \"user\", \"content\": messages}]\n\n        if agent_id is not None and memory_type == MemoryType.PROCEDURAL.value:\n            results = self._create_procedural_memory(messages, metadata=metadata, prompt=prompt)\n            return results\n\n        if self.config.llm.config.get(\"enable_vision\"):\n            messages = parse_vision_messages(messages, self.llm, self.config.llm.config.get(\"vision_details\"))\n        else:\n            messages = parse_vision_messages(messages)\n\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            future1 = executor.submit(self._add_to_vector_store, messages, metadata, filters, infer)\n            future2 = executor.submit(self._add_to_graph, messages, filters)\n\n            concurrent.futures.wait([future1, future2])\n\n            vector_store_result = future1.result()\n            graph_result = future2.result()\n\n        if self.api_version == \"v1.0\":\n            warnings.warn(\n                \"The current add API output format is deprecated. \"\n                \"To use the latest format, set `api_version='v1.1'`. \"\n                \"The current format will be removed in mem0ai 1.1.0 and later versions.\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            return vector_store_result\n\n        if self.enable_graph:\n            return {\n                \"results\": vector_store_result,\n                \"relations\": graph_result,\n            }\n\n        return {\"results\": vector_store_result}\n\n    def _add_to_vector_store(self, messages, metadata, filters, infer):\n        if not infer:\n            returned_memories = []\n            for message in messages:\n                if message[\"role\"] != \"system\":\n                    message_embeddings = self.embedding_model.embed(message[\"content\"], \"add\")\n                    memory_id = self._create_memory(message[\"content\"], message_embeddings, metadata)\n                    returned_memories.append({\"id\": memory_id, \"memory\": message[\"content\"], \"event\": \"ADD\"})\n            return returned_memories\n\n        parsed_messages = parse_messages(messages)\n\n        if self.config.custom_fact_extraction_prompt:\n            system_prompt = self.config.custom_fact_extraction_prompt\n            user_prompt = f\"Input:\\n{parsed_messages}\"\n        else:\n            system_prompt, user_prompt = get_fact_retrieval_messages(parsed_messages)\n\n        response = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n            response_format={\"type\": \"json_object\"},\n        )\n\n        try:\n            response = remove_code_blocks(response)\n            new_retrieved_facts = json.loads(response)[\"facts\"]\n        except Exception as e:\n            logging.error(f\"Error in new_retrieved_facts: {e}\")\n            new_retrieved_facts = []\n\n        retrieved_old_memory = []\n        new_message_embeddings = {}\n        for new_mem in new_retrieved_facts:\n            messages_embeddings = self.embedding_model.embed(new_mem, \"add\")\n            new_message_embeddings[new_mem] = messages_embeddings\n            existing_memories = self.vector_store.search(\n                query=new_mem,\n                vectors=messages_embeddings,\n                limit=5,\n                filters=filters,\n            )\n            for mem in existing_memories:\n                retrieved_old_memory.append({\"id\": mem.id, \"text\": mem.payload[\"data\"]})\n        unique_data = {}\n        for item in retrieved_old_memory:\n            unique_data[item[\"id\"]] = item\n        retrieved_old_memory = list(unique_data.values())\n        logging.info(f\"Total existing memories: {len(retrieved_old_memory)}\")\n\n        # mapping UUIDs with integers for handling UUID hallucinations\n        temp_uuid_mapping = {}\n        for idx, item in enumerate(retrieved_old_memory):\n            temp_uuid_mapping[str(idx)] = item[\"id\"]\n            retrieved_old_memory[idx][\"id\"] = str(idx)\n\n        function_calling_prompt = get_update_memory_messages(\n            retrieved_old_memory, new_retrieved_facts, self.config.custom_update_memory_prompt\n        )\n\n        try:\n            new_memories_with_actions = self.llm.generate_response(\n                messages=[{\"role\": \"user\", \"content\": function_calling_prompt}],\n                response_format={\"type\": \"json_object\"},\n            )\n        except Exception as e:\n            logging.error(f\"Error in new_memories_with_actions: {e}\")\n            new_memories_with_actions = []\n\n        try:\n            new_memories_with_actions = remove_code_blocks(new_memories_with_actions)\n            new_memories_with_actions = json.loads(new_memories_with_actions)\n        except Exception as e:\n            logging.error(f\"Invalid JSON response: {e}\")\n            new_memories_with_actions = []\n\n        returned_memories = []\n        try:\n            for resp in new_memories_with_actions.get(\"memory\", []):\n                logging.info(resp)\n                try:\n                    if not resp.get(\"text\"):\n                        logging.info(\"Skipping memory entry because of empty `text` field.\")\n                        continue\n                    elif resp.get(\"event\") == \"ADD\":\n                        memory_id = self._create_memory(\n                            data=resp.get(\"text\"),\n                            existing_embeddings=new_message_embeddings,\n                            metadata=metadata,\n                        )\n                        returned_memories.append(\n                            {\n                                \"id\": memory_id,\n                                \"memory\": resp.get(\"text\"),\n                                \"event\": resp.get(\"event\"),\n                            }\n                        )\n                    elif resp.get(\"event\") == \"UPDATE\":\n                        self._update_memory(\n                            memory_id=temp_uuid_mapping[resp[\"id\"]],\n                            data=resp.get(\"text\"),\n                            existing_embeddings=new_message_embeddings,\n                            metadata=metadata,\n                        )\n                        returned_memories.append(\n                            {\n                                \"id\": temp_uuid_mapping[resp.get(\"id\")],\n                                \"memory\": resp.get(\"text\"),\n                                \"event\": resp.get(\"event\"),\n                                \"previous_memory\": resp.get(\"old_memory\"),\n                            }\n                        )\n                    elif resp.get(\"event\") == \"DELETE\":\n                        self._delete_memory(memory_id=temp_uuid_mapping[resp.get(\"id\")])\n                        returned_memories.append(\n                            {\n                                \"id\": temp_uuid_mapping[resp.get(\"id\")],\n                                \"memory\": resp.get(\"text\"),\n                                \"event\": resp.get(\"event\"),\n                            }\n                        )\n                    elif resp.get(\"event\") == \"NONE\":\n                        logging.info(\"NOOP for Memory.\")\n                except Exception as e:\n                    logging.error(f\"Error in new_memories_with_actions: {e}\")\n        except Exception as e:\n            logging.error(f\"Error in new_memories_with_actions: {e}\")\n\n        capture_event(\n            \"mem0.add\",\n            self,\n            {\"version\": self.api_version, \"keys\": list(filters.keys()), \"sync_type\": \"sync\"},\n        )\n\n        return returned_memories\n\n    def _add_to_graph(self, messages, filters):\n        added_entities = []\n        if self.enable_graph:\n            if filters.get(\"user_id\") is None:\n                filters[\"user_id\"] = \"user\"\n\n            data = \"\\n\".join([msg[\"content\"] for msg in messages if \"content\" in msg and msg[\"role\"] != \"system\"])\n            added_entities = self.graph.add(data, filters)\n\n        return added_entities\n\n    def get(self, memory_id):\n        \"\"\"\n        Retrieve a memory by ID.\n\n        Args:\n            memory_id (str): ID of the memory to retrieve.\n\n        Returns:\n            dict: Retrieved memory.\n        \"\"\"\n        capture_event(\"mem0.get\", self, {\"memory_id\": memory_id, \"sync_type\": \"sync\"})\n        memory = self.vector_store.get(vector_id=memory_id)\n        if not memory:\n            return None\n\n        filters = {key: memory.payload[key] for key in [\"user_id\", \"agent_id\", \"run_id\"] if memory.payload.get(key)}\n\n        # Prepare base memory item\n        memory_item = MemoryItem(\n            id=memory.id,\n            memory=memory.payload[\"data\"],\n            hash=memory.payload.get(\"hash\"),\n            created_at=memory.payload.get(\"created_at\"),\n            updated_at=memory.payload.get(\"updated_at\"),\n        ).model_dump(exclude={\"score\"})\n\n        # Add metadata if there are additional keys\n        excluded_keys = {\n            \"user_id\",\n            \"agent_id\",\n            \"run_id\",\n            \"hash\",\n            \"data\",\n            \"created_at\",\n            \"updated_at\",\n            \"id\",\n        }\n        additional_metadata = {k: v for k, v in memory.payload.items() if k not in excluded_keys}\n        if additional_metadata:\n            memory_item[\"metadata\"] = additional_metadata\n\n        result = {**memory_item, **filters}\n\n        return result\n\n    def get_all(self, user_id=None, agent_id=None, run_id=None, limit=100):\n        \"\"\"\n        List all memories.\n\n        Returns:\n            list: List of all memories.\n        \"\"\"\n        filters = {}\n        if user_id:\n            filters[\"user_id\"] = user_id\n        if agent_id:\n            filters[\"agent_id\"] = agent_id\n        if run_id:\n            filters[\"run_id\"] = run_id\n\n        capture_event(\"mem0.get_all\", self, {\"limit\": limit, \"keys\": list(filters.keys()), \"sync_type\": \"sync\"})\n\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            future_memories = executor.submit(self._get_all_from_vector_store, filters, limit)\n            future_graph_entities = executor.submit(self.graph.get_all, filters, limit) if self.enable_graph else None\n\n            concurrent.futures.wait(\n                [future_memories, future_graph_entities] if future_graph_entities else [future_memories]\n            )\n\n            all_memories = future_memories.result()\n            graph_entities = future_graph_entities.result() if future_graph_entities else None\n\n        if self.enable_graph:\n            return {\"results\": all_memories, \"relations\": graph_entities}\n\n        if self.api_version == \"v1.0\":\n            warnings.warn(\n                \"The current get_all API output format is deprecated. \"\n                \"To use the latest format, set `api_version='v1.1'`. \"\n                \"The current format will be removed in mem0ai 1.1.0 and later versions.\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            return all_memories\n        else:\n            return {\"results\": all_memories}\n\n    def _get_all_from_vector_store(self, filters, limit):\n        memories = self.vector_store.list(filters=filters, limit=limit)\n\n        excluded_keys = {\n            \"user_id\",\n            \"agent_id\",\n            \"run_id\",\n            \"hash\",\n            \"data\",\n            \"created_at\",\n            \"updated_at\",\n            \"id\",\n        }\n        all_memories = [\n            {\n                **MemoryItem(\n                    id=mem.id,\n                    memory=mem.payload[\"data\"],\n                    hash=mem.payload.get(\"hash\"),\n                    created_at=mem.payload.get(\"created_at\"),\n                    updated_at=mem.payload.get(\"updated_at\"),\n                ).model_dump(exclude={\"score\"}),\n                **{key: mem.payload[key] for key in [\"user_id\", \"agent_id\", \"run_id\"] if key in mem.payload},\n                **(\n                    {\"metadata\": {k: v for k, v in mem.payload.items() if k not in excluded_keys}}\n                    if any(k for k in mem.payload if k not in excluded_keys)\n                    else {}\n                ),\n            }\n            for mem in memories[0]\n        ]\n        return all_memories\n\n    def search(self, query, user_id=None, agent_id=None, run_id=None, limit=100, filters=None):\n        \"\"\"\n        Search for memories.\n\n        Args:\n            query (str): Query to search for.\n            user_id (str, optional): ID of the user to search for. Defaults to None.\n            agent_id (str, optional): ID of the agent to search for. Defaults to None.\n            run_id (str, optional): ID of the run to search for. Defaults to None.\n            limit (int, optional): Limit the number of results. Defaults to 100.\n            filters (dict, optional): Filters to apply to the search. Defaults to None.\n\n        Returns:\n            list: List of search results.\n        \"\"\"\n        filters = filters or {}\n        if user_id:\n            filters[\"user_id\"] = user_id\n        if agent_id:\n            filters[\"agent_id\"] = agent_id\n        if run_id:\n            filters[\"run_id\"] = run_id\n\n        if not any(key in filters for key in (\"user_id\", \"agent_id\", \"run_id\")):\n            raise ValueError(\"One of the filters: user_id, agent_id or run_id is required!\")\n\n        capture_event(\n            \"mem0.search\",\n            self,\n            {\"limit\": limit, \"version\": self.api_version, \"keys\": list(filters.keys()), \"sync_type\": \"sync\"},\n        )\n\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            future_memories = executor.submit(self._search_vector_store, query, filters, limit)\n            future_graph_entities = (\n                executor.submit(self.graph.search, query, filters, limit) if self.enable_graph else None\n            )\n\n            concurrent.futures.wait(\n                [future_memories, future_graph_entities] if future_graph_entities else [future_memories]\n            )\n\n            original_memories = future_memories.result()\n            graph_entities = future_graph_entities.result() if future_graph_entities else None\n\n        if self.enable_graph:\n            return {\"results\": original_memories, \"relations\": graph_entities}\n\n        if self.api_version == \"v1.0\":\n            warnings.warn(\n                \"The current get_all API output format is deprecated. \"\n                \"To use the latest format, set `api_version='v1.1'`. \"\n                \"The current format will be removed in mem0ai 1.1.0 and later versions.\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            return original_memories\n        else:\n            return {\"results\": original_memories}\n\n    def _search_vector_store(self, query, filters, limit):\n        embeddings = self.embedding_model.embed(query, \"search\")\n        memories = self.vector_store.search(query=query, vectors=embeddings, limit=limit, filters=filters)\n\n        excluded_keys = {\n            \"user_id\",\n            \"agent_id\",\n            \"run_id\",\n            \"hash\",\n            \"data\",\n            \"created_at\",\n            \"updated_at\",\n            \"id\",\n        }\n\n        original_memories = [\n            {\n                **MemoryItem(\n                    id=mem.id,\n                    memory=mem.payload[\"data\"],\n                    hash=mem.payload.get(\"hash\"),\n                    created_at=mem.payload.get(\"created_at\"),\n                    updated_at=mem.payload.get(\"updated_at\"),\n                    score=mem.score,\n                ).model_dump(),\n                **{key: mem.payload[key] for key in [\"user_id\", \"agent_id\", \"run_id\"] if key in mem.payload},\n                **(\n                    {\"metadata\": {k: v for k, v in mem.payload.items() if k not in excluded_keys}}\n                    if any(k for k in mem.payload if k not in excluded_keys)\n                    else {}\n                ),\n            }\n            for mem in memories\n        ]\n\n        return original_memories\n\n    def update(self, memory_id, data):\n        \"\"\"\n        Update a memory by ID.\n\n        Args:\n            memory_id (str): ID of the memory to update.\n            data (dict): Data to update the memory with.\n\n        Returns:\n            dict: Updated memory.\n        \"\"\"\n        capture_event(\"mem0.update\", self, {\"memory_id\": memory_id, \"sync_type\": \"sync\"})\n\n        existing_embeddings = {data: self.embedding_model.embed(data, \"update\")}\n\n        self._update_memory(memory_id, data, existing_embeddings)\n        return {\"message\": \"Memory updated successfully!\"}\n\n    def delete(self, memory_id):\n        \"\"\"\n        Delete a memory by ID.\n\n        Args:\n            memory_id (str): ID of the memory to delete.\n        \"\"\"\n        capture_event(\"mem0.delete\", self, {\"memory_id\": memory_id, \"sync_type\": \"sync\"})\n        self._delete_memory(memory_id)\n        return {\"message\": \"Memory deleted successfully!\"}\n\n    def delete_all(self, user_id=None, agent_id=None, run_id=None):\n        \"\"\"\n        Delete all memories.\n\n        Args:\n            user_id (str, optional): ID of the user to delete memories for. Defaults to None.\n            agent_id (str, optional): ID of the agent to delete memories for. Defaults to None.\n            run_id (str, optional): ID of the run to delete memories for. Defaults to None.\n        \"\"\"\n        filters = {}\n        if user_id:\n            filters[\"user_id\"] = user_id\n        if agent_id:\n            filters[\"agent_id\"] = agent_id\n        if run_id:\n            filters[\"run_id\"] = run_id\n\n        if not filters:\n            raise ValueError(\n                \"At least one filter is required to delete all memories. If you want to delete all memories, use the `reset()` method.\"\n            )\n\n        capture_event(\"mem0.delete_all\", self, {\"keys\": list(filters.keys()), \"sync_type\": \"sync\"})\n        memories = self.vector_store.list(filters=filters)[0]\n        for memory in memories:\n            self._delete_memory(memory.id)\n\n        logger.info(f\"Deleted {len(memories)} memories\")\n\n        if self.enable_graph:\n            self.graph.delete_all(filters)\n\n        return {\"message\": \"Memories deleted successfully!\"}\n\n    def history(self, memory_id):\n        \"\"\"\n        Get the history of changes for a memory by ID.\n\n        Args:\n            memory_id (str): ID of the memory to get history for.\n\n        Returns:\n            list: List of changes for the memory.\n        \"\"\"\n        capture_event(\"mem0.history\", self, {\"memory_id\": memory_id, \"sync_type\": \"sync\"})\n        return self.db.get_history(memory_id)\n\n    def _create_memory(self, data, existing_embeddings, metadata=None):\n        logging.debug(f\"Creating memory with {data=}\")\n        if data in existing_embeddings:\n            embeddings = existing_embeddings[data]\n        else:\n            embeddings = self.embedding_model.embed(data, memory_action=\"add\")\n        memory_id = str(uuid.uuid4())\n        metadata = metadata or {}\n        metadata[\"data\"] = data\n        metadata[\"hash\"] = hashlib.md5(data.encode()).hexdigest()\n        metadata[\"created_at\"] = datetime.now(pytz.timezone(\"US/Pacific\")).isoformat()\n\n        self.vector_store.insert(\n            vectors=[embeddings],\n            ids=[memory_id],\n            payloads=[metadata],\n        )\n        self.db.add_history(memory_id, None, data, \"ADD\", created_at=metadata[\"created_at\"])\n        capture_event(\"mem0._create_memory\", self, {\"memory_id\": memory_id, \"sync_type\": \"sync\"})\n        return memory_id\n\n    def _create_procedural_memory(self, messages, metadata=None, prompt=None):\n        \"\"\"\n        Create a procedural memory\n\n        Args:\n            messages (list): List of messages to create a procedural memory from.\n            metadata (dict): Metadata to create a procedural memory from.\n            prompt (str, optional): Prompt to use for the procedural memory creation. Defaults to None.\n        \"\"\"\n        logger.info(\"Creating procedural memory\")\n\n        parsed_messages = [\n            {\"role\": \"system\", \"content\": prompt or PROCEDURAL_MEMORY_SYSTEM_PROMPT},\n            *messages,\n            {\n                \"role\": \"user\",\n                \"content\": \"Create procedural memory of the above conversation.\",\n            },\n        ]\n\n        try:\n            procedural_memory = self.llm.generate_response(messages=parsed_messages)\n        except Exception as e:\n            logger.error(f\"Error generating procedural memory summary: {e}\")\n            raise\n\n        if metadata is None:\n            raise ValueError(\"Metadata cannot be done for procedural memory.\")\n\n        metadata[\"memory_type\"] = MemoryType.PROCEDURAL.value\n        # Generate embeddings for the summary\n        embeddings = self.embedding_model.embed(procedural_memory, memory_action=\"add\")\n        # Create the memory\n        memory_id = self._create_memory(procedural_memory, {procedural_memory: embeddings}, metadata=metadata)\n        capture_event(\"mem0._create_procedural_memory\", self, {\"memory_id\": memory_id, \"sync_type\": \"sync\"})\n\n        # Return results in the same format as add()\n        result = {\"results\": [{\"id\": memory_id, \"memory\": procedural_memory, \"event\": \"ADD\"}]}\n\n        return result\n\n    def _update_memory(self, memory_id, data, existing_embeddings, metadata=None):\n        logger.info(f\"Updating memory with {data=}\")\n\n        try:\n            existing_memory = self.vector_store.get(vector_id=memory_id)\n        except Exception:\n            raise ValueError(f\"Error getting memory with ID {memory_id}. Please provide a valid 'memory_id'\")\n        prev_value = existing_memory.payload.get(\"data\")\n\n        new_metadata = metadata or {}\n        new_metadata[\"data\"] = data\n        new_metadata[\"hash\"] = hashlib.md5(data.encode()).hexdigest()\n        new_metadata[\"created_at\"] = existing_memory.payload.get(\"created_at\")\n        new_metadata[\"updated_at\"] = datetime.now(pytz.timezone(\"US/Pacific\")).isoformat()\n\n        if \"user_id\" in existing_memory.payload:\n            new_metadata[\"user_id\"] = existing_memory.payload[\"user_id\"]\n        if \"agent_id\" in existing_memory.payload:\n            new_metadata[\"agent_id\"] = existing_memory.payload[\"agent_id\"]\n        if \"run_id\" in existing_memory.payload:\n            new_metadata[\"run_id\"] = existing_memory.payload[\"run_id\"]\n\n        if data in existing_embeddings:\n            embeddings = existing_embeddings[data]\n        else:\n            embeddings = self.embedding_model.embed(data, \"update\")\n        self.vector_store.update(\n            vector_id=memory_id,\n            vector=embeddings,\n            payload=new_metadata,\n        )\n        logger.info(f\"Updating memory with ID {memory_id=} with {data=}\")\n        self.db.add_history(\n            memory_id,\n            prev_value,\n            data,\n            \"UPDATE\",\n            created_at=new_metadata[\"created_at\"],\n            updated_at=new_metadata[\"updated_at\"],\n        )\n        capture_event(\"mem0._update_memory\", self, {\"memory_id\": memory_id, \"sync_type\": \"sync\"})\n        return memory_id\n\n    def _delete_memory(self, memory_id):\n        logging.info(f\"Deleting memory with {memory_id=}\")\n        existing_memory = self.vector_store.get(vector_id=memory_id)\n        prev_value = existing_memory.payload[\"data\"]\n        self.vector_store.delete(vector_id=memory_id)\n        self.db.add_history(memory_id, prev_value, None, \"DELETE\", is_deleted=1)\n        capture_event(\"mem0._delete_memory\", self, {\"memory_id\": memory_id, \"sync_type\": \"sync\"})\n        return memory_id\n\n    def reset(self):\n        \"\"\"\n        Reset the memory store by:\n            Deletes the vector store collection\n            Resets the database\n            Recreates the vector store with a new client\n        \"\"\"\n        logger.warning(\"Resetting all memories\")\n\n        # Close the old connection if possible\n        if hasattr(self.db, 'connection') and self.db.connection:\n                self.db.connection.execute(\"DROP TABLE IF EXISTS history\")\n                self.db.connection.close()\n\n        self.db = SQLiteManager(self.config.history_db_path)\n\n        if hasattr(self.vector_store, 'reset'):\n            self.vector_store = VectorStoreFactory.reset(self.vector_store)\n        else:\n            logger.warning(\"Vector store does not support reset. Skipping.\")\n            self.vector_store.delete_col()\n            self.vector_store = VectorStoreFactory.create(\n                self.config.vector_store.provider, self.config.vector_store.config\n            )\n        capture_event(\"mem0.reset\", self, {\"sync_type\": \"sync\"})\n\n    def chat(self, query):\n        raise NotImplementedError(\"Chat function not implemented yet.\")\n",
            "Example": [
                "dasdasdasdasd\n\n",
                ""
            ]
        }
    ]
}