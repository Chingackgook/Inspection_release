{
  "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/ChatTTS",
  "API_Calls": [
    {
      "Name": "Chat",
      "Code": "\nimport os, sys\n\nif sys.platform == \"darwin\":\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\nnow_dir = os.getcwd()\nsys.path.append(now_dir)\n\nimport logging\n\nimport ChatTTS\n\nfrom tools.logger import get_logger\n\nlogger = get_logger(\"Test\", lv=logging.WARN)\n\nchat = ChatTTS.Chat(logger)\nchat.load(compile=False, source=\"huggingface\")  # Set to True for better performance\n\ntexts = [\n    \"的 话 语 音 太 短 了 会 造 成 生 成 音 频 错 误 ， 这 是 占 位 占 位 ， 老 大 爷 觉 得 车 夫 的 想 法 很 有 道 理 [uv_break]\",\n    \"的 话 评 分 只 是 衡 量 音 色 的 稳 定 性 ， 不 代 表 音 色 的 好 坏 ， 可 以 根 据 自 己 的 需 求 选 择 [uv_break] 合 适 的 音 色\",\n    \"然 后 举 个 简 单 的 例 子 ， 如 果 一 个 [uv_break] 沙 哑 且 结 巴 的 音 色 一 直 很 稳 定 ， 那 么 它 的 评 分 就 会 很 高 。\",\n    \"语 音 太 短 了 会 造 成 生 成 音 频 错 误 ， 这 是 占 位 [uv_break] 占 位 。 我 使 用 seed id 去 生 成 音 频 ， 但 是 生 成 的 音 频 不 稳 定\",\n    \"在d id 只 是 一 个 参 考 id [uv_break] 不 同 的 环 境 下 音 色 不 一 定 一 致 。 还 是 推 荐 使 用 。 pt 文 件 载 入 音 色\",\n    \"的 话 语 音 太 短 了 会 造 成 生 成 音 频 错 误 ， 这 是 占 位 占 位 。 音 色 标 的 男 女 [uv_break] 准 确 吗\",\n    \"， 当 前 第 一 批 测 试 的 音 色 有 两 千 条 [uv_break] ， 根 据 声 纹 相 似 性 简 单 打 标 ， 准 确 度 不 高 ， 特 别 是 特 征 一 项\",\n    \"语 音 太 短 了 会 造 成 生 成 音 频 错 误 ， 这 是 占 位 占 位 。 仅 供 参 考 。 如 果 大 家 有 更 好 的 标 注 方 法 ， 欢 迎 pr [uv_break] 。\",\n]\n\nparams_infer_code = ChatTTS.Chat.InferCodeParams(\n    spk_emb=chat.sample_random_speaker(),\n    temperature=0.3,\n    top_P=0.005,\n    top_K=1,\n    show_tqdm=False,\n)\n\nfail = False\n\nwavs = chat.infer(\n    texts,\n    skip_refine_text=True,\n    split_text=False,\n    params_infer_code=params_infer_code,\n)\n\nfor k, wav in enumerate(wavs):\n    if wav is None:\n        logger.warning(\"index\", k, \"is None\")\n        fail = True\n\nif fail:\n    import sys\n\n    sys.exit(1)\n\n",
      "Description": "ChatTTS的聊天功能"
    },
    {
      "Name": "Chat",
      "Code": "\nimport os, sys\n\nif sys.platform == \"darwin\":\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\nnow_dir = os.getcwd()\nsys.path.append(now_dir)\n\nimport logging\nimport re\n\nimport ChatTTS\n\nfrom tools.logger import get_logger\n\nlogger = get_logger(\"Test\", lv=logging.WARN)\n\nchat = ChatTTS.Chat(logger)\nchat.load(compile=False, source=\"huggingface\")  # Set to True for better performance\n\ntexts = [\n    \"总结一下，AI Agent是大模型功能的扩展，让AI更接近于通用人工智能，也就是我们常说的AGI。\",\n    \"你真是太聪明啦。\",\n]\n\nfail = False\n\nrefined = chat.infer(\n    texts,\n    refine_text_only=True,\n    stream=False,\n    split_text=False,\n    params_refine_text=ChatTTS.Chat.RefineTextParams(show_tqdm=False),\n)\n\ntrimre = re.compile(\"\\\\[[\\w_]+\\\\]\")\n\n\ndef trim_tags(txt: str) -> str:\n    global trimre\n    return trimre.sub(\"\", txt)\n\n\nfor i, t in enumerate(refined):\n    if len(trim_tags(t)) > 4 * len(texts[i]):\n        fail = True\n        logger.warning(\"in: %s, out: %s\", texts[i], t)\n\nif fail:\n    import sys\n\n    sys.exit(1)\n",
      "Description": "ChatTTS的文本润色功能"
    },
    {
      "Name": "Chat",
      "Code": "\nimport os, sys\n\nif sys.platform == \"darwin\":\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\nnow_dir = os.getcwd()\nsys.path.append(now_dir)\n\nimport logging\n\nimport torch\n\nimport ChatTTS\n\nfrom tools.logger import get_logger\nfrom tools.normalizer import normalizer_en_nemo_text\n\nlogger = get_logger(\"Test\", lv=logging.WARN)\n\nchat = ChatTTS.Chat(logger)\nchat.load(compile=False, source=\"huggingface\")  # Set to True for better performance\ntry:\n    chat.normalizer.register(\"en\", normalizer_en_nemo_text())\nexcept:\n    logger.warning(\"Package nemo_text_processing not found!\")\n\nrand_spk = chat.sample_random_speaker()\n\n\ntext = [\"What is [uv_break]your favorite english food?[laugh][lbreak]\"]\n\nfail = False\n\nrefined_text = chat.infer(\n    text,\n    refine_text_only=True,\n    params_refine_text=ChatTTS.Chat.RefineTextParams(\n        prompt=\"[oral_2][laugh_0][break_6]\",\n        manual_seed=12345,\n    ),\n    split_text=False,\n)\nif (\n    refined_text[0]\n    != \"what is [uv_break] your favorite english [uv_break] food [laugh] like [lbreak]\"\n):\n    fail = True\n    logger.warning(\"refined text is '%s'\", refined_text[0])\n\nparams = ChatTTS.Chat.InferCodeParams(\n    spk_emb=rand_spk,  # add sampled speaker\n    temperature=0.3,  # using custom temperature\n    top_P=0.7,  # top P decode\n    top_K=20,  # top K decode\n)\ninput_ids, attention_mask, text_mask = chat.tokenizer.encode(\n    chat.speaker.decorate_code_prompts(\n        text,\n        params.prompt,\n        params.txt_smp,\n        params.spk_emb,\n    ),\n    chat.config.gpt.num_vq,\n    prompt=(\n        chat.speaker.decode_prompt(params.spk_smp)\n        if params.spk_smp is not None\n        else None\n    ),\n    device=chat.device_gpt,\n)\nwith torch.inference_mode():\n    start_idx, end_idx = 0, torch.zeros(\n        input_ids.shape[0], device=input_ids.device, dtype=torch.long\n    ).fill_(input_ids.shape[1])\n\n    recoded_text = chat.tokenizer.decode(\n        chat.gpt._prepare_generation_outputs(\n            input_ids,\n            start_idx,\n            end_idx,\n            [],\n            [],\n            True,\n        ).ids\n    )\n\nif (\n    recoded_text[0]\n    != \"[Stts] [spk_emb] [speed_5] what is [uv_break] your favorite english food? [laugh] [lbreak] [Ptts]\"\n):\n    fail = True\n    logger.warning(\"recoded text is '%s'\", refined_text)\n\nif fail:\n    import sys\n\n    sys.exit(1)\n",
      "Description": "ChatTTS的文本编码功能"
    }
  ],
  "API_Implementations": [
    {
      "Implementation": "\nclass Chat:\n    def __init__(self, logger=logging.getLogger(__name__)):\n        self.logger = logger\n        utils_logger.set_logger(logger)\n\n        self.config = Config()\n\n        self.normalizer = Normalizer(\n            os.path.join(os.path.dirname(__file__), \"res\", \"homophones_map.json\"),\n            logger,\n        )\n        with open(\n            os.path.join(os.path.dirname(__file__), \"res\", \"sha256_map.json\")\n        ) as f:\n            self.sha256_map: Dict[str, str] = load(f)\n\n        self.context = GPT.Context()\n\n    def has_loaded(self, use_decoder=False):\n        not_finish = False\n        check_list = [\"vocos\", \"gpt\", \"tokenizer\", \"embed\"]\n\n        if use_decoder:\n            check_list.append(\"decoder\")\n        else:\n            check_list.append(\"dvae\")\n\n        for module in check_list:\n            if not hasattr(self, module):\n                self.logger.warning(f\"{module} not initialized.\")\n                not_finish = True\n\n        return not not_finish\n\n    def download_models(\n        self,\n        source: Literal[\"huggingface\", \"local\", \"custom\"] = \"local\",\n        force_redownload=False,\n        custom_path: Optional[torch.serialization.FILE_LIKE] = None,\n    ) -> Optional[str]:\n        if source == \"local\":\n            download_path = custom_path if custom_path is not None else os.getcwd()\n            if (\n                not check_all_assets(Path(download_path), self.sha256_map, update=True)\n                or force_redownload\n            ):\n                with tempfile.TemporaryDirectory() as tmp:\n                    download_all_assets(tmpdir=tmp, homedir=download_path)\n                if not check_all_assets(\n                    Path(download_path), self.sha256_map, update=False\n                ):\n                    self.logger.error(\n                        \"download to local path %s failed.\", download_path\n                    )\n                    return None\n        elif source == \"huggingface\":\n            try:\n                download_path = (\n                    get_latest_modified_file(\n                        os.path.join(\n                            os.getenv(\n                                \"HF_HOME\", os.path.expanduser(\"~/.cache/huggingface\")\n                            ),\n                            \"hub/models--2Noise--ChatTTS/snapshots\",\n                        )\n                    )\n                    if custom_path is None\n                    else get_latest_modified_file(\n                        os.path.join(custom_path, \"models--2Noise--ChatTTS/snapshots\")\n                    )\n                )\n            except:\n                download_path = None\n            if download_path is None or force_redownload:\n                self.logger.log(\n                    logging.INFO,\n                    f\"download from HF: https://huggingface.co/2Noise/ChatTTS\",\n                )\n                try:\n                    download_path = snapshot_download(\n                        repo_id=\"2Noise/ChatTTS\",\n                        allow_patterns=[\"*.yaml\", \"*.json\", \"*.safetensors\"],\n                        cache_dir=custom_path,\n                        force_download=force_redownload,\n                    )\n                except:\n                    download_path = None\n                else:\n                    self.logger.log(\n                        logging.INFO,\n                        f\"load latest snapshot from cache: {download_path}\",\n                    )\n        elif source == \"custom\":\n            self.logger.log(logging.INFO, f\"try to load from local: {custom_path}\")\n            if not check_all_assets(Path(custom_path), self.sha256_map, update=False):\n                self.logger.error(\"check models in custom path %s failed.\", custom_path)\n                return None\n            download_path = custom_path\n\n        if download_path is None:\n            self.logger.error(\"Model download failed\")\n            return None\n\n        return download_path\n\n    def load(\n        self,\n        source: Literal[\"huggingface\", \"local\", \"custom\"] = \"local\",\n        force_redownload=False,\n        compile: bool = False,\n        custom_path: Optional[torch.serialization.FILE_LIKE] = None,\n        device: Optional[torch.device] = None,\n        coef: Optional[torch.Tensor] = None,\n        use_flash_attn=False,\n        use_vllm=False,\n        experimental: bool = False,\n    ) -> bool:\n        download_path = self.download_models(source, force_redownload, custom_path)\n        if download_path is None:\n            return False\n        return self._load(\n            device=device,\n            compile=compile,\n            coef=coef,\n            use_flash_attn=use_flash_attn,\n            use_vllm=use_vllm,\n            experimental=experimental,\n            **{\n                k: os.path.join(download_path, v)\n                for k, v in asdict(self.config.path).items()\n            },\n        )\n\n    def unload(self):\n        logger = self.logger\n        self.normalizer.destroy()\n        del self.normalizer\n        del self.sha256_map\n        del_list = [\"vocos\", \"gpt\", \"decoder\", \"dvae\", \"tokenizer\", \"embed\"]\n        for module in del_list:\n            if hasattr(self, module):\n                delattr(self, module)\n        self.__init__(logger)\n\n    def sample_random_speaker(self) -> str:\n        return self.speaker.sample_random()\n\n    def sample_audio_speaker(self, wav: Union[np.ndarray, torch.Tensor]) -> str:\n        return self.speaker.encode_prompt(self.dvae.sample_audio(wav))\n\n    @dataclass(repr=False, eq=False)\n    class RefineTextParams:\n        prompt: str = \"\"\n        top_P: float = 0.7\n        top_K: int = 20\n        temperature: float = 0.7\n        repetition_penalty: float = 1.0\n        max_new_token: int = 384\n        min_new_token: int = 0\n        show_tqdm: bool = True\n        ensure_non_empty: bool = True\n        manual_seed: Optional[int] = None\n\n    @dataclass(repr=False, eq=False)\n    class InferCodeParams(RefineTextParams):\n        prompt: str = \"[speed_5]\"\n        spk_emb: Optional[str] = None\n        spk_smp: Optional[str] = None\n        txt_smp: Optional[str] = None\n        temperature: float = 0.3\n        repetition_penalty: float = 1.05\n        max_new_token: int = 2048\n        stream_batch: int = 24\n        stream_speed: int = 12000\n        pass_first_n_batches: int = 2\n\n    def infer(\n        self,\n        text,\n        stream=False,\n        lang=None,\n        skip_refine_text=False,\n        refine_text_only=False,\n        use_decoder=True,\n        do_text_normalization=True,\n        do_homophone_replacement=True,\n        split_text=True,\n        max_split_batch=4,\n        params_refine_text=RefineTextParams(),\n        params_infer_code=InferCodeParams(),\n    ):\n        self.context.set(False)\n\n        if split_text and isinstance(text, str):\n            if \"\\n\" in text:\n                text = text.split(\"\\n\")\n            else:\n                text = re.split(r\"(?<=。)|(?<=\\.\\s)\", text)\n                nt = []\n                if isinstance(text, list):\n                    for t in text:\n                        if t:\n                            nt.append(t)\n                    text = nt\n                else:\n                    text = [text]\n            self.logger.info(\"split text into %d parts\", len(text))\n            self.logger.debug(\"%s\", str(text))\n\n        if len(text) == 0:\n            return []\n\n        res_gen = self._infer(\n            text,\n            stream,\n            lang,\n            skip_refine_text,\n            refine_text_only,\n            use_decoder,\n            do_text_normalization,\n            do_homophone_replacement,\n            split_text,\n            max_split_batch,\n            params_refine_text,\n            params_infer_code,\n        )\n        if stream:\n            return res_gen\n        elif not refine_text_only:\n            stripped_wavs = []\n            for wavs in res_gen:\n                for wav in wavs:\n                    stripped_wavs.append(wav[np.abs(wav) > 1e-5])\n            if split_text:\n                return [np.concatenate(stripped_wavs)]\n            return stripped_wavs\n        else:\n            return next(res_gen)\n\n    def interrupt(self):\n        self.context.set(True)\n\n    @torch.no_grad()\n    def _load(\n        self,\n        vocos_ckpt_path: str = None,\n        dvae_ckpt_path: str = None,\n        gpt_ckpt_path: str = None,\n        embed_path: str = None,\n        decoder_ckpt_path: str = None,\n        tokenizer_path: str = None,\n        device: Optional[torch.device] = None,\n        compile: bool = False,\n        coef: Optional[str] = None,\n        use_flash_attn=False,\n        use_vllm=False,\n        experimental: bool = False,\n    ):\n        if device is None:\n            device = select_device(experimental=experimental)\n            self.logger.info(\"use device %s\", str(device))\n        self.device = device\n        self.device_gpt = device if \"mps\" not in str(device) else torch.device(\"cpu\")\n        self.compile = compile\n\n        feature_extractor = instantiate_class(\n            args=(), init=asdict(self.config.vocos.feature_extractor)\n        )\n        backbone = instantiate_class(args=(), init=asdict(self.config.vocos.backbone))\n        head = instantiate_class(args=(), init=asdict(self.config.vocos.head))\n        vocos = (\n            Vocos(feature_extractor=feature_extractor, backbone=backbone, head=head)\n            .to(\n                # Vocos on mps will crash, use cpu fallback.\n                # Plus, complex dtype used in the decode process of Vocos is not supported in torch_npu now,\n                # so we put this calculation of data on CPU instead of NPU.\n                \"cpu\"\n                if \"mps\" in str(device) or \"npu\" in str(device)\n                else device\n            )\n            .eval()\n        )\n        assert vocos_ckpt_path, \"vocos_ckpt_path should not be None\"\n        vocos.load_state_dict(load_safetensors(vocos_ckpt_path))\n        self.vocos = vocos\n        self.logger.log(logging.INFO, \"vocos loaded.\")\n\n        # computation of MelSpectrogram on npu is not support now, use cpu fallback.\n        dvae_device = torch.device(\"cpu\") if \"npu\" in str(self.device) else device\n        dvae = DVAE(\n            decoder_config=asdict(self.config.dvae.decoder),\n            encoder_config=asdict(self.config.dvae.encoder),\n            vq_config=asdict(self.config.dvae.vq),\n            dim=self.config.dvae.decoder.idim,\n            coef=coef,\n            device=dvae_device,\n        )\n        coef = str(dvae)\n        assert dvae_ckpt_path, \"dvae_ckpt_path should not be None\"\n        dvae.load_pretrained(dvae_ckpt_path, dvae_device)\n        self.dvae = dvae.eval()\n        self.logger.log(logging.INFO, \"dvae loaded.\")\n\n        embed = Embed(\n            self.config.embed.hidden_size,\n            self.config.embed.num_audio_tokens,\n            self.config.embed.num_text_tokens,\n            self.config.embed.num_vq,\n        )\n        embed.load_pretrained(embed_path, device=device)\n        self.embed = embed.to(device)\n        self.logger.log(logging.INFO, \"embed loaded.\")\n\n        gpt = GPT(\n            gpt_config=asdict(self.config.gpt),\n            embed=self.embed,\n            use_flash_attn=use_flash_attn,\n            use_vllm=use_vllm,\n            device=device,\n            device_gpt=self.device_gpt,\n            logger=self.logger,\n        ).eval()\n        assert gpt_ckpt_path, \"gpt_ckpt_path should not be None\"\n        gpt.load_pretrained(gpt_ckpt_path, embed_path, experimental=experimental)\n        gpt.prepare(compile=compile and \"cuda\" in str(device))\n        self.gpt = gpt\n        self.logger.log(logging.INFO, \"gpt loaded.\")\n\n        self.speaker = Speaker(\n            self.config.gpt.hidden_size, self.config.spk_stat, device\n        )\n        self.logger.log(logging.INFO, \"speaker loaded.\")\n\n        decoder = DVAE(\n            decoder_config=asdict(self.config.decoder),\n            dim=self.config.decoder.idim,\n            coef=coef,\n            device=device,\n        )\n        coef = str(decoder)\n        assert decoder_ckpt_path, \"decoder_ckpt_path should not be None\"\n        decoder.load_pretrained(decoder_ckpt_path, device)\n        self.decoder = decoder.eval()\n        self.logger.log(logging.INFO, \"decoder loaded.\")\n\n        if tokenizer_path:\n            self.tokenizer = Tokenizer(tokenizer_path)\n            self.logger.log(logging.INFO, \"tokenizer loaded.\")\n\n        self.coef = coef\n\n        return self.has_loaded()\n\n    def _infer(\n        self,\n        text: Union[List[str], str],\n        stream=False,\n        lang=None,\n        skip_refine_text=False,\n        refine_text_only=False,\n        use_decoder=True,\n        do_text_normalization=True,\n        do_homophone_replacement=True,\n        split_text=True,\n        max_split_batch=4,\n        params_refine_text=RefineTextParams(),\n        params_infer_code=InferCodeParams(),\n    ):\n\n        assert self.has_loaded(use_decoder=use_decoder)\n\n        if not isinstance(text, list):\n            text = [text]\n\n        text = [\n            self.normalizer(\n                t,\n                do_text_normalization,\n                do_homophone_replacement,\n                lang,\n            )\n            for t in text\n        ]\n\n        self.logger.debug(\"normed texts %s\", str(text))\n\n        if not skip_refine_text:\n            refined = self._refine_text(\n                text,\n                self.device,\n                params_refine_text,\n            )\n            text_tokens = refined.ids\n            text_tokens = [i[i.less(self.tokenizer.break_0_ids)] for i in text_tokens]\n            text = self.tokenizer.decode(text_tokens)\n            refined.destroy()\n            if refine_text_only:\n                if split_text and isinstance(text, list):\n                    text = \"\\n\".join(text)\n                yield text\n                return\n\n        if split_text and len(text) > 1 and params_infer_code.spk_smp is None:\n            refer_text = text[0]\n            result = next(\n                self._infer_code(\n                    refer_text,\n                    False,\n                    self.device,\n                    use_decoder,\n                    params_infer_code,\n                )\n            )\n            wavs = self._decode_to_wavs(\n                result.hiddens if use_decoder else result.ids,\n                use_decoder,\n            )\n            result.destroy()\n            assert len(wavs), 1\n            params_infer_code.spk_smp = self.sample_audio_speaker(wavs[0])\n            params_infer_code.txt_smp = refer_text\n\n        if stream:\n            length = 0\n            pass_batch_count = 0\n        if split_text:\n            n = len(text) // max_split_batch\n            if len(text) % max_split_batch:\n                n += 1\n        else:\n            n = 1\n            max_split_batch = len(text)\n        for i in range(n):\n            text_remain = text[i * max_split_batch :]\n            if len(text_remain) > max_split_batch:\n                text_remain = text_remain[:max_split_batch]\n            if split_text:\n                self.logger.info(\n                    \"infer split %d~%d\",\n                    i * max_split_batch,\n                    i * max_split_batch + len(text_remain),\n                )\n            for result in self._infer_code(\n                text_remain,\n                stream,\n                self.device,\n                use_decoder,\n                params_infer_code,\n            ):\n                wavs = self._decode_to_wavs(\n                    result.hiddens if use_decoder else result.ids,\n                    use_decoder,\n                )\n                result.destroy()\n                if stream:\n                    pass_batch_count += 1\n                    if pass_batch_count <= params_infer_code.pass_first_n_batches:\n                        continue\n                    a = length\n                    b = a + params_infer_code.stream_speed\n                    if b > wavs.shape[1]:\n                        b = wavs.shape[1]\n                    new_wavs = wavs[:, a:b]\n                    length = b\n                    yield new_wavs\n                else:\n                    yield wavs\n            if stream:\n                new_wavs = wavs[:, length:]\n                keep_cols = np.sum(np.abs(new_wavs) > 1e-5, axis=0) > 0\n                yield new_wavs[:][:, keep_cols]\n\n    @torch.inference_mode()\n    def _vocos_decode(self, spec: torch.Tensor) -> np.ndarray:\n        if \"mps\" in str(self.device) or \"npu\" in str(self.device):\n            return self.vocos.decode(spec.cpu()).cpu().numpy()\n        else:\n            return self.vocos.decode(spec).cpu().numpy()\n\n    @torch.inference_mode()\n    def _decode_to_wavs(\n        self,\n        result_list: List[torch.Tensor],\n        use_decoder: bool,\n    ):\n        decoder = self.decoder if use_decoder else self.dvae\n        max_x_len = -1\n        if len(result_list) == 0:\n            return np.array([], dtype=np.float32)\n        for result in result_list:\n            if result.size(0) > max_x_len:\n                max_x_len = result.size(0)\n        batch_result = torch.zeros(\n            (len(result_list), result_list[0].size(1), max_x_len),\n            dtype=result_list[0].dtype,\n            device=result_list[0].device,\n        )\n        for i in range(len(result_list)):\n            src = result_list[i]\n            batch_result[i].narrow(1, 0, src.size(0)).copy_(src.permute(1, 0))\n            del src\n        del_all(result_list)\n        mel_specs = decoder(batch_result)\n        del batch_result\n        wavs = self._vocos_decode(mel_specs)\n        del mel_specs\n        return wavs\n\n    @torch.no_grad()\n    def _infer_code(\n        self,\n        text: Tuple[List[str], str],\n        stream: bool,\n        device: torch.device,\n        return_hidden: bool,\n        params: InferCodeParams,\n    ):\n\n        gpt = self.gpt\n\n        if not isinstance(text, list):\n            text = [text]\n\n        assert len(text), \"text should not be empty\"\n\n        if not isinstance(params.temperature, list):\n            temperature = [params.temperature] * self.config.gpt.num_vq\n        else:\n            temperature = params.temperature\n\n        input_ids, attention_mask, text_mask = self.tokenizer.encode(\n            self.speaker.decorate_code_prompts(\n                text,\n                params.prompt,\n                params.txt_smp,\n                params.spk_emb,\n            ),\n            self.config.gpt.num_vq,\n            prompt=(\n                self.speaker.decode_prompt(params.spk_smp)\n                if params.spk_smp is not None\n                else None\n            ),\n            device=self.device_gpt,\n        )\n        start_idx = input_ids.shape[-2]\n\n        num_code = self.config.gpt.num_audio_tokens - 1\n\n        logits_warpers, logits_processors = gen_logits(\n            num_code=num_code,\n            top_P=params.top_P,\n            top_K=params.top_K,\n            repetition_penalty=params.repetition_penalty,\n        )\n\n        if gpt.is_vllm:\n            from .model.velocity import SamplingParams\n\n            sample_params = SamplingParams(\n                temperature=temperature,\n                max_new_token=params.max_new_token,\n                max_tokens=8192,\n                min_new_token=params.min_new_token,\n                logits_processors=(logits_processors, logits_warpers),\n                eos_token=num_code,\n                infer_text=False,\n                start_idx=start_idx,\n            )\n            input_ids = [i.tolist() for i in input_ids]\n\n            result = gpt.llm.generate(\n                None,\n                sample_params,\n                input_ids,\n            )\n\n            token_ids = []\n            hidden_states = []\n            for i in result:\n                token_ids.append(torch.tensor(i.outputs[0].token_ids))\n                hidden_states.append(\n                    i.outputs[0].hidden_states.to(torch.float32).to(self.device)\n                )\n\n            del text_mask, input_ids\n\n            return [\n                GPT.GenerationOutputs(\n                    ids=token_ids,\n                    hiddens=hidden_states,\n                    attentions=[],\n                ),\n            ]\n\n        emb = self.embed(input_ids, text_mask)\n\n        del text_mask\n\n        if params.spk_emb is not None:\n            self.speaker.apply(\n                emb,\n                params.spk_emb,\n                input_ids,\n                self.tokenizer.spk_emb_ids,\n                self.gpt.device_gpt,\n            )\n\n        result = gpt.generate(\n            emb,\n            input_ids,\n            temperature=torch.tensor(temperature, device=device),\n            eos_token=num_code,\n            attention_mask=attention_mask,\n            max_new_token=params.max_new_token,\n            min_new_token=params.min_new_token,\n            logits_processors=(*logits_processors, *logits_warpers),\n            infer_text=False,\n            return_hidden=return_hidden,\n            stream=stream,\n            show_tqdm=params.show_tqdm,\n            ensure_non_empty=params.ensure_non_empty,\n            stream_batch=params.stream_batch,\n            manual_seed=params.manual_seed,\n            context=self.context,\n        )\n\n        del emb, input_ids\n\n        return result\n\n    @torch.no_grad()\n    def _refine_text(\n        self,\n        text: str,\n        device: torch.device,\n        params: RefineTextParams,\n    ):\n\n        gpt = self.gpt\n\n        if not isinstance(text, list):\n            text = [text]\n\n        input_ids, attention_mask, text_mask = self.tokenizer.encode(\n            self.speaker.decorate_text_prompts(text, params.prompt),\n            self.config.gpt.num_vq,\n            device=self.device_gpt,\n        )\n\n        logits_warpers, logits_processors = gen_logits(\n            num_code=self.tokenizer.len,\n            top_P=params.top_P,\n            top_K=params.top_K,\n            repetition_penalty=params.repetition_penalty,\n        )\n\n        if gpt.is_vllm:\n            from .model.velocity import SamplingParams\n\n            sample_params = SamplingParams(\n                repetition_penalty=params.repetition_penalty,\n                temperature=params.temperature,\n                top_p=params.top_P,\n                top_k=params.top_K,\n                max_new_token=params.max_new_token,\n                max_tokens=8192,\n                min_new_token=params.min_new_token,\n                logits_processors=(logits_processors, logits_warpers),\n                eos_token=self.tokenizer.eos_token,\n                infer_text=True,\n                start_idx=input_ids.shape[-2],\n            )\n            input_ids_list = [i.tolist() for i in input_ids]\n            del input_ids\n\n            result = gpt.llm.generate(\n                None, sample_params, input_ids_list, params.show_tqdm\n            )\n            token_ids = []\n            hidden_states = []\n            for i in result:\n                token_ids.append(torch.tensor(i.outputs[0].token_ids))\n                hidden_states.append(i.outputs[0].hidden_states)\n\n            del text_mask, input_ids_list, result\n\n            return GPT.GenerationOutputs(\n                ids=token_ids,\n                hiddens=hidden_states,\n                attentions=[],\n            )\n\n        emb = self.embed(input_ids, text_mask)\n\n        del text_mask\n\n        result = next(\n            gpt.generate(\n                emb,\n                input_ids,\n                temperature=torch.tensor([params.temperature], device=device),\n                eos_token=self.tokenizer.eos_token,\n                attention_mask=attention_mask,\n                max_new_token=params.max_new_token,\n                min_new_token=params.min_new_token,\n                logits_processors=(*logits_processors, *logits_warpers),\n                infer_text=True,\n                stream=False,\n                show_tqdm=params.show_tqdm,\n                ensure_non_empty=params.ensure_non_empty,\n                manual_seed=params.manual_seed,\n                context=self.context,\n            )\n        )\n\n        del emb, input_ids\n\n        return result\n",
      "Description": "这是一个类，里面的方法可以用来实现聊天功能",
      "Path": "/mnt/autor_name/haoTingDeWenJianjia/ChatTTS/ChatTTS/core.py",
      "Name": "Chat",
      "Examples": [
        "\nimport ChatTTS\nimport torch\nimport torchaudio\n\nchat = ChatTTS.Chat()\nchat.load(\n    source=\"local\",          # 使用本地模式\n    custom_path=\"/mnt/autor_name/haoTingDeWenJianJia/ChatTTS/Model\",  # 替换为你的模型目录路径\n    force_redownload=False, # 禁用强制重新下载\n    compile=False,          # 可选是否编译模型\n    device=\"cuda\"            # 指定运行设备\n)\n\ntexts = [\"Hello\", \"Hola\"]\n\nwavs = chat.infer(texts)\n\nfor i in range(len(wavs)):\n    try:\n        torchaudio.save(f\"basic_output{i}.wav\", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)\n    except:\n        torchaudio.save(f\"basic_output{i}.wav\", torch.from_numpy(wavs[i]), 24000)\n"
      ]
    }
  ]
}