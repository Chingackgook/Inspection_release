{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/Spark-TTS",
    "API_Calls": [
        {
            "Name": "generate_voice",
            "Description": "在命令行推理脚本中初始化 TTS 模型,用户通过命令行参数指定文本、参考音频等，脚本会用 model.inference(...) 生成语音并保存到本地，实现批量或单条文本转语音。为开发者或批处理任务提供自动化语音合成能力。",
            "Code": "# Copyright (c) 2025 SparkAudio\n#               2025 Xinsheng Wang (w.xinshawn@gmail.com)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport argparse\nimport torch\nimport soundfile as sf\nimport logging\nfrom datetime import datetime\nimport platform\n\nfrom cli.SparkTTS import SparkTTS\n\n\ndef parse_args():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run TTS inference.\")\n\n    parser.add_argument(\n        \"--model_dir\",\n        type=str,\n        default=\"pretrained_models/Spark-TTS-0.5B\",\n        help=\"Path to the model directory\",\n    )\n    parser.add_argument(\n        \"--save_dir\",\n        type=str,\n        default=\"example/results\",\n        help=\"Directory to save generated audio files\",\n    )\n    parser.add_argument(\"--device\", type=int, default=0, help=\"CUDA device number\")\n    parser.add_argument(\n        \"--text\", type=str, required=True, help=\"Text for TTS generation\"\n    )\n    parser.add_argument(\"--prompt_text\", type=str, help=\"Transcript of prompt audio\")\n    parser.add_argument(\n        \"--prompt_speech_path\",\n        type=str,\n        help=\"Path to the prompt audio file\",\n    )\n    parser.add_argument(\"--gender\", choices=[\"male\", \"female\"])\n    parser.add_argument(\n        \"--pitch\", choices=[\"very_low\", \"low\", \"moderate\", \"high\", \"very_high\"]\n    )\n    parser.add_argument(\n        \"--speed\", choices=[\"very_low\", \"low\", \"moderate\", \"high\", \"very_high\"]\n    )\n    return parser.parse_args()\n\n\ndef run_tts(args):\n    \"\"\"Perform TTS inference and save the generated audio.\"\"\"\n    logging.info(f\"Using model from: {args.model_dir}\")\n    logging.info(f\"Saving audio to: {args.save_dir}\")\n\n    # Ensure the save directory exists\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    # Convert device argument to torch.device\n    if platform.system() == \"Darwin\" and torch.backends.mps.is_available():\n        # macOS with MPS support (Apple Silicon)\n        device = torch.device(f\"mps:{args.device}\")\n        logging.info(f\"Using MPS device: {device}\")\n    elif torch.cuda.is_available():\n        # System with CUDA support\n        device = torch.device(f\"cuda:{args.device}\")\n        logging.info(f\"Using CUDA device: {device}\")\n    else:\n        # Fall back to CPU\n        device = torch.device(\"cpu\")\n        logging.info(\"GPU acceleration not available, using CPU\")\n\n    # Initialize the model\n    model = SparkTTS(args.model_dir, device)\n\n    # Generate unique filename using timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    save_path = os.path.join(args.save_dir, f\"{timestamp}.wav\")\n\n    logging.info(\"Starting inference...\")\n\n    # Perform inference and save the output audio\n    with torch.no_grad():\n        wav = model.inference(\n            args.text,\n            args.prompt_speech_path,\n            prompt_text=args.prompt_text,\n            gender=args.gender,\n            pitch=args.pitch,\n            speed=args.speed,\n        )\n        sf.write(save_path, wav, samplerate=16000)\n\n    logging.info(f\"Audio saved at: {save_path}\")\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(\n        level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n    )\n\n    args = parse_args()\n    run_tts(args)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Spark-TTS/cli/inference.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "class_SparkTTS",
            "Description": "SparkTTS 类作用是实现文本转语音和智能语音合成的核心功能，为用户和开发者提供统一、易用的语音生成入口。",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/Spark-TTS/cli/SparkTTS.py",
            "Implementation": "# Copyright (c) 2025 SparkAudio\n#               2025 Xinsheng Wang (w.xinshawn@gmail.com)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport torch\nfrom typing import Tuple\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nfrom sparktts.utils.file import load_config\nfrom sparktts.models.audio_tokenizer import BiCodecTokenizer\nfrom sparktts.utils.token_parser import LEVELS_MAP, GENDER_MAP, TASK_TOKEN_MAP\n\n\nclass SparkTTS:\n    \"\"\"\n    Spark-TTS for text-to-speech generation.\n    \"\"\"\n\n    def __init__(self, model_dir: Path, device: torch.device = torch.device(\"cuda:0\")):\n        \"\"\"\n        Initializes the SparkTTS model with the provided configurations and device.\n\n        Args:\n            model_dir (Path): Directory containing the model and config files.\n            device (torch.device): The device (CPU/GPU) to run the model on.\n        \"\"\"\n        self.device = device\n        self.model_dir = model_dir\n        self.configs = load_config(f\"{model_dir}/config.yaml\")\n        self.sample_rate = self.configs[\"sample_rate\"]\n        self._initialize_inference()\n\n    def _initialize_inference(self):\n        \"\"\"Initializes the tokenizer, model, and audio tokenizer for inference.\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(f\"{self.model_dir}/LLM\")\n        self.model = AutoModelForCausalLM.from_pretrained(f\"{self.model_dir}/LLM\")\n        self.audio_tokenizer = BiCodecTokenizer(self.model_dir, device=self.device)\n        self.model.to(self.device)\n\n    def process_prompt(\n        self,\n        text: str,\n        prompt_speech_path: Path,\n        prompt_text: str = None,\n    ) -> Tuple[str, torch.Tensor]:\n        \"\"\"\n        Process input for voice cloning.\n\n        Args:\n            text (str): The text input to be converted to speech.\n            prompt_speech_path (Path): Path to the audio file used as a prompt.\n            prompt_text (str, optional): Transcript of the prompt audio.\n\n        Return:\n            Tuple[str, torch.Tensor]: Input prompt; global tokens\n        \"\"\"\n\n        global_token_ids, semantic_token_ids = self.audio_tokenizer.tokenize(\n            prompt_speech_path\n        )\n        global_tokens = \"\".join(\n            [f\"<|bicodec_global_{i}|>\" for i in global_token_ids.squeeze()]\n        )\n\n        # Prepare the input tokens for the model\n        if prompt_text is not None:\n            semantic_tokens = \"\".join(\n                [f\"<|bicodec_semantic_{i}|>\" for i in semantic_token_ids.squeeze()]\n            )\n            inputs = [\n                TASK_TOKEN_MAP[\"tts\"],\n                \"<|start_content|>\",\n                prompt_text,\n                text,\n                \"<|end_content|>\",\n                \"<|start_global_token|>\",\n                global_tokens,\n                \"<|end_global_token|>\",\n                \"<|start_semantic_token|>\",\n                semantic_tokens,\n            ]\n        else:\n            inputs = [\n                TASK_TOKEN_MAP[\"tts\"],\n                \"<|start_content|>\",\n                text,\n                \"<|end_content|>\",\n                \"<|start_global_token|>\",\n                global_tokens,\n                \"<|end_global_token|>\",\n            ]\n\n        inputs = \"\".join(inputs)\n\n        return inputs, global_token_ids\n\n    def process_prompt_control(\n        self,\n        gender: str,\n        pitch: str,\n        speed: str,\n        text: str,\n    ):\n        \"\"\"\n        Process input for voice creation.\n\n        Args:\n            gender (str): female | male.\n            pitch (str): very_low | low | moderate | high | very_high\n            speed (str): very_low | low | moderate | high | very_high\n            text (str): The text input to be converted to speech.\n\n        Return:\n            str: Input prompt\n        \"\"\"\n        assert gender in GENDER_MAP.keys()\n        assert pitch in LEVELS_MAP.keys()\n        assert speed in LEVELS_MAP.keys()\n\n        gender_id = GENDER_MAP[gender]\n        pitch_level_id = LEVELS_MAP[pitch]\n        speed_level_id = LEVELS_MAP[speed]\n\n        pitch_label_tokens = f\"<|pitch_label_{pitch_level_id}|>\"\n        speed_label_tokens = f\"<|speed_label_{speed_level_id}|>\"\n        gender_tokens = f\"<|gender_{gender_id}|>\"\n\n        attribte_tokens = \"\".join(\n            [gender_tokens, pitch_label_tokens, speed_label_tokens]\n        )\n\n        control_tts_inputs = [\n            TASK_TOKEN_MAP[\"controllable_tts\"],\n            \"<|start_content|>\",\n            text,\n            \"<|end_content|>\",\n            \"<|start_style_label|>\",\n            attribte_tokens,\n            \"<|end_style_label|>\",\n        ]\n\n        return \"\".join(control_tts_inputs)\n\n    @torch.no_grad()\n    def inference(\n        self,\n        text: str,\n        prompt_speech_path: Path = None,\n        prompt_text: str = None,\n        gender: str = None,\n        pitch: str = None,\n        speed: str = None,\n        temperature: float = 0.8,\n        top_k: float = 50,\n        top_p: float = 0.95,\n    ) -> torch.Tensor:\n        \"\"\"\n        Performs inference to generate speech from text, incorporating prompt audio and/or text.\n\n        Args:\n            text (str): The text input to be converted to speech.\n            prompt_speech_path (Path): Path to the audio file used as a prompt.\n            prompt_text (str, optional): Transcript of the prompt audio.\n            gender (str): female | male.\n            pitch (str): very_low | low | moderate | high | very_high\n            speed (str): very_low | low | moderate | high | very_high\n            temperature (float, optional): Sampling temperature for controlling randomness. Default is 0.8.\n            top_k (float, optional): Top-k sampling parameter. Default is 50.\n            top_p (float, optional): Top-p (nucleus) sampling parameter. Default is 0.95.\n\n        Returns:\n            torch.Tensor: Generated waveform as a tensor.\n        \"\"\"\n        if gender is not None:\n            prompt = self.process_prompt_control(gender, pitch, speed, text)\n\n        else:\n            prompt, global_token_ids = self.process_prompt(\n                text, prompt_speech_path, prompt_text\n            )\n        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(self.device)\n\n        # Generate speech using the model\n        generated_ids = self.model.generate(\n            **model_inputs,\n            max_new_tokens=3000,\n            do_sample=True,\n            top_k=top_k,\n            top_p=top_p,\n            temperature=temperature,\n        )\n\n        # Trim the output tokens to remove the input tokens\n        generated_ids = [\n            output_ids[len(input_ids) :]\n            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n        ]\n\n        # Decode the generated tokens into text\n        predicts = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n        # Extract semantic token IDs from the generated text\n        pred_semantic_ids = (\n            torch.tensor([int(token) for token in re.findall(r\"bicodec_semantic_(\\d+)\", predicts)])\n            .long()\n            .unsqueeze(0)\n        )\n\n        if gender is not None:\n            global_token_ids = (\n                torch.tensor([int(token) for token in re.findall(r\"bicodec_global_(\\d+)\", predicts)])\n                .long()\n                .unsqueeze(0)\n                .unsqueeze(0)\n            )\n\n        # Convert semantic tokens back to waveform\n        wav = self.audio_tokenizer.detokenize(\n            global_token_ids.to(self.device).squeeze(0),\n            pred_semantic_ids.to(self.device),\n        )\n\n        return wav",
            "Examples": [
                "import torch\nimport os\nimport sys\nimport soundfile as sf\nimport importlib.util\n\n# 动态导入 cli/SparkTTS.py\nsparktts_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'cli', 'SparkTTS.py')\nspec = importlib.util.spec_from_file_location(\"SparkTTS\", sparktts_path)\nif spec is None or spec.loader is None:\n    raise ImportError(f\"无法加载 {sparktts_path}\")\nSparkTTS_module = importlib.util.module_from_spec(spec)\nsys.modules[\"SparkTTS\"] = SparkTTS_module\nspec.loader.exec_module(SparkTTS_module)\nSparkTTS = SparkTTS_module.SparkTTS\n\nif __name__ == \"__main__\":\n    # 配置参数\n    model_dir = \"pretrained_models/Spark-TTS-0.5B\"  # 请确保模型已下载到此目录\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    text = \"你好，欢迎使用Spark-TTS语音合成系统。\"\n    save_path = \"test_output.wav\"\n\n    # 初始化模型\n    model = SparkTTS(model_dir, device)\n\n    # 合成语音（可控TTS，无需参考音频）\n    with torch.no_grad():\n        wav = model.inference(\n            text,\n            gender=\"male\",         # 或 \"female\"\n            pitch=\"moderate\",      # \"very_low\", \"low\", \"moderate\", \"high\", \"very_high\"\n            speed=\"moderate\"       # 同上\n        )\n        sf.write(save_path, wav, samplerate=16000)\n    print(f\"合成音频已保存到: {os.path.abspath(save_path)}\")\n\n"
            ]
        }
    ]
}