{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/lit-llama",
    "API_Calls": [
        {
            "Name": "call_generate",
            "Description": "call_generate",
            "Code": "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n\nimport sys\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama import LLaMA, Tokenizer\nfrom lit_llama.utils import lazy_load, llama_model_lookup, quantization\n\n\ndef main(\n    prompt: str = \"Hello, my name is\",\n    *,\n    num_samples: int = 1,\n    max_new_tokens: int = 50,\n    top_k: int = 200,\n    temperature: float = 0.8,\n    checkpoint_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\"),\n    tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\"),\n    quantize: Optional[str] = 'llm.int8',\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained LLaMA model and tokenizer.\n\n    Args:\n        prompt: The prompt string to use for generating the samples.\n        num_samples: The number of text samples to generate.\n        max_new_tokens: The number of generation steps to take.\n        top_k: The number of top most probable tokens to consider in the sampling process.\n        temperature: A value controlling the randomness of the sampling process. Higher values result in more random\n            samples.\n        checkpoint_path: The checkpoint path to load.\n        tokenizer_path: The tokenizer path to load.\n        quantize: Whether to quantize the model and using which method:\n            ``\"llm.int8\"``: LLM.int8() mode,\n            ``\"gptq.int4\"``: GPTQ 4-bit mode.\n    \"\"\"\n    assert checkpoint_path.is_file(), checkpoint_path\n    assert tokenizer_path.is_file(), tokenizer_path\n\n    precision = \"bf16-true\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"32-true\"\n    fabric = L.Fabric(devices=1, precision=precision)\n\n    print(\"Loading model ...\", file=sys.stderr)\n    t0 = time.time()\n    with lazy_load(checkpoint_path) as checkpoint:\n        name = llama_model_lookup(checkpoint)\n\n        with fabric.init_module(empty_init=True), quantization(mode=quantize):\n            model = LLaMA.from_name(name)\n\n        model.load_state_dict(checkpoint)\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n\n    model.eval()\n    model = fabric.setup(model)\n\n    tokenizer = Tokenizer(tokenizer_path)\n    encoded = tokenizer.encode(prompt, bos=True, eos=False, device=fabric.device)\n    prompt_length = encoded.size(0)\n\n    L.seed_everything(1234)\n    for i in range(num_samples):\n        t0 = time.perf_counter()\n        y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k)\n        t = time.perf_counter() - t0\n\n        model.reset_cache()\n        print(tokenizer.decode(y))\n        tokens_generated = y.size(0) - prompt_length\n        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n    if fabric.device.type == \"cuda\":\n        print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)\n\n\nif __name__ == \"__main__\":\n    from jsonargparse import CLI\n\n    torch.set_float32_matmul_precision(\"high\")\n    warnings.filterwarnings(\n        # Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31\n        \"ignore\", \n        message=\"ComplexHalf support is experimental and many operators don't support it yet\"\n    )\n    warnings.filterwarnings(\n        # Triggered in bitsandbytes/autograd/_functions.py:298\n        \"ignore\", \n        message=\"MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\",\n    )\n    CLI(main)\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/lit-llama/generate.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "generate",
            "Description": "generate",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/lit-llama/generate.py",
            "Implementation": "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n\nimport sys\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional\n\nimport lightning as L\nimport torch\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom lit_llama import LLaMA, Tokenizer\nfrom lit_llama.utils import lazy_load, llama_model_lookup, quantization\n\n\n@torch.no_grad()\ndef generate(\n    model: LLaMA,\n    idx: torch.Tensor,\n    max_new_tokens: int,\n    *,\n    max_seq_length: Optional[int] = None,\n    temperature: float = 1.0,\n    top_k: Optional[int] = None,\n    eos_id: Optional[int] = None,\n) -> torch.Tensor:\n    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n\n    The implementation of this function is modified from A. Karpathy's nanoGPT.\n\n    Args:\n        model: The model to use.\n        idx: Tensor of shape (T) with indices of the prompt sequence.\n        max_new_tokens: The number of new tokens to generate.\n        max_seq_length: The maximum sequence length allowed.\n        temperature: Scales the predicted logits by 1 / temperature\n        top_k: If specified, only sample among the tokens with the k highest probabilities\n        eos_id: If specified, stop generating any more token once the <eos> token is triggered\n    \"\"\"\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = idx.size(0)\n    T_new = T + max_new_tokens\n    if max_seq_length is None:\n        max_seq_length = min(T_new, model.config.block_size)\n\n    device, dtype = idx.device, idx.dtype\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty(T_new, dtype=dtype, device=device)\n    empty[:T] = idx\n    idx = empty\n    input_pos = torch.arange(0, T, device=device)\n\n    if idx.device.type == \"xla\":\n        import torch_xla.core.xla_model as xm\n\n        xm.mark_step()\n\n    # generate max_new_tokens tokens\n    for _ in range(max_new_tokens):\n        x = idx.index_select(0, input_pos).view(1, -1)\n\n        # forward\n        logits = model(x, max_seq_length, input_pos)\n        logits = logits[0, -1] / temperature\n\n        # optionally crop the logits to only the top k options\n        if top_k is not None:\n            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n            logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n\n        probs = torch.nn.functional.softmax(logits, dim=-1)\n        idx_next = torch.multinomial(probs, num_samples=1).to(dtype=dtype)\n\n        # advance\n        input_pos = input_pos[-1:] + 1\n\n        if idx.device.type == \"xla\":\n            xm.mark_step()\n\n        # concatenate the new generation\n        idx = idx.index_copy(0, input_pos, idx_next)\n\n        # if <eos> token is triggered, return the output (stop generation)\n        if idx_next == eos_id:\n            return idx[:input_pos]  # include the EOS token\n\n    return idx",
            "Examples": [
                "\n"
            ]
        }
    ]
}