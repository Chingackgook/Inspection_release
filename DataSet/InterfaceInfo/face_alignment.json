{
    "Project_Root": "/mnt/autor_name/haoTingDeWenJianJia/face-alignment",
    "API_Calls": [
        {
            "Name": "call_FaceAlignment",
            "Description": "call_FaceAlignment",
            "Code": "import face_alignment\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom skimage import io\nimport collections\n\n\n# Optionally set detector and some additional detector parameters\nface_detector = 'sfd'\nface_detector_kwargs = {\n    \"filter_threshold\" : 0.8\n}\n\n# Run the 3D face alignment on a test image, without CUDA.\nfa = face_alignment.FaceAlignment(face_alignment.LandmarksType.THREE_D, device='cpu', flip_input=True,\n                                  face_detector=face_detector, face_detector_kwargs=face_detector_kwargs)\n\ntry:\n    input_img = io.imread('../test/assets/aflw-test.jpg')\nexcept FileNotFoundError:\n    input_img = io.imread('test/assets/aflw-test.jpg')\n\npreds = fa.get_landmarks(input_img)[-1]\n\n# 2D-Plot\nplot_style = dict(marker='o',\n                  markersize=4,\n                  linestyle='-',\n                  lw=2)\n\npred_type = collections.namedtuple('prediction_type', ['slice', 'color'])\npred_types = {'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\n              'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\n              'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\n              'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\n              'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\n              'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\n              'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\n              'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\n              'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\n              }\n\nfig = plt.figure(figsize=plt.figaspect(.5))\nax = fig.add_subplot(1, 2, 1)\nax.imshow(input_img)\n\nfor pred_type in pred_types.values():\n    ax.plot(preds[pred_type.slice, 0],\n            preds[pred_type.slice, 1],\n            color=pred_type.color, **plot_style)\n\nax.axis('off')\n\n# 3D-Plot\nax = fig.add_subplot(1, 2, 2, projection='3d')\nsurf = ax.scatter(preds[:, 0] * 1.2,\n                  preds[:, 1],\n                  preds[:, 2],\n                  c='cyan',\n                  alpha=1.0,\n                  edgecolor='b')\n\nfor pred_type in pred_types.values():\n    ax.plot3D(preds[pred_type.slice, 0] * 1.2,\n              preds[pred_type.slice, 1],\n              preds[pred_type.slice, 2], color='blue')\n\nax.view_init(elev=90., azim=90.)\nax.set_xlim(ax.get_xlim()[::-1])\nplt.show()\n",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/face-alignment/examples/detect_landmarks_in_image.py"
        }
    ],
    "API_Implementations": [
        {
            "Name": "FaceAlignment",
            "Description": "FaceAlignment impl",
            "Path": "/mnt/autor_name/haoTingDeWenJianJia/face-alignment/face_alignment/api.py",
            "Implementation": "import torch\nimport warnings\nfrom enum import IntEnum\nfrom skimage import io\nimport numpy as np\nfrom packaging import version\nfrom tqdm import tqdm\n\nfrom .utils import *\nfrom .folder_data import FolderData\n\n\nclass LandmarksType(IntEnum):\n    \"\"\"Enum class defining the type of landmarks to detect.\n\n    ``TWO_D`` - the detected points ``(x,y)`` are detected in a 2D space and follow the visible contour of the face\n    ``TWO_HALF_D`` - this points represent the projection of the 3D points into 3D\n    ``THREE_D`` - detect the points ``(x,y,z)``` in a 3D space\n\n    \"\"\"\n    TWO_D = 1\n    TWO_HALF_D = 2\n    THREE_D = 3\n\n\nclass NetworkSize(IntEnum):\n    # TINY = 1\n    # SMALL = 2\n    # MEDIUM = 3\n    LARGE = 4\n\n\ndefault_model_urls = {\n    '2DFAN-4': 'https://www.adrianbulat.com/downloads/python-fan/2DFAN4-cd938726ad.zip',\n    '3DFAN-4': 'https://www.adrianbulat.com/downloads/python-fan/3DFAN4-4a694010b9.zip',\n    'depth': 'https://www.adrianbulat.com/downloads/python-fan/depth-6c4283c0e0.zip',\n}\n\nmodels_urls = {\n    '1.6': {\n        '2DFAN-4': 'https://www.adrianbulat.com/downloads/python-fan/2DFAN4_1.6-c827573f02.zip',\n        '3DFAN-4': 'https://www.adrianbulat.com/downloads/python-fan/3DFAN4_1.6-ec5cf40a1d.zip',\n        'depth': 'https://www.adrianbulat.com/downloads/python-fan/depth_1.6-2aa3f18772.zip',\n    },\n    '1.5': {\n        '2DFAN-4': 'https://www.adrianbulat.com/downloads/python-fan/2DFAN4_1.5-a60332318a.zip',\n        '3DFAN-4': 'https://www.adrianbulat.com/downloads/python-fan/3DFAN4_1.5-176570af4d.zip',\n        'depth': 'https://www.adrianbulat.com/downloads/python-fan/depth_1.5-bc10f98e39.zip',\n    },\n}\n\n\nclass FaceAlignment:\n    def __init__(self, landmarks_type, network_size=NetworkSize.LARGE,\n                 device='cuda', dtype=torch.float32, flip_input=False, face_detector='sfd', face_detector_kwargs=None, verbose=False):\n        self.device = device\n        self.flip_input = flip_input\n        self.landmarks_type = landmarks_type\n        self.verbose = verbose\n        self.dtype = dtype\n\n        if version.parse(torch.__version__) < version.parse('1.5.0'):\n            raise ImportError(f'Unsupported pytorch version detected. Minimum supported version of pytorch: 1.5.0\\\n                            Either upgrade (recommended) your pytorch setup, or downgrade to face-alignment 1.2.0')\n\n        network_size = int(network_size)\n        pytorch_version = torch.__version__\n        if 'dev' in pytorch_version:\n            pytorch_version = pytorch_version.rsplit('.', 2)[0]\n        else:\n            pytorch_version = pytorch_version.rsplit('.', 1)[0]\n\n        if 'cuda' in device:\n            torch.backends.cudnn.benchmark = True\n\n        # Get the face detector\n        face_detector_module = __import__('face_alignment.detection.' + face_detector,\n                                          globals(), locals(), [face_detector], 0)\n        face_detector_kwargs = face_detector_kwargs or {}\n        self.face_detector = face_detector_module.FaceDetector(device=device, verbose=verbose, **face_detector_kwargs)\n\n        # Initialise the face alignemnt networks\n        if landmarks_type == LandmarksType.TWO_D:\n            network_name = '2DFAN-' + str(network_size)\n        else:\n            network_name = '3DFAN-' + str(network_size)\n        self.face_alignment_net = torch.jit.load(\n            load_file_from_url(models_urls.get(pytorch_version, default_model_urls)[network_name]))\n\n        self.face_alignment_net.to(device, dtype=dtype)\n        self.face_alignment_net.eval()\n\n        # Initialiase the depth prediciton network\n        if landmarks_type == LandmarksType.THREE_D:\n            self.depth_prediciton_net = torch.jit.load(\n                load_file_from_url(models_urls.get(pytorch_version, default_model_urls)['depth']))\n\n            self.depth_prediciton_net.to(device, dtype=dtype)\n            self.depth_prediciton_net.eval()\n\n    def get_landmarks(self, image_or_path, detected_faces=None, return_bboxes=False, return_landmark_score=False):\n        \"\"\"Deprecated, please use get_landmarks_from_image\n\n        Arguments:\n            image_or_path {string or numpy.array or torch.tensor} -- The input image or path to it\n\n        Keyword Arguments:\n            detected_faces {list of numpy.array} -- list of bounding boxes, one for each face found\n            in the image (default: {None})\n            return_bboxes {boolean} -- If True, return the face bounding boxes in addition to the keypoints.\n            return_landmark_score {boolean} -- If True, return the keypoint scores along with the keypoints.\n        \"\"\"\n        return self.get_landmarks_from_image(image_or_path, detected_faces, return_bboxes, return_landmark_score)\n\n    @torch.no_grad()\n    def get_landmarks_from_image(self, image_or_path, detected_faces=None, return_bboxes=False,\n                                 return_landmark_score=False):\n        \"\"\"Predict the landmarks for each face present in the image.\n\n        This function predicts a set of 68 2D or 3D images, one for each image present.\n        If detect_faces is None the method will also run a face detector.\n\n         Arguments:\n            image_or_path {string or numpy.array or torch.tensor} -- The input image or path to it.\n\n        Keyword Arguments:\n            detected_faces {list of numpy.array} -- list of bounding boxes, one for each face found\n            in the image (default: {None})\n            return_bboxes {boolean} -- If True, return the face bounding boxes in addition to the keypoints.\n            return_landmark_score {boolean} -- If True, return the keypoint scores along with the keypoints.\n\n        Return:\n            result:\n                1. if both return_bboxes and return_landmark_score are False, result will be:\n                    landmark\n                2. Otherwise, result will be one of the following, depending on the actual value of return_* arguments.\n                    (landmark, landmark_score, detected_face)\n                    (landmark, None,           detected_face)\n                    (landmark, landmark_score, None         )\n        \"\"\"\n        image = get_image(image_or_path)\n\n        if detected_faces is None:\n            detected_faces = self.face_detector.detect_from_image(image.copy())\n\n        if len(detected_faces) == 0:\n            warnings.warn(\"No faces were detected.\")\n            if return_bboxes or return_landmark_score:\n                return None, None, None\n            else:\n                return None\n\n        landmarks = []\n        landmarks_scores = []\n        for i, d in enumerate(detected_faces):\n            center = np.array(\n                [d[2] - (d[2] - d[0]) / 2.0, d[3] - (d[3] - d[1]) / 2.0])\n            center[1] = center[1] - (d[3] - d[1]) * 0.12\n            scale = (d[2] - d[0] + d[3] - d[1]) / self.face_detector.reference_scale\n\n            inp = crop(image, center, scale)\n            inp = torch.from_numpy(inp.transpose(\n                (2, 0, 1))).float()\n\n            inp = inp.to(self.device, dtype=self.dtype)\n            inp.div_(255.0).unsqueeze_(0)\n\n            out = self.face_alignment_net(inp).detach()\n            if self.flip_input:\n                out += flip(self.face_alignment_net(flip(inp)).detach(), is_label=True)\n            out = out.to(device='cpu', dtype=torch.float32).numpy()\n\n            pts, pts_img, scores = get_preds_fromhm(out, center, scale)\n            pts, pts_img = torch.from_numpy(pts), torch.from_numpy(pts_img)\n            pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)\n            scores = scores.squeeze(0)\n\n            if self.landmarks_type == LandmarksType.THREE_D:\n                heatmaps = np.zeros((68, 256, 256), dtype=np.float32)\n                for i in range(68):\n                    if pts[i, 0] > 0 and pts[i, 1] > 0:\n                        heatmaps[i] = draw_gaussian(\n                            heatmaps[i], pts[i], 2)\n                heatmaps = torch.from_numpy(\n                    heatmaps).unsqueeze_(0)\n\n                heatmaps = heatmaps.to(self.device, dtype=self.dtype)\n                depth_pred = self.depth_prediciton_net(\n                    torch.cat((inp, heatmaps), 1)).data.cpu().view(68, 1).to(dtype=torch.float32)\n                pts_img = torch.cat(\n                    (pts_img, depth_pred * (1.0 / (256.0 / (200.0 * scale)))), 1)\n\n            landmarks.append(pts_img.numpy())\n            landmarks_scores.append(scores)\n\n        if not return_bboxes:\n            detected_faces = None\n        if not return_landmark_score:\n            landmarks_scores = None\n        if return_bboxes or return_landmark_score:\n            return landmarks, landmarks_scores, detected_faces\n        else:\n            return landmarks\n\n    @torch.no_grad()\n    def get_landmarks_from_batch(self, image_batch, detected_faces=None, return_bboxes=False,\n                                 return_landmark_score=False):\n        \"\"\"Predict the landmarks for each face present in the image.\n\n        This function predicts a set of 68 2D or 3D images, one for each image in a batch in parallel.\n        If detect_faces is None the method will also run a face detector.\n\n         Arguments:\n            image_batch {torch.tensor} -- The input images batch\n\n        Keyword Arguments:\n            detected_faces {list of numpy.array} -- list of bounding boxes, one for each face found\n            in the image (default: {None})\n            return_bboxes {boolean} -- If True, return the face bounding boxes in addition to the keypoints.\n            return_landmark_score {boolean} -- If True, return the keypoint scores along with the keypoints.\n\n        Return:\n            result:\n                1. if both return_bboxes and return_landmark_score are False, result will be:\n                    landmarks\n                2. Otherwise, result will be one of the following, depending on the actual value of return_* arguments.\n                    (landmark, landmark_score, detected_face)\n                    (landmark, None,           detected_face)\n                    (landmark, landmark_score, None         )\n        \"\"\"\n\n        if detected_faces is None:\n            detected_faces = self.face_detector.detect_from_batch(image_batch)\n\n        if len(detected_faces) == 0:\n            warnings.warn(\"No faces were detected.\")\n            if return_bboxes or return_landmark_score:\n                return None, None, None\n            else:\n                return None\n\n        landmarks = []\n        landmarks_scores_list = []\n        # A batch for each frame\n        for i, faces in enumerate(detected_faces):\n            res = self.get_landmarks_from_image(\n                image_batch[i].cpu().numpy().transpose(1, 2, 0),\n                detected_faces=faces,\n                return_landmark_score=return_landmark_score,\n            )\n            if return_landmark_score:\n                landmark_set, landmarks_scores, _ = res\n                landmarks_scores_list.append(landmarks_scores)\n            else:\n                landmark_set = res\n            # Bacward compatibility\n            if landmark_set is not None:\n                landmark_set = np.concatenate(landmark_set, axis=0)\n            else:\n                landmark_set = []\n            landmarks.append(landmark_set)\n\n        if not return_bboxes:\n            detected_faces = None\n        if not return_landmark_score:\n            landmarks_scores_list = None\n        if return_bboxes or return_landmark_score:\n            return landmarks, landmarks_scores_list, detected_faces\n        else:\n            return landmarks\n\n    def get_landmarks_from_directory(self, path, extensions=['.jpg', '.png'], recursive=True, show_progress_bar=True,\n                                     return_bboxes=False, return_landmark_score=False):\n        \"\"\"Scan a directory for images with a given extension type(s) and predict the landmarks for each\n            face present in the images found.\n\n         Arguments:\n            path {str} -- path to the target directory containing the images\n\n        Keyword Arguments:\n            extensions {list of str} -- list containing the image extensions considered (default: ['.jpg', '.png'])\n            recursive {boolean} -- If True, scans for images recursively (default: True)\n            show_progress_bar {boolean} -- If True displays a progress bar (default: True)\n            return_bboxes {boolean} -- If True, return the face bounding boxes in addition to the keypoints.\n            return_landmark_score {boolean} -- If True, return the keypoint scores along with the keypoints.\n        \"\"\"\n        dataset = FolderData(path, self.face_detector.tensor_or_path_to_ndarray, extensions, recursive, self.verbose)\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2, prefetch_factor=4)\n      \n        predictions = {}\n        for (image_path, image) in tqdm(dataloader, disable=not show_progress_bar):\n            image_path, image = image_path[0], image[0]\n            bounding_boxes = self.face_detector.detect_from_image(image)\n            if return_bboxes or return_landmark_score:\n                preds, bbox, score = self.get_landmarks_from_image(\n                    image, bounding_boxes, return_bboxes=return_bboxes, return_landmark_score=return_landmark_score)\n                predictions[image_path] = (preds, bbox, score)\n            else:\n                preds = self.get_landmarks_from_image(image, bounding_boxes)\n                predictions[image_path] = preds\n\n        return predictions\n",
            "Examples": [
                "\n"
            ]
        }
    ]
}