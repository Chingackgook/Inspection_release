$$$$$ä»£ç é€»è¾‘åˆ†æ$$$$$
è¿™æ®µä»£ç çš„ä¸»è¦æ‰§è¡Œé€»è¾‘æ˜¯å°†ä¸€æ®µæ–‡æœ¬å†…å®¹è¿›è¡Œåˆ†å—ã€ç¿»è¯‘ï¼Œå¹¶å°†ç¿»è¯‘ç»“æœä¿å­˜åˆ° Excel æ–‡ä»¶ä¸­ã€‚å®ƒçš„æ‰§è¡Œæµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š

### 1. å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
ä»£ç å¼€å§‹æ—¶ï¼Œå¯¼å…¥äº†å¤šä¸ªåº“å’Œæ¨¡å—ï¼ŒåŒ…æ‹¬ `pandas`ã€`json`ã€`concurrent.futures` ç­‰ï¼Œè¿™äº›åº“ç”¨äºæ•°æ®å¤„ç†ã€å¹¶å‘æ‰§è¡Œã€æ–‡ä»¶æ“ä½œç­‰ã€‚

### 2. å®šä¹‰è¾…åŠ©å‡½æ•°
- **`split_chunks_by_chars`**: è¯¥å‡½æ•°è´Ÿè´£å°†è¾“å…¥æ–‡æœ¬æŒ‰å­—ç¬¦æ•°è¿›è¡Œåˆ†å—ã€‚å®ƒè¯»å–ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼ŒæŒ‰è¡Œåˆ†å‰²æˆå¥å­ï¼Œå¹¶å°†å¥å­ç»„åˆæˆä¸è¶…è¿‡æŒ‡å®šå­—ç¬¦æ•°çš„å—ã€‚
- **`get_previous_content` å’Œ `get_after_content`**: è¿™ä¸¤ä¸ªå‡½æ•°ç”¨äºè·å–å½“å‰ç¿»è¯‘å—å‰åå†…å®¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨ç¿»è¯‘æ—¶æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ã€‚

### 3. ç¿»è¯‘å—çš„å¤„ç†
- **`translate_chunk`**: è¯¥å‡½æ•°è´Ÿè´£ç¿»è¯‘å•ä¸ªæ–‡æœ¬å—ã€‚å®ƒè°ƒç”¨ `translate_lines` å‡½æ•°è¿›è¡Œç¿»è¯‘ï¼Œå¹¶ä¼ é€’ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œä¸»é¢˜æç¤ºã€‚ç¿»è¯‘å®Œæˆåï¼Œè¿”å›å—çš„ç´¢å¼•ã€è‹±æ–‡ç»“æœå’Œç¿»è¯‘æ–‡æœ¬ã€‚

### 4. è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
- **`similar`**: è¯¥å‡½æ•°ä½¿ç”¨ `SequenceMatcher` æ¥è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚è¿™åœ¨åç»­æ­¥éª¤ä¸­ç”¨äºéªŒè¯ç¿»è¯‘ç»“æœçš„å‡†ç¡®æ€§ã€‚

### 5. ä¸»ç¿»è¯‘é€»è¾‘
- **`translate_all`**: è¿™æ˜¯ä»£ç çš„ä¸»å‡½æ•°ï¼Œè´Ÿè´£åè°ƒæ•´ä¸ªç¿»è¯‘è¿‡ç¨‹ã€‚
  - é¦–å…ˆï¼Œè°ƒç”¨ `split_chunks_by_chars` å‡½æ•°å°†æ–‡æœ¬åˆ†å—ã€‚
  - ç„¶åï¼Œè¯»å–ä¸»é¢˜æç¤ºã€‚
  - ä½¿ç”¨ `concurrent.futures.ThreadPoolExecutor` å®ç°å¹¶å‘ç¿»è¯‘ï¼Œæäº¤æ¯ä¸ªå—çš„ç¿»è¯‘ä»»åŠ¡ã€‚
  - ä½¿ç”¨ `Progress` æ˜¾ç¤ºç¿»è¯‘è¿›åº¦ã€‚
  - æ”¶é›†æ¯ä¸ªç¿»è¯‘å—çš„ç»“æœï¼Œå¹¶æ ¹æ®åŸå§‹é¡ºåºè¿›è¡Œæ’åºã€‚

### 6. ç»“æœå¤„ç†ä¸ä¿å­˜
- ç»“æœå¤„ç†éƒ¨åˆ†å°†æºæ–‡æœ¬å’Œç¿»è¯‘æ–‡æœ¬åˆ†åˆ«å­˜å‚¨åœ¨åˆ—è¡¨ä¸­ï¼Œå¹¶è¿›è¡Œç›¸ä¼¼åº¦æ£€æŸ¥ã€‚
  - å¦‚æœç¿»è¯‘ç»“æœä¸åŸå§‹å—çš„ç›¸ä¼¼åº¦ä½äº0.9ï¼ŒæŠ›å‡ºé”™è¯¯ï¼›å¦‚æœç›¸ä¼¼åº¦ä½äº1.0ï¼Œåˆ™å‘å‡ºè­¦å‘Šã€‚
- æœ€åï¼Œå°†ç¿»è¯‘ç»“æœä¸æ—¶é—´æˆ³å¯¹é½ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ° Excel æ–‡ä»¶ä¸­ã€‚
- åœ¨ä¿å­˜ä¹‹å‰ï¼Œæ£€æŸ¥ç¿»è¯‘æ–‡æœ¬çš„é•¿åº¦ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œä¿®å‰ªã€‚

### 7. ç¨‹åºå…¥å£
- `if __name__ == '__main__':` è¿™ä¸€è¡Œç¡®ä¿åœ¨ç›´æ¥è¿è¡Œè¯¥è„šæœ¬æ—¶ï¼Œä¼šè°ƒç”¨ `translate_all` å‡½æ•°å¯åŠ¨æ•´ä¸ªæµç¨‹ã€‚

### æ€»ç»“
æ•´ä¸ªä»£ç çš„ä¸»è¦é€»è¾‘æ˜¯é€šè¿‡åˆ†å—å¤„ç†æ–‡æœ¬ï¼Œåˆ©ç”¨å¹¶å‘æ‰§è¡Œæé«˜ç¿»è¯‘æ•ˆç‡ï¼Œç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œå‡†ç¡®ç¿»è¯‘ï¼Œå¹¶åœ¨å®Œæˆåå°†ç»“æœä¿å­˜åˆ° Excel æ–‡ä»¶ä¸­ã€‚é€šè¿‡ç›¸ä¼¼åº¦æ£€éªŒç¡®ä¿ç¿»è¯‘ç»“æœçš„è´¨é‡ï¼ŒåŒæ—¶æä¾›äº†ä¸°å¯Œçš„é”™è¯¯å¤„ç†å’Œç”¨æˆ·åé¦ˆæœºåˆ¶ã€‚


$$$$$ä»£ç æ‰§è¡Œè¡¥å…¨åˆ†æ$$$$$
åœ¨è€ƒè™‘å°†è¿™æ®µä»£ç é€šè¿‡ `exec` å‡½æ•°ç›´æ¥è¿è¡Œæ—¶ï¼Œç¡®å®ä¼šé¢ä¸´ä¸€äº›é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯å¯èƒ½å‡ºç°çš„é—®é¢˜å’Œç›¸åº”çš„è§£å†³æ–¹æ¡ˆï¼š

### å¯èƒ½å‡ºç°çš„é—®é¢˜

1. **äº¤äº’å¼è¾“å…¥**: 
   - å¦‚æœä»£ç ä¸­æœ‰ä»»ä½•äº¤äº’å¼è¾“å…¥ï¼ˆå¦‚ `input()`ã€`argparse` ç­‰ï¼‰ï¼Œåœ¨é€šè¿‡ `exec` è¿è¡Œæ—¶ï¼Œè¿™äº›è¾“å…¥å°†æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå› ä¸º `exec` ä¸æ”¯æŒç”¨æˆ·è¾“å…¥ã€‚
   
2. **æ¨¡å—å…¥å£ç¼ºå¤±**:
   - åŸä»£ç ä¸­ä½¿ç”¨äº† `if __name__ == "__main__"` æ¥ç¡®ä¿åªæœ‰åœ¨ç›´æ¥è¿è¡Œè¯¥è„šæœ¬æ—¶æ‰ä¼šè°ƒç”¨ `translate_all` å‡½æ•°ã€‚å¦‚æœç›´æ¥ä½¿ç”¨ `exec`ï¼Œè¿™ä¸ªå…¥å£å°†ä¸ä¼šè¢«è§¦å‘ï¼Œå¯¼è‡´ä»£ç ä¸ä¼šæ‰§è¡Œã€‚

3. **ç¼ºå°‘å¿…è¦çš„æ–‡ä»¶å’Œæ•°æ®**:
   - ä»£ç ä¸­ä¾èµ–äº†å¤šä¸ªå¤–éƒ¨æ–‡ä»¶ï¼ˆå¦‚ `_3_2_SPLIT_BY_MEANING`ã€`_4_1_TERMINOLOGY`ã€`_2_CLEANED_CHUNKS`ã€`_4_2_TRANSLATION`ï¼‰ï¼Œå¦‚æœè¿™äº›æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„ä¸æ­£ç¡®ï¼Œå°†å¯¼è‡´ä»£ç è¿è¡Œå¤±è´¥ã€‚

4. **å¹¶å‘æ‰§è¡Œçš„é—®é¢˜**:
   - `concurrent.futures.ThreadPoolExecutor` çš„ä½¿ç”¨åœ¨æŸäº›ç¯å¢ƒä¸‹å¯èƒ½ä¼šé‡åˆ°é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ²¡æœ‰é€‚å½“çš„ä¸Šä¸‹æ–‡æˆ–äº‹ä»¶å¾ªç¯çš„æƒ…å†µä¸‹ã€‚

### è§£å†³æ–¹æ¡ˆ

1. **å»é™¤äº¤äº’å¼è¾“å…¥**:
   - éœ€è¦å°†æ‰€æœ‰çš„è¾“å…¥éƒ¨åˆ†æ›¿æ¢ä¸ºç›´æ¥èµ‹å€¼ã€‚ä¾‹å¦‚ï¼Œè¯»å–æ–‡ä»¶çš„è·¯å¾„ã€ä¸»é¢˜æç¤ºç­‰ï¼Œå¯ä»¥é€šè¿‡ç¡¬ç¼–ç çš„æ–¹å¼è¿›è¡Œèµ‹å€¼ã€‚

2. **æ·»åŠ å…¥å£**:
   - éœ€è¦åœ¨ä»£ç çš„æœ€åæ·»åŠ ä¸€ä¸ªå‡½æ•°è°ƒç”¨ï¼Œç¡®ä¿åœ¨æ‰§è¡Œæ—¶èƒ½è§¦å‘ä¸»è¦çš„ç¿»è¯‘é€»è¾‘ã€‚å¯ä»¥ç›´æ¥è°ƒç”¨ `translate_all()`ï¼Œå¹¶åœ¨å…¶å†…éƒ¨å¤„ç†æ‰€æœ‰å¿…è¦çš„è¾“å…¥ã€‚

3. **æä¾›æ¨¡æ‹Ÿæ•°æ®**:
   - å¦‚æœä»£ç ä¾èµ–äºå¤–éƒ¨æ–‡ä»¶ï¼Œå¯ä»¥åœ¨ä»£ç ä¸­åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿæ•°æ®æˆ–ä½¿ç”¨å­—ç¬¦ä¸²ä»£æ›¿æ–‡ä»¶è¯»å–ï¼Œä»¥ç¡®ä¿ä»£ç å¯ä»¥åœ¨æ²¡æœ‰å¤–éƒ¨ä¾èµ–çš„æƒ…å†µä¸‹è¿è¡Œã€‚

4. **å¤„ç†å¹¶å‘æ‰§è¡Œ**:
   - ç¡®ä¿åœ¨ `exec` çš„ä¸Šä¸‹æ–‡ä¸­é€‚å½“åœ°å¤„ç†å¹¶å‘æ‰§è¡Œï¼Œå¯èƒ½éœ€è¦è€ƒè™‘å°†å…¶æ”¾åœ¨ä¸€ä¸ªå‡½æ•°å†…éƒ¨ï¼Œä»¥ç¡®ä¿ä¸Šä¸‹æ–‡çš„æ­£ç¡®æ€§ã€‚

### ä¿®æ”¹æ€è·¯

- å°†æ‰€æœ‰æ–‡ä»¶è·¯å¾„æ›¿æ¢ä¸ºç›¸åº”çš„å­—ç¬¦ä¸²ï¼Œç¡®ä¿åœ¨æµ‹è¯•æ—¶å¯ä»¥ç›´æ¥è¿è¡Œã€‚
- ç¡®ä¿æ‰€æœ‰éœ€è¦çš„å‚æ•°ï¼ˆå¦‚ä¸»é¢˜æç¤ºï¼‰éƒ½èƒ½åœ¨ä»£ç ä¸­ç›´æ¥å®šä¹‰ã€‚
- æ·»åŠ ä¸€ä¸ª `main()` å‡½æ•°ï¼Œè°ƒç”¨ `translate_all()`ï¼Œå¹¶ç¡®ä¿åœ¨ `exec` æ—¶èƒ½å¤Ÿè§¦å‘è¿™ä¸ªå‡½æ•°ã€‚
- å¤„ç†å¹¶å‘éƒ¨åˆ†ï¼Œç¡®ä¿åœ¨ `exec` ç¯å¢ƒä¸­èƒ½å¤Ÿæ­£å¸¸è¿è¡Œã€‚

é€šè¿‡ä»¥ä¸Šåˆ†æå’Œè§£å†³æ–¹æ¡ˆçš„å®æ–½ï¼Œä»£ç å°†èƒ½å¤Ÿåœ¨ `exec` çš„ä¸Šä¸‹æ–‡ä¸­é¡ºåˆ©è¿è¡Œï¼Œè€Œä¸éœ€è¦ç”¨æˆ·äº¤äº’æˆ–å¤–éƒ¨æ–‡ä»¶çš„ä¾èµ–ã€‚


$$$$$ä»£ç æ‰§è¡Œè¡¥å…¨ç»“æœ$$$$$
```python
import pandas as pd
import json
import concurrent.futures
from core.translate_lines import translate_lines
from core._4_1_summarize import search_things_to_note_in_prompt
from core._8_1_audio_task import check_len_then_trim
from core._6_gen_sub import align_timestamp
from core.utils import *
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from difflib import SequenceMatcher
from core.utils.models import *

console = Console()

# Mocking file paths and data for execution
_3_2_SPLIT_BY_MEANING = 'mock_split_by_meaning.txt'
_4_1_TERMINOLOGY = 'mock_terminology.json'
_2_CLEANED_CHUNKS = 'mock_cleaned_chunks.xlsx'
_4_2_TRANSLATION = 'mock_translation.xlsx'

# Mock data for testing
with open(_3_2_SPLIT_BY_MEANING, 'w', encoding='utf-8') as f:
    f.write("This is the first sentence.\nThis is the second sentence.\nThis is the third sentence.")

with open(_4_1_TERMINOLOGY, 'w', encoding='utf-8') as f:
    json.dump({"theme": "Test Theme"}, f)

# Function to split text into chunks
def split_chunks_by_chars(chunk_size, max_i): 
    """Split text into chunks based on character count, return a list of multi-line text chunks"""
    with open(_3_2_SPLIT_BY_MEANING, "r", encoding="utf-8") as file:
        sentences = file.read().strip().split('\n')

    chunks = []
    chunk = ''
    sentence_count = 0
    for sentence in sentences:
        if len(chunk) + len(sentence + '\n') > chunk_size or sentence_count == max_i:
            chunks.append(chunk.strip())
            chunk = sentence + '\n'
            sentence_count = 1
        else:
            chunk += sentence + '\n'
            sentence_count += 1
    chunks.append(chunk.strip())
    return chunks

# Get context from surrounding chunks
def get_previous_content(chunks, chunk_index):
    return None if chunk_index == 0 else chunks[chunk_index - 1].split('\n')[-3:] # Get last 3 lines
def get_after_content(chunks, chunk_index):
    return None if chunk_index == len(chunks) - 1 else chunks[chunk_index + 1].split('\n')[:2] # Get first 2 lines

# ğŸ” Translate a single chunk
def translate_chunk(chunk, chunks, theme_prompt, i):
    things_to_note_prompt = search_things_to_note_in_prompt(chunk)
    previous_content_prompt = get_previous_content(chunks, i)
    after_content_prompt = get_after_content(chunks, i)
    translation, english_result = translate_lines(chunk, previous_content_prompt, after_content_prompt, things_to_note_prompt, theme_prompt, i)
    return i, english_result, translation

# Add similarity calculation function
def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()

# ğŸš€ Main function to translate all chunks
def translate_all():
    console.print("[bold green]Start Translating All...[/bold green]")
    chunks = split_chunks_by_chars(chunk_size=600, max_i=10)
    with open(_4_1_TERMINOLOGY, 'r', encoding='utf-8') as file:
        theme_prompt = json.load(file).get('theme')

    # ğŸ”„ Use concurrent execution for translation
    with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
        task = progress.add_task("[cyan]Translating chunks...", total=len(chunks))
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:  # Mock max_workers
            futures = []
            for i, chunk in enumerate(chunks):
                future = executor.submit(translate_chunk, chunk, chunks, theme_prompt, i)
                futures.append(future)
            results = []
            for future in concurrent.futures.as_completed(futures):
                results.append(future.result())
                progress.update(task, advance=1)

    results.sort(key=lambda x: x[0])  # Sort results based on original order
    
    # ğŸ’¾ Save results to lists and Excel file
    src_text, trans_text = [], []
    for i, chunk in enumerate(chunks):
        chunk_lines = chunk.split('\n')
        src_text.extend(chunk_lines)
        
        # Calculate similarity between current chunk and translation results
        chunk_text = ''.join(chunk_lines).lower()
        matching_results = [(r, similar(''.join(r[1].split('\n')).lower(), chunk_text)) 
                          for r in results]
        best_match = max(matching_results, key=lambda x: x[1])
        
        # Check similarity and handle exceptions
        if best_match[1] < 0.9:
            console.print(f"[yellow]Warning: No matching translation found for chunk {i}[/yellow]")
            raise ValueError(f"Translation matching failed (chunk {i})")
        elif best_match[1] < 1.0:
            console.print(f"[yellow]Warning: Similar match found (chunk {i}, similarity: {best_match[1]:.3f})[/yellow]")
            
        trans_text.extend(best_match[0][2].split('\n'))
    
    # Trim long translation text
    df_text = pd.DataFrame({'text': ["This is a cleaned chunk."]})  # Mock data for cleaned chunks
    df_text['text'] = df_text['text'].str.strip('"').str.strip()
    df_translate = pd.DataFrame({'Source': src_text, 'Translation': trans_text})
    subtitle_output_configs = [('trans_subs_for_audio.srt', ['Translation'])]
    df_time = align_timestamp(df_text, df_translate, subtitle_output_configs, output_dir=None, for_display=False)
    console.print(df_time)
    # apply check_len_then_trim to df_time['Translation'], only when duration > MIN_TRIM_DURATION.
    df_time['Translation'] = df_time.apply(lambda x: check_len_then_trim(x['Translation'], 0) if 0 > load_key("min_trim_duration") else x['Translation'], axis=1)
    console.print(df_time)
    
    df_time.to_excel(_4_2_TRANSLATION, index=False)
    console.print("[bold green]âœ… Translation completed and results saved.[/bold green]")

# Trigger the main function
translate_all()
```


$$$$$ä»£ç å…³é”®å‡½æ•°/æ–¹æ³•å®šä½ä¸åˆ†ç±»åˆ†æ$$$$$
æ ¹æ®ä½ æä¾›çš„ä»£ç ç‰‡æ®µå’Œå‡½æ•°/ç±»åˆ—è¡¨ï¼Œä»¥ä¸‹æ˜¯å¯¹é—®é¢˜çš„å›ç­”ï¼š

### ques1
åœ¨è¿™ä¸ªä»£ç ç‰‡æ®µä¸­è¢«è°ƒç”¨çš„å…³é”®å‡½æ•°/æ–¹æ³•æ˜¯ï¼š
- `translate_lines`

### ques2
å¯¹äºæ¯ä¸ªè¢«è°ƒç”¨çš„å‡½æ•°/æ–¹æ³•çš„åˆ†ç±»ï¼š
- `translate_lines` æ˜¯ä¸€ä¸ªç‹¬ç«‹å‡½æ•°è°ƒç”¨ã€‚

### ques3
ç”±äº `translate_lines` æ˜¯ä¸€ä¸ªç‹¬ç«‹å‡½æ•°è°ƒç”¨ï¼Œå› æ­¤æ²¡æœ‰å¯¹åº”çš„å¯¹è±¡åˆå§‹åŒ–éƒ¨åˆ†å’Œåˆå§‹åŒ–å‚æ•°ã€‚è¯¥å‡½æ•°å¹¶ä¸å±äºä»»ä½•ç±»ã€‚

æ€»ç»“ï¼š
1. è¢«è°ƒç”¨çš„å‡½æ•°æ˜¯ `translate_lines`ã€‚
2. `translate_lines` æ˜¯ä¸€ä¸ªç‹¬ç«‹å‡½æ•°è°ƒç”¨ã€‚
3. `translate_lines` ä¸å±äºä»»ä½•ç±»ï¼Œå› æ­¤æ²¡æœ‰åˆå§‹åŒ–éƒ¨åˆ†å’Œå‚æ•°ã€‚


$$$$$ä»£ç æ¥å£è°ƒç”¨æ›¿æ¢åˆ†æ$$$$$
æ ¹æ®ä½ æä¾›çš„æ¥å£æ–‡æ¡£å’ŒåŸä»£ç çš„è°ƒç”¨æƒ…å†µï¼Œä»¥ä¸‹æ˜¯å¯¹è¢«è°ƒç”¨çš„å‡½æ•°/æ–¹æ³•çš„åˆ†æç»“æœï¼š

### è¢«è°ƒç”¨çš„å‡½æ•°/æ–¹æ³•
1. **`translate_lines`** - è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹å‡½æ•°è°ƒç”¨ã€‚

### è°ƒç”¨åˆ†æ
- **è°ƒç”¨å½¢å¼**: `translate_lines(chunk, previous_content_prompt, after_content_prompt, things_to_note_prompt, theme_prompt, i)`
- **å‚æ•°å¯¹åº”**:
  - `lines`: `chunk`
  - `previous_content_prompt`: `previous_content_prompt`
  - `after_content_prompt`: `after_content_prompt`
  - `things_to_note_prompt`: `things_to_note_prompt`
  - `summary_prompt`: `theme_prompt`
  - `index`: `i`

### æ›¿æ¢åˆ†æ
1. **ç‹¬ç«‹å‡½æ•°è°ƒç”¨**: å°† `translate_lines` çš„è°ƒç”¨æ›¿æ¢ä¸º `exe.run("translate_lines", lines=chunk, previous_content_prompt=previous_content_prompt, after_content_prompt=after_content_prompt, things_to_note_prompt=things_to_note_prompt, summary_prompt=theme_prompt, index=i)`ã€‚

### ä»£ç ä¸­æ²¡æœ‰ç±»çš„åˆå§‹åŒ–
- ç”±äºåœ¨åŸä»£ç ä¸­æ²¡æœ‰ä»»ä½•ç±»çš„å®ä¾‹åŒ–åŠå…¶æ–¹æ³•çš„è°ƒç”¨ï¼Œæ‰€æœ‰çš„å‡½æ•°è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹å‡½æ•°çš„è°ƒç”¨ï¼Œå› æ­¤æ²¡æœ‰éœ€è¦æ›¿æ¢ä¸º `exe.create_interface_objects` çš„éƒ¨åˆ†ã€‚

### æ€»ç»“
- åªéœ€å°† `translate_lines` çš„è°ƒç”¨æ›¿æ¢ä¸º `exe.run` çš„å½¢å¼ã€‚
- æ²¡æœ‰ç±»çš„åˆå§‹åŒ–éƒ¨åˆ†éœ€è¦æ›¿æ¢ã€‚

æ¥ä¸‹æ¥å¯ä»¥æ ¹æ®ä»¥ä¸Šåˆ†æç»“æœç”Ÿæˆç›¸åº”çš„ä»£ç æ›¿æ¢ã€‚


$$$$$ä»£ç æ¥å£è°ƒç”¨æ›¿æ¢ç»“æœ$$$$$
import pandas as pd
import json
import concurrent.futures
from core.translate_lines import translate_lines
from core._4_1_summarize import search_things_to_note_in_prompt
from core._8_1_audio_task import check_len_then_trim
from core._6_gen_sub import align_timestamp
from core.utils import *
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from difflib import SequenceMatcher
from core.utils.models import *
console = Console()
_3_2_SPLIT_BY_MEANING = 'mock_split_by_meaning.txt'
_4_1_TERMINOLOGY = 'mock_terminology.json'
_2_CLEANED_CHUNKS = 'mock_cleaned_chunks.xlsx'
_4_2_TRANSLATION = 'mock_translation.xlsx'
with open(_3_2_SPLIT_BY_MEANING, 'w', encoding='utf-8') as f:
    f.write('This is the first sentence.\nThis is the second sentence.\nThis is the third sentence.')
with open(_4_1_TERMINOLOGY, 'w', encoding='utf-8') as f:
    json.dump({'theme': 'Test Theme'}, f)

def split_chunks_by_chars(chunk_size, max_i):
    """Split text into chunks based on character count, return a list of multi-line text chunks"""
    with open(_3_2_SPLIT_BY_MEANING, 'r', encoding='utf-8') as file:
        sentences = file.read().strip().split('\n')
    chunks = []
    chunk = ''
    sentence_count = 0
    for sentence in sentences:
        if len(chunk) + len(sentence + '\n') > chunk_size or sentence_count == max_i:
            chunks.append(chunk.strip())
            chunk = sentence + '\n'
            sentence_count = 1
        else:
            chunk += sentence + '\n'
            sentence_count += 1
    chunks.append(chunk.strip())
    return chunks

def get_previous_content(chunks, chunk_index):
    return None if chunk_index == 0 else chunks[chunk_index - 1].split('\n')[-3:]

def get_after_content(chunks, chunk_index):
    return None if chunk_index == len(chunks) - 1 else chunks[chunk_index + 1].split('\n')[:2]

def translate_chunk(chunk, chunks, theme_prompt, i):
    things_to_note_prompt = search_things_to_note_in_prompt(chunk)
    previous_content_prompt = get_previous_content(chunks, i)
    after_content_prompt = get_after_content(chunks, i)
    (translation, english_result) = exe.run('translate_lines', lines=chunk, previous_content_prompt=previous_content_prompt, after_content_prompt=after_content_prompt, things_to_note_prompt=things_to_note_prompt, summary_prompt=theme_prompt, index=i)
    return (i, english_result, translation)

def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()

def translate_all():
    console.print('[bold green]Start Translating All...[/bold green]')
    chunks = split_chunks_by_chars(chunk_size=600, max_i=10)
    with open(_4_1_TERMINOLOGY, 'r', encoding='utf-8') as file:
        theme_prompt = json.load(file).get('theme')
    with Progress(SpinnerColumn(), TextColumn('[progress.description]{task.description}'), transient=True) as progress:
        task = progress.add_task('[cyan]Translating chunks...', total=len(chunks))
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for (i, chunk) in enumerate(chunks):
                future = executor.submit(translate_chunk, chunk, chunks, theme_prompt, i)
                futures.append(future)
            results = []
            for future in concurrent.futures.as_completed(futures):
                results.append(future.result())
                progress.update(task, advance=1)
    results.sort(key=lambda x: x[0])
    (src_text, trans_text) = ([], [])
    for (i, chunk) in enumerate(chunks):
        chunk_lines = chunk.split('\n')
        src_text.extend(chunk_lines)
        chunk_text = ''.join(chunk_lines).lower()
        matching_results = [(r, similar(''.join(r[1].split('\n')).lower(), chunk_text)) for r in results]
        best_match = max(matching_results, key=lambda x: x[1])
        if best_match[1] < 0.9:
            console.print(f'[yellow]Warning: No matching translation found for chunk {i}[/yellow]')
            raise ValueError(f'Translation matching failed (chunk {i})')
        elif best_match[1] < 1.0:
            console.print(f'[yellow]Warning: Similar match found (chunk {i}, similarity: {best_match[1]:.3f})[/yellow]')
        trans_text.extend(best_match[0][2].split('\n'))
    df_text = pd.DataFrame({'text': ['This is a cleaned chunk.']})
    df_text['text'] = df_text['text'].str.strip('"').str.strip()
    df_translate = pd.DataFrame({'Source': src_text, 'Translation': trans_text})
    subtitle_output_configs = [('trans_subs_for_audio.srt', ['Translation'])]
    df_time = align_timestamp(df_text, df_translate, subtitle_output_configs, output_dir=None, for_display=False)
    console.print(df_time)
    df_time['Translation'] = df_time.apply(lambda x: check_len_then_trim(x['Translation'], 0) if 0 > load_key('min_trim_duration') else x['Translation'], axis=1)
    console.print(df_time)
    df_time.to_excel(_4_2_TRANSLATION, index=False)
    console.print('[bold green]âœ… Translation completed and results saved.[/bold green]')
translate_all()


$$$$$ä»£ç ä¼˜åŒ–åˆ†æ$$$$$
å¥½çš„ï¼Œä¸‹é¢æ˜¯å¯¹ä½ çš„é—®é¢˜çš„é€ä¸€å›ç­”ï¼š

### 1. æ–‡ä»¶æˆ–è·¯å¾„è¾“å…¥çš„åœ°æ–¹
è¿™æ®µä»£ç ä¸­æœ‰å‡ ä¸ªåœ°æ–¹æ¶‰åŠåˆ°æ–‡ä»¶æˆ–è·¯å¾„çš„è¾“å…¥ï¼Œä¸»è¦é›†ä¸­åœ¨ä»¥ä¸‹ä»£ç ç‰‡æ®µï¼š

```python
with open(_3_2_SPLIT_BY_MEANING, 'r', encoding='utf-8') as file:
    sentences = file.read().strip().split('\n')

with open(_4_1_TERMINOLOGY, 'r', encoding='utf-8') as file:
    theme_prompt = json.load(file).get('theme')
```

### 2. æ–‡ä»¶æˆ–è·¯å¾„è¾“å‡ºçš„åœ°æ–¹
æ–‡ä»¶æˆ–è·¯å¾„çš„è¾“å‡ºä¸»è¦åœ¨ä»¥ä¸‹ä»£ç ç‰‡æ®µä¸­ï¼š

```python
with open(_3_2_SPLIT_BY_MEANING, 'w', encoding='utf-8') as f:
    f.write('This is the first sentence.\nThis is the second sentence.\nThis is the third sentence.')

with open(_4_1_TERMINOLOGY, 'w', encoding='utf-8') as f:
    json.dump({'theme': 'Test Theme'}, f)

df_time.to_excel(_4_2_TRANSLATION, index=False)
```

### 3. éœ€è¦æ‰‹åŠ¨ä¿®æ”¹çš„åœ°æ–¹
ä»¥ä¸‹æ˜¯å¯èƒ½éœ€è¦æ‰‹åŠ¨ä¿®æ”¹çš„åœ°æ–¹ï¼š

- æ–‡ä»¶è·¯å¾„ï¼š
  - `_3_2_SPLIT_BY_MEANING`ã€`_4_1_TERMINOLOGY`ã€`_4_2_TRANSLATION` è¿™å‡ ä¸ªè·¯å¾„å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚
  
- å‚æ•°ï¼š
  - `chunk_size=600` å’Œ `max_i=10` è¿™ä¸¤ä¸ªå‚æ•°åœ¨ `split_chunks_by_chars` å‡½æ•°ä¸­å®šä¹‰ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚
  
- `load_key('min_trim_duration')` çš„è¿”å›å€¼å¯èƒ½éœ€è¦æ ¹æ®ä½ çš„é€»è¾‘è¿›è¡Œè°ƒæ•´ã€‚

### 4. è¯­æ³•é”™è¯¯æˆ–é€»è¾‘é”™è¯¯
åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæ²¡æœ‰æ˜æ˜¾çš„è¯­æ³•é”™è¯¯ã€‚ä¸è¿‡ï¼Œå…³äºä¸»é€»è¾‘çš„è¿è¡Œï¼Œé€šå¸¸åœ¨ Python è„šæœ¬ä¸­ä½¿ç”¨ `if __name__ == '__main__':` æ¥ç¡®ä¿ä»£ç åªåœ¨è„šæœ¬ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œï¼Œè€Œä¸æ˜¯åœ¨è¢«å¯¼å…¥æ—¶æ‰§è¡Œã€‚æ ¹æ®å½“å‰ä»£ç ï¼Œæ²¡æœ‰ä½¿ç”¨è¿™ä¸ªç»“æ„æ¥è¿è¡Œ `translate_all()` å‡½æ•°ï¼Œè¿™æ„å‘³ç€å¦‚æœè¿™ä¸ªè„šæœ¬è¢«å…¶ä»–æ¨¡å—å¯¼å…¥ï¼Œ`translate_all()` å‡½æ•°ä¼šç«‹å³æ‰§è¡Œï¼Œè¿™å¯èƒ½ä¸æ˜¯é¢„æœŸçš„è¡Œä¸ºã€‚

å»ºè®®åœ¨ä»£ç åº•éƒ¨æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```python
if __name__ == '__main__':
    translate_all()
```

è¿™æ ·å¯ä»¥ç¡®ä¿åªæœ‰åœ¨è„šæœ¬ç›´æ¥è¿è¡Œæ—¶æ‰ä¼šè°ƒç”¨ `translate_all()` å‡½æ•°ã€‚


$$$$$ä»£ç ä¼˜åŒ–ç»“æœ$$$$$
å¥½çš„ï¼Œä¸‹é¢æ˜¯å¯¹æºä»£ç çš„ä¼˜åŒ–ï¼ŒæŒ‰ç…§ä½ çš„è¦æ±‚è¿›è¡Œäº†ä¿®æ”¹ï¼š

```python
from Inspection.core.executor import Executor
from Inspection.adapters.custom_adapters.VideoLingo import ENV_DIR
from Inspection.adapters.custom_adapters.VideoLingo import *
exe = Executor('VideoLingo','simulation')
FILE_RECORD_PATH = exe.now_record_path

# å¯èƒ½éœ€è¦æ‰‹åŠ¨ä¿®æ”¹çš„éƒ¨åˆ†ï¼š
chunk_size = 600  # æ¯ä¸ªå—çš„å­—ç¬¦æ•°
max_i = 10        # æœ€å¤§å¥å­æ•°
# end

# å¯¼å…¥åŸæœ‰çš„åŒ…
import pandas as pd
import json
import concurrent.futures
from core.translate_lines import translate_lines
from core._4_1_summarize import search_things_to_note_in_prompt
from core._8_1_audio_task import check_len_then_trim
from core._6_gen_sub import align_timestamp
from core.utils import *
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from difflib import SequenceMatcher
from core.utils.models import *

console = Console()
_3_2_SPLIT_BY_MEANING = f'{ENV_DIR}/mock_split_by_meaning.txt'
_4_1_TERMINOLOGY = f'{ENV_DIR}/mock_terminology.json'
_2_CLEANED_CHUNKS = f'{ENV_DIR}/mock_cleaned_chunks.xlsx'
_4_2_TRANSLATION = f'{FILE_RECORD_PATH}/mock_translation.xlsx'

with open(_3_2_SPLIT_BY_MEANING, 'w', encoding='utf-8') as f:
    f.write('This is the first sentence.\nThis is the second sentence.\nThis is the third sentence.')
with open(_4_1_TERMINOLOGY, 'w', encoding='utf-8') as f:
    json.dump({'theme': 'Test Theme'}, f)

def split_chunks_by_chars(chunk_size, max_i):
    """Split text into chunks based on character count, return a list of multi-line text chunks"""
    with open(_3_2_SPLIT_BY_MEANING, 'r', encoding='utf-8') as file:
        sentences = file.read().strip().split('\n')
    chunks = []
    chunk = ''
    sentence_count = 0
    for sentence in sentences:
        if len(chunk) + len(sentence + '\n') > chunk_size or sentence_count == max_i:
            chunks.append(chunk.strip())
            chunk = sentence + '\n'
            sentence_count = 1
        else:
            chunk += sentence + '\n'
            sentence_count += 1
    chunks.append(chunk.strip())
    return chunks

def get_previous_content(chunks, chunk_index):
    return None if chunk_index == 0 else chunks[chunk_index - 1].split('\n')[-3:]

def get_after_content(chunks, chunk_index):
    return None if chunk_index == len(chunks) - 1 else chunks[chunk_index + 1].split('\n')[:2]

def translate_chunk(chunk, chunks, theme_prompt, i):
    things_to_note_prompt = search_things_to_note_in_prompt(chunk)
    previous_content_prompt = get_previous_content(chunks, i)
    after_content_prompt = get_after_content(chunks, i)
    (translation, english_result) = exe.run('translate_lines', lines=chunk, previous_content_prompt=previous_content_prompt, after_content_prompt=after_content_prompt, things_to_note_prompt=things_to_note_prompt, summary_prompt=theme_prompt, index=i)
    return (i, english_result, translation)

def similar(a, b):
    return SequenceMatcher(None, a, b).ratio()

# ç›´æ¥è¿è¡Œä¸»é€»è¾‘
console.print('[bold green]Start Translating All...[/bold green]')
chunks = split_chunks_by_chars(chunk_size=chunk_size, max_i=max_i)
with open(_4_1_TERMINOLOGY, 'r', encoding='utf-8') as file:
    theme_prompt = json.load(file).get('theme')
with Progress(SpinnerColumn(), TextColumn('[progress.description]{task.description}'), transient=True) as progress:
    task = progress.add_task('[cyan]Translating chunks...', total=len(chunks))
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for (i, chunk) in enumerate(chunks):
            future = executor.submit(translate_chunk, chunk, chunks, theme_prompt, i)
            futures.append(future)
        results = []
        for future in concurrent.futures.as_completed(futures):
            results.append(future.result())
            progress.update(task, advance=1)
results.sort(key=lambda x: x[0])
(src_text, trans_text) = ([], [])
for (i, chunk) in enumerate(chunks):
    chunk_lines = chunk.split('\n')
    src_text.extend(chunk_lines)
    chunk_text = ''.join(chunk_lines).lower()
    matching_results = [(r, similar(''.join(r[1].split('\n')).lower(), chunk_text)) for r in results]
    best_match = max(matching_results, key=lambda x: x[1])
    if best_match[1] < 0.9:
        console.print(f'[yellow]Warning: No matching translation found for chunk {i}[/yellow]')
        raise ValueError(f'Translation matching failed (chunk {i})')
    elif best_match[1] < 1.0:
        console.print(f'[yellow]Warning: Similar match found (chunk {i}, similarity: {best_match[1]:.3f})[/yellow]')
    trans_text.extend(best_match[0][2].split('\n'))
df_text = pd.DataFrame({'text': ['This is a cleaned chunk.']})
df_text['text'] = df_text['text'].str.strip('"').str.strip()
df_translate = pd.DataFrame({'Source': src_text, 'Translation': trans_text})
subtitle_output_configs = [('trans_subs_for_audio.srt', ['Translation'])]
df_time = align_timestamp(df_text, df_translate, subtitle_output_configs, output_dir=None, for_display=False)
console.print(df_time)
df_time['Translation'] = df_time.apply(lambda x: check_len_then_trim(x['Translation'], 0) if 0 > load_key('min_trim_duration') else x['Translation'], axis=1)
console.print(df_time)
df_time.to_excel(_4_2_TRANSLATION, index=False)
console.print('[bold green]âœ… Translation completed and results saved.[/bold green]')
```

ä»¥ä¸Šæ˜¯ä¼˜åŒ–åçš„ä»£ç ï¼Œå·²æŒ‰ç…§ä½ çš„è¦æ±‚è¿›è¡Œäº†ä¿®æ”¹ã€‚